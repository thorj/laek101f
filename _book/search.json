[{"path":"index.html","id":"general-information","chapter":"1 General information","heading":"1 General information","text":"","code":"\nlibrary(tidyverse)\nlibrary(knitr)\nlibrary(kableExtra)"},{"path":"index.html","id":"instructors","chapter":"1 General information","heading":"1.1 Instructors","text":"Lectures: Þórarinn Jónmundsson (thj73@hi.)Workshops:Supervision: Thor Aspelund (thor@hi.)","code":""},{"path":"index.html","id":"about-me","chapter":"1 General information","heading":"1.2 About me","text":"","code":""},{"path":"index.html","id":"education","chapter":"1 General information","heading":"1.2.1 Education","text":"B.Sc. economics [2017]; B.Sc. applied mathematics [2020]M.Sc. mathematical statistics [2020]Ph.D. biostatistics [current]Research project: Identifying causal candidate proteins cardiometabolic disease","code":""},{"path":"index.html","id":"previous-teaching-experience","chapter":"1 General information","heading":"1.2.2 Previous teaching experience","text":"Teacher assistant:\nIntroduction probability statistics (STÆ203G)\nLinear algebra (STÆ107G)\nBayesian data analysis (STÆ529M)\nApplied linear statistical models (STÆ312M)\nR beginners (MAS103M)\nStatistical consulting (MAS201M)\nIntroduction probability statistics (STÆ203G)Linear algebra (STÆ107G)Bayesian data analysis (STÆ529M)Applied linear statistical models (STÆ312M)R beginners (MAS103M)Statistical consulting (MAS201M)Lecturer\nApplied linear statistical models (STÆ312M)\nApplied linear statistical models (STÆ312M)","code":""},{"path":"index.html","id":"contact-information","chapter":"1 General information","heading":"1.2.3 Contact information","text":"Email: thj73@hi.isOffice: Læknagarður (Lg-306)Mobile: 698-2126","code":""},{"path":"index.html","id":"course-description","chapter":"1 General information","heading":"1.3 Course description","text":"Compulsory course. aim course provide post graduate students practical generic skills required research. Items covered course descriptive statistics, effect statistics, validity reliability, inferential statistics, common parametric nonparametric statistical tests multiple regression analysis. students introduced computer statistical analysis practical computer classes.","code":""},{"path":"index.html","id":"learning-outcomes","chapter":"1 General information","heading":"1.4 Learning outcomes","text":"students:Understand basic concepts statistics data analysis.Can apply basic statistical methods data analysed thesis.Can understand criticize statistical analysis data health research.literate R.Understand theory behind various statistical calculations, able apply knowledge analysis data.","code":""},{"path":"index.html","id":"textbooks","chapter":"1 General information","heading":"1.5 Textbooks","text":"lecture notes based following texts:Textbooks English:\nModern Dive (MD)\nR Data Science (R4DS)\nStatistical Thinking 21st Century (ST21)\nModern Dive (MD)R Data Science (R4DS)Statistical Thinking 21st Century (ST21)Textbooks Icelandic:\nTölfræði frá grunni (TG)\nR frá grunni (RG)\nTölfræði frá grunni (TG)R frá grunni (RG)sufficient read lecture notes interested can read books pace.","code":""},{"path":"index.html","id":"software","chapter":"1 General information","heading":"1.6 Software","text":"RRStudio","code":""},{"path":"index.html","id":"lectures-and-workshops","chapter":"1 General information","heading":"1.7 Lectures and workshops","text":"Lectures 9:10-11:30 workshops 12:30 14:00. lectures recorded streamed.. Lecture workshop attendance mandatory. Student’s strongly encouraged utilize workshops!.can see rough plan course. Depending course goes may subject change. Lectures marked open might used extra lecture workshop slots.","code":""},{"path":"index.html","id":"tentative-lecture-schedule","chapter":"1 General information","heading":"1.7.1 Tentative lecture schedule","text":"","code":""},{"path":"index.html","id":"workshop-schedule","chapter":"1 General information","heading":"1.7.2 Workshop schedule","text":"","code":""},{"path":"index.html","id":"grading","chapter":"1 General information","heading":"1.8 Grading","text":"final grade class based following partition:Four assignments R, worth 10% final grade.Two longer assignments, worth 10% final grade.final exam worth 40% final grade.must get score 5/10 higher final exam finish course.","code":""},{"path":"index.html","id":"assignments","chapter":"1 General information","heading":"1.9 Assignments","text":"schedule assignments. encouraged form groups 2-4 people solve assignments. choose , please hand one solution per group.","code":""},{"path":"index.html","id":"final-exam","chapter":"1 General information","heading":"1.10 Final exam","text":"final exam take-home exam handed March 1st.","code":""},{"path":"index.html","id":"final-exam-eligibility","chapter":"1 General information","heading":"1.10.1 Final exam eligibility","text":"need hand four R assignments home assignments eligible take final exam.","code":""},{"path":"lecture-1-r-and-r-basics.html","id":"lecture-1-r-and-r-basics","chapter":"2 Lecture 1: R and R basics","heading":"2 Lecture 1: R and R basics","text":"","code":""},{"path":"lecture-1-r-and-r-basics.html","id":"installing-r-and-rstudio","chapter":"2 Lecture 1: R and R basics","heading":"2.1 Installing R and RStudio","text":"install RStudio, need install R. Please visit link download R operating system. Afterwards, can download RStudio .Alternatively, can create account RStudio Cloud cloud-based version RStudio.","code":""},{"path":"lecture-1-r-and-r-basics.html","id":"what-is-the-difference-between-r-and-rstudio","chapter":"2 Lecture 1: R and R basics","heading":"2.2 What is the difference between R and RStudio?","text":"R programming language specifically designed statistics RStudio Integrated Development Environment (IDE). RStudio combines multiple tool single graphical user interface (GUI) ease development R programs. Examples tools source-code environment, file navigation, plot viewer.exist alternatives RStudio. personally use Nvim-R like many others started R journey RStudio.","code":""},{"path":"lecture-1-r-and-r-basics.html","id":"what-is-the-difference-between-.r-and-.rmd-files","chapter":"2 Lecture 1: R and R basics","heading":"2.3 What is the difference between .R and .Rmd files?","text":"file .R extension script file R. script file contains code written. .R file “stupid” sense follow instructions letter. tell computer run R scrip opening command prompt/terminal, navigating root folder, writing Rscript yourScript.R.file .Rmd extension markdown file. allows combine code text write detailed reports. example .Rmd file lecture.","code":""},{"path":"lecture-1-r-and-r-basics.html","id":"hello-world","chapter":"2 Lecture 1: R and R basics","heading":"2.4 Hello world","text":"simple piece R code inside .Rmd document:anatomy code follows:# Example embedded code. # symbol tells R ignore whatever comes # particular line. useful allows us comment code. Comments allow explain code . Writing informative, concise comments extremely important. good chance either share code collaborator revisit later time. cases, comments can quickly bring reader speed.# Example embedded code. # symbol tells R ignore whatever comes # particular line. useful allows us comment code. Comments allow explain code . Writing informative, concise comments extremely important. good chance either share code collaborator revisit later time. cases, comments can quickly bring reader speed.print('Hello world'). line composed two things: print() function string 'Hello world'. function like recipe; inputs outputs. recipe print() take input display command line. many predefined functions R can also create . string data type R. quotes tell R whatever considered string. remove quotes, R whine. can also tell R something string double quotes. Pick whichever prefer.print('Hello world'). line composed two things: print() function string 'Hello world'. function like recipe; inputs outputs. recipe print() take input display command line. many predefined functions R can also create . string data type R. quotes tell R whatever considered string. remove quotes, R whine. can also tell R something string double quotes. Pick whichever prefer.","code":"\n# Example of embedded code\nprint('Hello world')\n#> [1] \"Hello world\"\n# Function printMyString\n# Input: A string.\n# Output: The input string is printed out in the command line.\nprintMyString <- function(someString) {\n    print(someString)\n}\n# Test case\nprintMyString(\"This is a custom function I made. It's pretty useless.\")\n#> [1] \"This is a custom function I made. It's pretty useless.\""},{"path":"lecture-1-r-and-r-basics.html","id":"other-basic-objects","chapter":"2 Lecture 1: R and R basics","heading":"2.5 Other basic objects","text":"R many objects besides strings integers, doubles (reals), vectors, factors, logical, data frames.integer whole number. Examples integers 1, -2, 1000, 94.integer whole number. Examples integers 1, -2, 1000, 94.double real number. Examples real numbers 1, -2, 3.14, sin(0.5).double real number. Examples real numbers 1, -2, 3.14, sin(0.5).vector collection values. define vector c() function. example c(1, -2, 3.14, sin(0.5)) vector doubles c('', 'ab', 'bla bla') vector strings. Note elements vector data type. discrepancy, R try coerce elements data type.vector collection values. define vector c() function. example c(1, -2, 3.14, sin(0.5)) vector doubles c('', 'ab', 'bla bla') vector strings. Note elements vector data type. discrepancy, R try coerce elements data type.factor way store categorical values. example, imagine conducted study patients. One pieces information store smoking status patients. Patients can current smokers, previous smokers, never smokers. levels factor never, previous, current. discuss factor variables later.factor way store categorical values. example, imagine conducted study patients. One pieces information store smoking status patients. Patients can current smokers, previous smokers, never smokers. levels factor never, previous, current. discuss factor variables later.logical value either TRUE FALSE. Logical values can also encoded 1 0 TRUE FALSE respectively.logical value either TRUE FALSE. Logical values can also encoded 1 0 TRUE FALSE respectively.data frame collection vectors. can think data frame Excel spreadsheet columns rows. data frames later.data frame collection vectors. can think data frame Excel spreadsheet columns rows. data frames later.Note exhaustive list.","code":""},{"path":"lecture-1-r-and-r-basics.html","id":"storing-objects-in-memory","chapter":"2 Lecture 1: R and R basics","heading":"2.6 Storing objects in memory","text":"can store objects memory <- operator. Whatever right <- object want store memory whatever left <- name object. quick example:vector c(1, 2, 3) exists now memory named variable x rather typing c(1, 2, 3) whenever want work vector, can simply use x instead. example, imagine wanted add 1 vector c(1, 2, 3); since stored c(1, 2, 3) object x R, can simply write:careful! reuse variable names pre-existing object overwritten new object.","code":"\nx <- c(1, 2, 3)\nx\n#> [1] 1 2 3\nx + 1\n#> [1] 2 3 4\nx <- 'x is no longer a vector of numbers; it is a string'\nx\n#> [1] \"x is no longer a vector of numbers; it is a string\""},{"path":"lecture-1-r-and-r-basics.html","id":"packages","chapter":"2 Lecture 1: R and R basics","heading":"2.7 Packages","text":"R many built-functions data types sometimes need . Luckily, good chance someone R community already solved problem made work publicly available R package. R package thus simply collection functions data types (sometimes even data) can load R use.can download install package install.packages() function. load package use library() function. enough install package must load every time open RStudio (intend use ).","code":"\n# This can be run once\ninstall.packages('ggplot2')\n# This is something you have to run every time \n# you restart R\nlibrary('ggplot2')"},{"path":"lecture-1-r-and-r-basics.html","id":"the-mtcars-data","chapter":"2 Lecture 1: R and R basics","heading":"2.8 The mtcars data","text":"’s time start working data. Included R mtcars data frame:Note entire data set printed. annoying avoided. Functions like as_tibble() included tibble library converts data frame tibble, much nicer data type work (opinion). begin installing loading tibble library:use mtcars data frame input as_tibble() function.things worth mentioning :data truncated. see first ten rows however many columns fit page.data truncated. see first ten rows however many columns fit page.get information total number rows columns tibble.get information total number rows columns tibble.get data type column. Note dbl underneath column name.get data type column. Note dbl underneath column name.Since prefer using tibble format, going overwrite data frame mtcars tibble version [recall discussion ]:can refer specific variables tibbles (data frames) $ operator:Sometimes want access number rows /columns data. can use nrow() ncol() functions respectively. Alternatively, can use dim() function get simultaneously.","code":"\nmtcars\n#>                      mpg cyl  disp  hp drat    wt  qsec vs\n#> Mazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0\n#> Mazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0\n#> Datsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1\n#> Hornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1\n#> Hornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0\n#> Valiant             18.1   6 225.0 105 2.76 3.460 20.22  1\n#> Duster 360          14.3   8 360.0 245 3.21 3.570 15.84  0\n#> Merc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1\n#> Merc 230            22.8   4 140.8  95 3.92 3.150 22.90  1\n#> Merc 280            19.2   6 167.6 123 3.92 3.440 18.30  1\n#> Merc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1\n#> Merc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0\n#> Merc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0\n#> Merc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0\n#> Cadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0\n#> Lincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0\n#> Chrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0\n#> Fiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1\n#> Honda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1\n#> Toyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1\n#> Toyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1\n#> Dodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0\n#> AMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0\n#> Camaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0\n#> Pontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0\n#> Fiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1\n#> Porsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0\n#> Lotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1\n#> Ford Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0\n#> Ferrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0\n#> Maserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0\n#> Volvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1\n#>                     am gear carb\n#> Mazda RX4            1    4    4\n#> Mazda RX4 Wag        1    4    4\n#> Datsun 710           1    4    1\n#> Hornet 4 Drive       0    3    1\n#> Hornet Sportabout    0    3    2\n#> Valiant              0    3    1\n#> Duster 360           0    3    4\n#> Merc 240D            0    4    2\n#> Merc 230             0    4    2\n#> Merc 280             0    4    4\n#> Merc 280C            0    4    4\n#> Merc 450SE           0    3    3\n#> Merc 450SL           0    3    3\n#> Merc 450SLC          0    3    3\n#> Cadillac Fleetwood   0    3    4\n#> Lincoln Continental  0    3    4\n#> Chrysler Imperial    0    3    4\n#> Fiat 128             1    4    1\n#> Honda Civic          1    4    2\n#> Toyota Corolla       1    4    1\n#> Toyota Corona        0    3    1\n#> Dodge Challenger     0    3    2\n#> AMC Javelin          0    3    2\n#> Camaro Z28           0    3    4\n#> Pontiac Firebird     0    3    2\n#> Fiat X1-9            1    4    1\n#> Porsche 914-2        1    5    2\n#> Lotus Europa         1    5    2\n#> Ford Pantera L       1    5    4\n#> Ferrari Dino         1    5    6\n#> Maserati Bora        1    5    8\n#> Volvo 142E           1    4    2\n# Install tibble package\ninstall.packages('tibble')\n# Load tibble package\nlibrary(tibble)\nas_tibble(mtcars)\n#> # A tibble: 32 × 11\n#>      mpg   cyl  disp    hp  drat    wt  qsec    vs    am\n#>    <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n#>  1  21       6  160    110  3.9   2.62  16.5     0     1\n#>  2  21       6  160    110  3.9   2.88  17.0     0     1\n#>  3  22.8     4  108     93  3.85  2.32  18.6     1     1\n#>  4  21.4     6  258    110  3.08  3.22  19.4     1     0\n#>  5  18.7     8  360    175  3.15  3.44  17.0     0     0\n#>  6  18.1     6  225    105  2.76  3.46  20.2     1     0\n#>  7  14.3     8  360    245  3.21  3.57  15.8     0     0\n#>  8  24.4     4  147.    62  3.69  3.19  20       1     0\n#>  9  22.8     4  141.    95  3.92  3.15  22.9     1     0\n#> 10  19.2     6  168.   123  3.92  3.44  18.3     1     0\n#> # … with 22 more rows, and 2 more variables: gear <dbl>,\n#> #   carb <dbl>\nmtcars <- as_tibble(mtcars)\nmtcars$mpg\n#>  [1] 21.0 21.0 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 17.8\n#> [12] 16.4 17.3 15.2 10.4 10.4 14.7 32.4 30.4 33.9 21.5 15.5\n#> [23] 15.2 13.3 19.2 27.3 26.0 30.4 15.8 19.7 15.0 21.4\nnrow(mtcars)\n#> [1] 32\nncol(mtcars)\n#> [1] 11\ndim(mtcars)\n#> [1] 32 11"},{"path":"lecture-1-r-and-r-basics.html","id":"summarizing-data-frames","chapter":"2 Lecture 1: R and R basics","heading":"2.9 Summarizing data frames","text":"can quickly summarize data summary() function:nice way get quick feel data.","code":"\nsummary(mtcars)\n#>       mpg             cyl             disp      \n#>  Min.   :10.40   Min.   :4.000   Min.   : 71.1  \n#>  1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8  \n#>  Median :19.20   Median :6.000   Median :196.3  \n#>  Mean   :20.09   Mean   :6.188   Mean   :230.7  \n#>  3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0  \n#>  Max.   :33.90   Max.   :8.000   Max.   :472.0  \n#>        hp             drat             wt       \n#>  Min.   : 52.0   Min.   :2.760   Min.   :1.513  \n#>  1st Qu.: 96.5   1st Qu.:3.080   1st Qu.:2.581  \n#>  Median :123.0   Median :3.695   Median :3.325  \n#>  Mean   :146.7   Mean   :3.597   Mean   :3.217  \n#>  3rd Qu.:180.0   3rd Qu.:3.920   3rd Qu.:3.610  \n#>  Max.   :335.0   Max.   :4.930   Max.   :5.424  \n#>       qsec             vs               am        \n#>  Min.   :14.50   Min.   :0.0000   Min.   :0.0000  \n#>  1st Qu.:16.89   1st Qu.:0.0000   1st Qu.:0.0000  \n#>  Median :17.71   Median :0.0000   Median :0.0000  \n#>  Mean   :17.85   Mean   :0.4375   Mean   :0.4062  \n#>  3rd Qu.:18.90   3rd Qu.:1.0000   3rd Qu.:1.0000  \n#>  Max.   :22.90   Max.   :1.0000   Max.   :1.0000  \n#>       gear            carb      \n#>  Min.   :3.000   Min.   :1.000  \n#>  1st Qu.:3.000   1st Qu.:2.000  \n#>  Median :4.000   Median :2.000  \n#>  Mean   :3.688   Mean   :2.812  \n#>  3rd Qu.:4.000   3rd Qu.:4.000  \n#>  Max.   :5.000   Max.   :8.000"},{"path":"lecture-1-r-and-r-basics.html","id":"subsetting-our-data","chapter":"2 Lecture 1: R and R basics","heading":"2.10 Subsetting our data","text":"Let us assume interested specific subset mtcars data. can create subset mtcars columns interested using select() function dplyr package. tibble package, install load dplyr package, something hope grown comfortable point:now create subset tibble mtcars data consist rows variables mpg, hp, wt, representing miles per gallon, gross horsepower, weight (1000 lbs) respectively.Dissecting select() function, see first supply function data set want subset (case mtcars) list selected variables. can also use select() function throw variables - prefix.","code":"\n# Install dplyr\ninstall.packages('dplyr')\n# Load dplyr package\nlibrary(dplyr)\nsmallData <- select(mtcars, mpg, hp, wt)\nsmallData\n#> # A tibble: 32 × 3\n#>      mpg    hp    wt\n#>    <dbl> <dbl> <dbl>\n#>  1  21     110  2.62\n#>  2  21     110  2.88\n#>  3  22.8    93  2.32\n#>  4  21.4   110  3.22\n#>  5  18.7   175  3.44\n#>  6  18.1   105  3.46\n#>  7  14.3   245  3.57\n#>  8  24.4    62  3.19\n#>  9  22.8    95  3.15\n#> 10  19.2   123  3.44\n#> # … with 22 more rows\nthrowOutVariables <- select(mtcars, -mpg, -hp, -wt)\nthrowOutVariables\n#> # A tibble: 32 × 8\n#>      cyl  disp  drat  qsec    vs    am  gear  carb\n#>    <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n#>  1     6  160   3.9   16.5     0     1     4     4\n#>  2     6  160   3.9   17.0     0     1     4     4\n#>  3     4  108   3.85  18.6     1     1     4     1\n#>  4     6  258   3.08  19.4     1     0     3     1\n#>  5     8  360   3.15  17.0     0     0     3     2\n#>  6     6  225   2.76  20.2     1     0     3     1\n#>  7     8  360   3.21  15.8     0     0     3     4\n#>  8     4  147.  3.69  20       1     0     4     2\n#>  9     4  141.  3.92  22.9     1     0     4     2\n#> 10     6  168.  3.92  18.3     1     0     4     4\n#> # … with 22 more rows"},{"path":"lecture-1-r-and-r-basics.html","id":"creating-new-variables","chapter":"2 Lecture 1: R and R basics","heading":"2.11 Creating new variables","text":"two ways making new variables: $ operator mutate() function dplyr package. Let’s create two new variables randomly drawing numbers interval [0, 1].Note overwrite smallData used mutate() function.","code":"\nsmallData$var1 <- runif(n = nrow(smallData))\nsmallData <- mutate(smallData, var2 = runif(n = nrow(smallData)))\nsmallData\n#> # A tibble: 32 × 5\n#>      mpg    hp    wt    var1   var2\n#>    <dbl> <dbl> <dbl>   <dbl>  <dbl>\n#>  1  21     110  2.62 0.941   0.849 \n#>  2  21     110  2.88 0.213   0.501 \n#>  3  22.8    93  2.32 0.212   0.858 \n#>  4  21.4   110  3.22 0.647   0.0926\n#>  5  18.7   175  3.44 0.473   0.424 \n#>  6  18.1   105  3.46 0.448   0.497 \n#>  7  14.3   245  3.57 0.0548  0.979 \n#>  8  24.4    62  3.19 0.284   0.189 \n#>  9  22.8    95  3.15 0.00903 0.302 \n#> 10  19.2   123  3.44 0.336   0.212 \n#> # … with 22 more rows"},{"path":"lecture-1-r-and-r-basics.html","id":"plotting-our-data","chapter":"2 Lecture 1: R and R basics","heading":"2.12 Plotting our data","text":"create plots use ggplot2 package installed earlier. cover plots detail later now example create scatter plot using ggplot() geom_point() functions.","code":"\nggplot(data = smallData, aes(x = var1, y = var2)) + \n    geom_point()"},{"path":"lecture-1-r-and-r-basics.html","id":"chaining-multiple-functions-together","chapter":"2 Lecture 1: R and R basics","heading":"2.13 Chaining multiple functions together","text":"far used select(), mutate(), ggplot() separately can actually chain together pipe operator %>% package magrittr. use pipe became ubiquitous got added R built-operator version 4.1. “base” R pipe |>.Let’s recreate plot previous section pipes.","code":"\ninstall.packages('magrittr')\nlibrary(magrittr)\nmtcars %>%\n    mutate(var1 = runif(nrow(mtcars)), \n           var2 = runif(nrow(mtcars))) %>% \n    select(var1, var2) %>%  # this step is unnecessary\n    ggplot(aes(x = var1, y = var2)) + \n        geom_point()"},{"path":"lecture-1-r-and-r-basics.html","id":"the-tidyverse-package","chapter":"2 Lecture 1: R and R basics","heading":"2.14 The tidyverse package","text":"course lecture installed loaded ggplot2, tibble, dplyr, magrittr. packages () belong tidyverse collection. loading tidyverse load ggplot2, dplyr, tidyr, readr, purrr, tibble, stringr, forcats. can read package : https://www.tidyverse.org/packages/install load tidyverse simply run:","code":"\ninstall.packages('tidyverse')\nlibrary(tidyverse)"},{"path":"lecture-2-data-wrangling.html","id":"lecture-2-data-wrangling","chapter":"3 Lecture 2: Data wrangling","heading":"3 Lecture 2: Data wrangling","text":"","code":"\nlibrary(tidyverse)\nlibrary(here) # relative paths\nlibrary(readxl)"},{"path":"lecture-2-data-wrangling.html","id":"r-projects-and-workflows","chapter":"3 Lecture 2: Data wrangling","heading":"3.1 R Projects and workflows","text":"Last week used mtcars data set included R. want load external data tell R find . tell R files , need supply R file paths. good opportunity introduce basic workflow.Let us assume tasked performing preliminary analysis data set. supervisor expects us :Present results couple days.Present results couple days.Hand preliminary analysis resident data scientist downstream modeling.Hand preliminary analysis resident data scientist downstream modeling.thus two things keep mind; need perform preliminary analysis need way data scientist can continue work painlessly possible. need get organized. means:project work , create dedicated folder specific project. files relevant project placed project folder.project work , create dedicated folder specific project. files relevant project placed project folder.project folder compartmentalized! , multiple subdirectories within project folder, serving specific purpose.project folder compartmentalized! , multiple subdirectories within project folder, serving specific purpose.example layout hypothetical scenario:created project folder relevant structure next thing create R project file. creating R project, telling R base operations (working directory). means can use relative paths, namely, instead writing full file path names like C:\\Documents\\ourProject\\data\\someData.csv suffices write data\\someData.csv.Finally, recommend use package .Rmd files use location working directory. Using file structure example, since .Rmd file saved within report try point data file someData.csv data/someData.csv met error. looks location R project file .Rproj sets file paths relative .Rproj.completing preliminary analysis can simply hand ourProject directory data scientist. reproducible research basic form.","code":"Within folder ourProject:\n-code/      code to wrangle original data\n-data/      output of code\n-raw/       original data files\n-report/    our .Rmd file"},{"path":"lecture-2-data-wrangling.html","id":"more-intricate-workflow","chapter":"3 Lecture 2: Data wrangling","heading":"3.1.1 More intricate workflow","text":"Look Snakemake, renv, github, DiagrammeR, Docker.","code":""},{"path":"lecture-2-data-wrangling.html","id":"the-anatomy-of-a-file-and-external-data","chapter":"3 Lecture 2: Data wrangling","heading":"3.2 The anatomy of a file and external data","text":"Consider following data:first line called header. names columns (sometimes referred fields) stated. Note data comes header line. functions read data memory assume header line don’t explicitly state file one. lines follow header store data values. semicolon ; value acts delimiter. delimiter helps R map values respective columns. delimiter necessarily ;. non exhaustive list:Comma ,. Data comma delimiter typically stored .csv files. .csv stands Comma-Separated Values. can read .csv files read_csv() function readr.Comma ,. Data comma delimiter typically stored .csv files. .csv stands Comma-Separated Values. can read .csv files read_csv() function readr.Semicolon ;. Data semicolon delimiter typically stored .csv files well. can read data semicolon-separated values read_csv2() function readr.Semicolon ;. Data semicolon delimiter typically stored .csv files well. can read data semicolon-separated values read_csv2() function readr.Tab \\t. Data \\t (tab) delimiter typically stored .csv .tsv files. .tsv stands Tab-Separated Values. can read .tsv files read_tsv() function readr.Tab \\t. Data \\t (tab) delimiter typically stored .csv .tsv files. .tsv stands Tab-Separated Values. can read .tsv files read_tsv() function readr.delimiters. can use general function read_delim() specify (ones mentioned ). Finally, Excel spreadsheets .xls .xlsx quite common well. read R can use readxl package.examples use , readr, readxl read dummy data created (see Appendix).","code":"id;type;age\n1;a;30\n2;b;37\n3;a;42\ndummyTSV <- read_tsv(here('data', 'l2_data_tsv.txt'))\n#> Rows: 10000 Columns: 4\n#> ── Column specification ──────────────────────────────────────────────────\n#> Delimiter: \"\\t\"\n#> chr (1): type\n#> dbl (3): id, age, metric1\n#> \n#> ℹ Use `spec()` to retrieve the full column specification for this data.\n#> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\ndummyCSV <- read_csv(here('data', 'l2_data_csv.txt'))\n#> Rows: 10000 Columns: 4\n#> ── Column specification ──────────────────────────────────────────────────\n#> Delimiter: \",\"\n#> chr (1): type\n#> dbl (3): id, age, metric1\n#> \n#> ℹ Use `spec()` to retrieve the full column specification for this data.\n#> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\ndumyCSV2 <- read_csv2(here('data', 'l2_data_csv2.txt'))\n#> ℹ Using \"','\" as decimal and \"'.'\" as grouping mark. Use `read_delim()` for more control.\n#> Rows: 10000 Columns: 4\n#> ── Column specification ──────────────────────────────────────────────────\n#> Delimiter: \";\"\n#> chr (1): type\n#> dbl (2): id, age\n#> \n#> ℹ Use `spec()` to retrieve the full column specification for this data.\n#> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\ndummyXLSX <- read_xlsx(here('data', 'l2_data_excel.xlsx'))\ndummyXLSX\n#> # A tibble: 10,000 × 4\n#>       id type    age metric1\n#>    <dbl> <chr> <dbl>   <dbl>\n#>  1     1 a        32    31.4\n#>  2     2 c        33    42.4\n#>  3     3 a        41    40.4\n#>  4     4 b        37    75.7\n#>  5     5 a        35    66.9\n#>  6     6 c        37    64.8\n#>  7     7 c        40    52.0\n#>  8     8 b        34    35.7\n#>  9     9 b        36    30.2\n#> 10    10 c        36    34.5\n#> # … with 9,990 more rows"},{"path":"lecture-2-data-wrangling.html","id":"factors","chapter":"3 Lecture 2: Data wrangling","heading":"3.3 Factors","text":"continue use dummyXLSX. Let’s get overview data glimpse() function dplyr:glimpse() see four variables, three type double one character. Let’s use summary() function last lecture probe data:looks well except type variable takes values , b, c. like see just many observations fall type. good opportunity discuss factor variables. factor variable variable levels labels. levels type aforementioned , b, c, (values type takes) labels contains “name” level. example, participant diabetes, b participant heart disease c controls. One levels factor variable baseline. Unless told otherwise, R arrange levels alphabetical order type, baseline since first letter alphabet.Let’s create new variable based schema .","code":"\nglimpse(dummyXLSX)\n#> Rows: 10,000\n#> Columns: 4\n#> $ id      <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,…\n#> $ type    <chr> \"a\", \"c\", \"a\", \"b\", \"a\", \"c\", \"c\", \"b\", \"b…\n#> $ age     <dbl> 32, 33, 41, 37, 35, 37, 40, 34, 36, 36, 37…\n#> $ metric1 <dbl> 31.41350, 42.44945, 40.40657, 75.74094, 66…\nsummary(dummyXLSX)\n#>        id            type                age       \n#>  Min.   :    1   Length:10000       Min.   :23.00  \n#>  1st Qu.: 2501   Class :character   1st Qu.:32.00  \n#>  Median : 5000   Mode  :character   Median :35.00  \n#>  Mean   : 5000                      Mean   :34.51  \n#>  3rd Qu.: 7500                      3rd Qu.:37.00  \n#>  Max.   :10000                      Max.   :45.00  \n#>     metric1      \n#>  Min.   :-13.54  \n#>  1st Qu.: 39.62  \n#>  Median : 49.94  \n#>  Mean   : 49.82  \n#>  3rd Qu.: 59.91  \n#>  Max.   :102.87\ndummyXLSX <- \n    dummyXLSX %>% \n    mutate(factorType = factor(type, \n                               levels = c('a', 'b', 'c'), \n                               labels = c('Diabetes', 'Heart disease', 'Control')))\nsummary(dummyXLSX)\n#>        id            type                age       \n#>  Min.   :    1   Length:10000       Min.   :23.00  \n#>  1st Qu.: 2501   Class :character   1st Qu.:32.00  \n#>  Median : 5000   Mode  :character   Median :35.00  \n#>  Mean   : 5000                      Mean   :34.51  \n#>  3rd Qu.: 7500                      3rd Qu.:37.00  \n#>  Max.   :10000                      Max.   :45.00  \n#>     metric1               factorType  \n#>  Min.   :-13.54   Diabetes     :3424  \n#>  1st Qu.: 39.62   Heart disease:3292  \n#>  Median : 49.94   Control      :3284  \n#>  Mean   : 49.82                       \n#>  3rd Qu.: 59.91                       \n#>  Max.   :102.87"},{"path":"lecture-2-data-wrangling.html","id":"filter","chapter":"3 Lecture 2: Data wrangling","heading":"3.4 Filter","text":"Now, say want restrict analysis participants scored 50 “test” dummyXLSX (variable metric1). useful tool filter() function dplyr allows us subset data based logic operations.Note half observations gone. can filter data even , example restricting analysis participants Diabetes Heart disease scored higher 50 metric1.resulting data even smaller. list operators can use compare values:>: Used \\(x > y\\) read x greater y.>: Used \\(x > y\\) read x greater y.<: Used \\(x < y\\) read x lesser y.<: Used \\(x < y\\) read x lesser y.>=: Used \\(x \\geq y\\) read x greater equal y>=: Used \\(x \\geq y\\) read x greater equal y<=: Used \\(x \\leq y\\) read x lesser equal y.<=: Used \\(x \\leq y\\) read x lesser equal y.==: Used \\(x = y\\) read x y. double == mistake.==: Used \\(x = y\\) read x y. double == mistake.small example ==:","code":"\ndummyXLSX %>%\n    filter(metric1 > 50)\n#> # A tibble: 4,978 × 5\n#>       id type    age metric1 factorType   \n#>    <dbl> <chr> <dbl>   <dbl> <fct>        \n#>  1     4 b        37    75.7 Heart disease\n#>  2     5 a        35    66.9 Diabetes     \n#>  3     6 c        37    64.8 Control      \n#>  4     7 c        40    52.0 Control      \n#>  5    13 a        33    62.7 Diabetes     \n#>  6    15 b        37    60.4 Heart disease\n#>  7    17 b        38    75.3 Heart disease\n#>  8    19 c        38    61.5 Control      \n#>  9    20 a        32    76.1 Diabetes     \n#> 10    21 c        32    61.0 Control      \n#> # … with 4,968 more rows\ndummyXLSX %>%\n    filter(metric1 > 50, type %in% c('a', 'b'))\n#> # A tibble: 3,328 × 5\n#>       id type    age metric1 factorType   \n#>    <dbl> <chr> <dbl>   <dbl> <fct>        \n#>  1     4 b        37    75.7 Heart disease\n#>  2     5 a        35    66.9 Diabetes     \n#>  3    13 a        33    62.7 Diabetes     \n#>  4    15 b        37    60.4 Heart disease\n#>  5    17 b        38    75.3 Heart disease\n#>  6    20 a        32    76.1 Diabetes     \n#>  7    23 a        31    54.4 Diabetes     \n#>  8    25 a        34    51.7 Diabetes     \n#>  9    26 b        29    57.0 Heart disease\n#> 10    30 b        32    54.7 Heart disease\n#> # … with 3,318 more rows\ndummyXLSX %>%\n    filter(factorType == 'Diabetes')\n#> # A tibble: 3,424 × 5\n#>       id type    age metric1 factorType\n#>    <dbl> <chr> <dbl>   <dbl> <fct>     \n#>  1     1 a        32    31.4 Diabetes  \n#>  2     3 a        41    40.4 Diabetes  \n#>  3     5 a        35    66.9 Diabetes  \n#>  4    12 a        30    34.8 Diabetes  \n#>  5    13 a        33    62.7 Diabetes  \n#>  6    14 a        35    49.6 Diabetes  \n#>  7    20 a        32    76.1 Diabetes  \n#>  8    22 a        36    49.0 Diabetes  \n#>  9    23 a        31    54.4 Diabetes  \n#> 10    24 a        35    33.2 Diabetes  \n#> # … with 3,414 more rows"},{"path":"lecture-2-data-wrangling.html","id":"ifelse-and-case_when","chapter":"3 Lecture 2: Data wrangling","heading":"3.5 ifelse and case_when","text":"Assume want create new variable participants scored higher 50 metric1 labeled high scored lower equal 50 labeled low. can use ifelse() function end. First example dissection.ifelse() work? function takes three inputs: test want perform (metric1 score higher 50?), action test true (High), action test false (metric1 \\(\\leq\\) 50; Low). can create complicated tests & | operators symbolize respectively.Assume want create variable combines multiple scenarios. possible nest ifelse() within () quickly becomes obnoxious confusing. case_when() comes allows us strictly define value new variable takes based scenario. one:","code":"\ndummyXLSX <-\n    dummyXLSX %>%\n    mutate(scoreCat = ifelse(metric1 > 50, 'High', 'Low'))\ndummyXLSX\n#> # A tibble: 10,000 × 6\n#>       id type    age metric1 factorType    scoreCat\n#>    <dbl> <chr> <dbl>   <dbl> <fct>         <chr>   \n#>  1     1 a        32    31.4 Diabetes      Low     \n#>  2     2 c        33    42.4 Control       Low     \n#>  3     3 a        41    40.4 Diabetes      Low     \n#>  4     4 b        37    75.7 Heart disease High    \n#>  5     5 a        35    66.9 Diabetes      High    \n#>  6     6 c        37    64.8 Control       High    \n#>  7     7 c        40    52.0 Control       High    \n#>  8     8 b        34    35.7 Heart disease Low     \n#>  9     9 b        36    30.2 Heart disease Low     \n#> 10    10 c        36    34.5 Control       Low     \n#> # … with 9,990 more rows\ndummyXLSX <-\n    dummyXLSX %>%\n    mutate(scoreCatAge = ifelse(metric1 > 50 & age > 30, 'Both true', 'One or both false'))\ndummyXLSX %>%\n    select(age, metric1, scoreCatAge)\n#> # A tibble: 10,000 × 3\n#>      age metric1 scoreCatAge      \n#>    <dbl>   <dbl> <chr>            \n#>  1    32    31.4 One or both false\n#>  2    33    42.4 One or both false\n#>  3    41    40.4 One or both false\n#>  4    37    75.7 Both true        \n#>  5    35    66.9 Both true        \n#>  6    37    64.8 Both true        \n#>  7    40    52.0 Both true        \n#>  8    34    35.7 One or both false\n#>  9    36    30.2 One or both false\n#> 10    36    34.5 One or both false\n#> # … with 9,990 more rows\ndummyXLSX %>%\n    mutate(orStatement = ifelse(metric1 > 50 | age < 30, 'One or both true', 'Both false')) %>%\n    select(age, metric1, orStatement)\n#> # A tibble: 10,000 × 3\n#>      age metric1 orStatement     \n#>    <dbl>   <dbl> <chr>           \n#>  1    32    31.4 Both false      \n#>  2    33    42.4 Both false      \n#>  3    41    40.4 Both false      \n#>  4    37    75.7 One or both true\n#>  5    35    66.9 One or both true\n#>  6    37    64.8 One or both true\n#>  7    40    52.0 One or both true\n#>  8    34    35.7 Both false      \n#>  9    36    30.2 Both false      \n#> 10    36    34.5 Both false      \n#> # … with 9,990 more rows\ndummyXLSX %>%\n    mutate(scenarios = case_when(age > 35 & metric1 < 50 & type == 'c' ~ 'Control, > 35, < 50',\n                                 age > 35 & metric1 > 50 & type == 'b' ~ 'Heart, > 35, > 50',\n                                 age <= 35 & metric1 <= 50 & type == 'a' ~ 'Diabetes, <= 35, <= 50',\n                                 TRUE ~ 'Whatever does not fit')) %>% \n    select(age, metric1, type, scenarios)\n#> # A tibble: 10,000 × 4\n#>      age metric1 type  scenarios             \n#>    <dbl>   <dbl> <chr> <chr>                 \n#>  1    32    31.4 a     Diabetes, <= 35, <= 50\n#>  2    33    42.4 c     Whatever does not fit \n#>  3    41    40.4 a     Whatever does not fit \n#>  4    37    75.7 b     Heart, > 35, > 50     \n#>  5    35    66.9 a     Whatever does not fit \n#>  6    37    64.8 c     Whatever does not fit \n#>  7    40    52.0 c     Whatever does not fit \n#>  8    34    35.7 b     Whatever does not fit \n#>  9    36    30.2 b     Whatever does not fit \n#> 10    36    34.5 c     Control, > 35, < 50   \n#> # … with 9,990 more rows"},{"path":"lecture-2-data-wrangling.html","id":"summarize-and-group_by","chapter":"3 Lecture 2: Data wrangling","heading":"3.6 summarize and group_by","text":"preliminary analysis can divided two (roughly) parts: numerical analysis graphical analysis.\nCommon statistics asked compute sample size, mean, standard deviation, minimum, maximum, quantiles (typically 2.5% 97.5% percentiles) sample. end following functions: n(), mean(), sd(), min(), max(), quantile(). functions take numerical vector return statistic interest. discuss statistics detail next week.Let’s compute statistics variable age:alternative way use summarize() function:methods yield answer. Now, imagine want compute statistics stratified type patient. can done base R function recommend using summarize() function group_by() function. put simply, group_by() function allows us define groups “wrangle” based groups. Let’s see happens.code basically save one additional line. Pretty convenient, right?","code":"\nnrow(dummyXLSX)\n#> [1] 10000\nmean(dummyXLSX$age)\n#> [1] 34.5138\nsd(dummyXLSX$age)\n#> [1] 3.182487\nmin(dummyXLSX$age)\n#> [1] 23\nmax(dummyXLSX$age)\n#> [1] 45\nquantile(dummyXLSX$age, probs = 0.025)\n#> 2.5% \n#>   28\nquantile(dummyXLSX$age, probs = 0.975)\n#> 97.5% \n#>    41\ndummyXLSX %>%\n    summarize(sampleSize = n(), \n              mean = mean(age), \n              standardDev = sd(age), \n              mini = min(age), \n              maxi = max(age), \n              q025 = quantile(age, 0.025), \n              q975 = quantile(age, 0.975))\n#> # A tibble: 1 × 7\n#>   sampleSize  mean standardDev  mini  maxi  q025  q975\n#>        <int> <dbl>       <dbl> <dbl> <dbl> <dbl> <dbl>\n#> 1      10000  34.5        3.18    23    45    28    41\ndummyXLSX %>% \n    group_by(type) %>%\n    summarize(sampleSize = n(), \n              mean = mean(age), \n              standardDev = sd(age), \n              mini = min(age), \n              maxi = max(age), \n              q025 = quantile(age, 0.025), \n              q975 = quantile(age, 0.975))\n#> # A tibble: 3 × 8\n#>   type  sampleSize  mean standardDev  mini  maxi  q025  q975\n#>   <chr>      <int> <dbl>       <dbl> <dbl> <dbl> <dbl> <dbl>\n#> 1 a           3424  34.5        3.17    23    45    28    41\n#> 2 b           3292  34.5        3.16    24    45    28    41\n#> 3 c           3284  34.5        3.21    24    45    28    41"},{"path":"lecture-2-data-wrangling.html","id":"gather-and-spread","chapter":"3 Lecture 2: Data wrangling","heading":"3.7 gather and spread","text":"Data can long wide. data wide one column variable. Imagine conducted study took measurements twice research period. data look something like :Now, might actually beneficial measurement row. measurement row say data long. convert wide long use gather() function. actually superseeded pivot_longer() ’m used gather(). function gather() takes : names two new columns creating (“key”, “value”) selection columns. want measurement line selection measure1 measure2.Alternatively can just use - prefix tell R columns ignore:go long wide function spread() pivot_wider(). encourage try .","code":"\nwideExample <- \n    tibble(id = 1:3, \n       measure1 = rnorm(3), \n       measure2 = rnorm(3))\nwideExample\n#> # A tibble: 3 × 3\n#>      id measure1 measure2\n#>   <int>    <dbl>    <dbl>\n#> 1     1    -1.00   -0.479\n#> 2     2     1.47   -1.49 \n#> 3     3    -1.31    0.214\nwideExample %>%\n    gather(measurement, value, measure1, measure2)\n#> # A tibble: 6 × 3\n#>      id measurement  value\n#>   <int> <chr>        <dbl>\n#> 1     1 measure1    -1.00 \n#> 2     2 measure1     1.47 \n#> 3     3 measure1    -1.31 \n#> 4     1 measure2    -0.479\n#> 5     2 measure2    -1.49 \n#> 6     3 measure2     0.214\nwideExample %>%\n    gather(measurement, value, -id)\n#> # A tibble: 6 × 3\n#>      id measurement  value\n#>   <int> <chr>        <dbl>\n#> 1     1 measure1    -1.00 \n#> 2     2 measure1     1.47 \n#> 3     3 measure1    -1.31 \n#> 4     1 measure2    -0.479\n#> 5     2 measure2    -1.49 \n#> 6     3 measure2     0.214"},{"path":"lecture-2-data-wrangling.html","id":"why-this-is-useful","chapter":"3 Lecture 2: Data wrangling","heading":"3.7.1 Why this is useful","text":"long format often useful want fit models repeated measurements. ’s also useful want summarize multiple variables simultaneously. Recall summarize() section summarized age. many variables might interested summarizing metric1. example combine gather(), group_by(), summarize().another example include type participants.","code":"\n# Let's add another variable to dummyXLSX\ndummyXLSX %>%\n    mutate(someVar = rnorm(n = n())) %>%\n    gather(variables, values, age, metric1, someVar) %>%\n    group_by(variables) %>%\n    summarize(mean = mean(values), \n              sd = sd(values),\n              median = median(values))\n#> # A tibble: 3 × 4\n#>   variables      mean    sd   median\n#>   <chr>         <dbl> <dbl>    <dbl>\n#> 1 age       34.5       3.18 35      \n#> 2 metric1   49.8      15.0  49.9    \n#> 3 someVar    0.000478  1.00 -0.00834\ndummyXLSX %>%\n    mutate(someVar = rnorm(n = n())) %>%\n    gather(variables, values, age, metric1, someVar) %>%\n    group_by(variables, type) %>%\n    summarize(mean = mean(values), \n              sd = sd(values),\n              median = median(values)) %>%\n    arrange(type)\n#> `summarise()` has grouped output by 'variables'. You can override using the `.groups` argument.\n#> # A tibble: 9 × 5\n#> # Groups:   variables [3]\n#>   variables type     mean     sd  median\n#>   <chr>     <chr>   <dbl>  <dbl>   <dbl>\n#> 1 age       a     34.5     3.17  35     \n#> 2 metric1   a     49.5    14.9   49.5   \n#> 3 someVar   a      0.0427  0.998  0.0463\n#> 4 age       b     34.5     3.16  34     \n#> 5 metric1   b     50.1    15.1   50.2   \n#> 6 someVar   b      0.0172  1.00   0.0432\n#> 7 age       c     34.5     3.21  35     \n#> 8 metric1   c     49.8    15.2   50.1   \n#> 9 someVar   c     -0.0325  1.01  -0.0338"},{"path":"lecture-2-data-wrangling.html","id":"appendix","chapter":"3 Lecture 2: Data wrangling","heading":"3.8 Appendix","text":"","code":"\nlibrary(here)\nlibrary(xlsx)\n\n# Set seed for reproducibility\nset.seed(1)\n\nn <- 10000\n\n# Create fake data with research ID, \n# participant type, age, and performance\n# metric score\n\nid <- 1:n\ntype <- sample(x = c('a', 'b', 'c'), size = n, replace = T)\nage <- floor(rnorm(n = n, mean = 35, sd = sqrt(10)))\nmetric1 <- rnorm(n = n, mean = 50, sd = 15)\n\nd <- data.frame(id, type, age, metric1)\n\n# CSV\nwrite.table(x = d, file = here('data', 'l2_data_csv.txt'), \n            row.names = F, quote = F, sep = ',')\n# CSV2\nwrite.table(x = d, file = here('data', 'l2_data_csv2.txt'), \n            row.names = F, quote = F, sep = ';')\n# TSV\nwrite.table(x = d, file = here('data', 'l2_data_tsv.txt'), \n            row.names = F, quote = F, sep = '\\t')\n# Excel\nwrite.xlsx(x = d, file = here('data', 'l2_data_excel.xlsx'), \n           row.names = F, sheetName = 'DummyData')"},{"path":"lecture-3-plots-and-table1.html","id":"lecture-3-plots-and-table1","chapter":"4 Lecture 3: Plots and table1","heading":"4 Lecture 3: Plots and table1","text":"begin loading data:data pulse based Icelandic data set pulse students measured two different time points. two groups; control group stationary two measurements, case group active two measurements. original data Icelandic translate columns levels factor variables. also omitted variables simplicity’s sake. curious translation can look Appendix bottom document.goal today’s lecture compare two groups. graphically tables. begin using glimpse() function get brief overview data.see three chr variables (smokes, drinks, intervention) want cast factor variables. Furthermore, even though sex dbl cast factor variable well.done , now use summary() function get quick overview data:first time see missing values. Note NA’s height, weight, firstPulse, secondPulse, . Missing values can imputed beyond scope course. Instead, just going remove data na.omit() function.Let’s use summary() function see changed:NA’s gone cost reduced data set. conduct research important try minimize missing values affects power study.","code":"\nlibrary(tidyverse)  # General data wrangling & plots\nlibrary(knitr)      # Tables \nlibrary(kableExtra) # Fancy tables\nlibrary(cowplot)    # For pretty plots\nlibrary(table1)     # Table1\nlibrary(here)       # paths\npulse <- read_csv2(\"https://notendur.hi.is/thj73/data/pulseEn.csv\")\npulse\n#> # A tibble: 471 × 10\n#>       id height weight   age   sex smokes drinks firstPulse\n#>    <dbl>  <dbl>  <dbl> <dbl> <dbl> <chr>  <chr>       <dbl>\n#>  1     1    161     60    23     1 no     no             83\n#>  2     2    185    115    52     2 <NA>   yes            80\n#>  3     3    167     NA    22     1 no     yes            43\n#>  4     4    174     67    21     1 no     yes            76\n#>  5     5    163     57    20     1 no     yes            71\n#>  6     6    175     59    20     1 no     yes            65\n#>  7     7    178     70    39     1 <NA>   yes            77\n#>  8     8    191     94    21     2 no     yes            79\n#>  9     9    176     68    20     1 no     yes            73\n#> 10    10    176     82    70     2 no     yes            65\n#> # … with 461 more rows, and 2 more variables:\n#> #   secondPulse <dbl>, intervention <chr>\nglimpse(pulse)\n#> Rows: 471\n#> Columns: 10\n#> $ id           <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12…\n#> $ height       <dbl> 161, 185, 167, 174, 163, 175, 178, 19…\n#> $ weight       <dbl> 60, 115, NA, 67, 57, 59, 70, 94, 68, …\n#> $ age          <dbl> 23, 52, 22, 21, 20, 20, 39, 21, 20, 7…\n#> $ sex          <dbl> 1, 2, 1, 1, 1, 1, 1, 2, 1, 2, 2, 2, 1…\n#> $ smokes       <chr> \"no\", NA, \"no\", \"no\", \"no\", \"no\", NA,…\n#> $ drinks       <chr> \"no\", \"yes\", \"yes\", \"yes\", \"yes\", \"ye…\n#> $ firstPulse   <dbl> 83, 80, 43, 76, 71, 65, 77, 79, 73, 6…\n#> $ secondPulse  <dbl> 84, 103, 52, 105, 68, 65, 75, 83, 90,…\n#> $ intervention <chr> \"stationary\", \"active\", \"stationary\",…\npulse <-\n    pulse %>% \n    mutate(sex = factor(sex), \n           smokes = factor(smokes), \n           drinks = factor(drinks), \n           intervention = factor(intervention, \n                                 levels = c('stationary', 'active')))\nsummary(pulse)\n#>        id            height           weight      \n#>  Min.   :  1.0   Min.   : 150.0   Min.   : 40.00  \n#>  1st Qu.:118.5   1st Qu.: 166.0   1st Qu.: 60.00  \n#>  Median :236.0   Median : 172.0   Median : 70.00  \n#>  Mean   :236.0   Mean   : 179.6   Mean   : 85.38  \n#>  3rd Qu.:353.5   3rd Qu.: 181.0   3rd Qu.: 81.00  \n#>  Max.   :471.0   Max.   :1725.0   Max.   :846.00  \n#>                  NA's   :1        NA's   :10      \n#>       age        sex      smokes     drinks   \n#>  Min.   :19.00   1:307   no  :412   no  : 76  \n#>  1st Qu.:20.00   2:164   yes : 46   yes :389  \n#>  Median :22.00           NA's: 13   NA's:  6  \n#>  Mean   :24.24                                \n#>  3rd Qu.:25.00                                \n#>  Max.   :70.00                                \n#>                                               \n#>    firstPulse      secondPulse         intervention\n#>  Min.   : 42.00   Min.   : 42.00   stationary:286  \n#>  1st Qu.: 64.00   1st Qu.: 68.00   active    :183  \n#>  Median : 71.50   Median : 77.00   NA's      :  2  \n#>  Mean   : 71.98   Mean   : 82.21                   \n#>  3rd Qu.: 80.00   3rd Qu.: 93.00                   \n#>  Max.   :120.00   Max.   :162.00                   \n#>  NA's   :17       NA's   :14\npulse <- \n    pulse %>%\n    na.omit()\nsummary(pulse)\n#>        id            height           weight      \n#>  Min.   :  1.0   Min.   : 150.0   Min.   : 40.00  \n#>  1st Qu.:116.2   1st Qu.: 165.0   1st Qu.: 60.00  \n#>  Median :237.5   Median : 172.0   Median : 70.00  \n#>  Mean   :237.1   Mean   : 180.3   Mean   : 84.89  \n#>  3rd Qu.:358.8   3rd Qu.: 181.0   3rd Qu.: 81.00  \n#>  Max.   :471.0   Max.   :1725.0   Max.   :846.00  \n#>       age        sex     smokes    drinks   \n#>  Min.   :19.00   1:275   no :384   no : 65  \n#>  1st Qu.:20.00   2:151   yes: 42   yes:361  \n#>  Median :22.00                              \n#>  Mean   :23.92                              \n#>  3rd Qu.:25.00                              \n#>  Max.   :70.00                              \n#>    firstPulse      secondPulse         intervention\n#>  Min.   : 43.00   Min.   : 45.00   stationary:257  \n#>  1st Qu.: 64.00   1st Qu.: 68.00   active    :169  \n#>  Median : 72.00   Median : 78.00                   \n#>  Mean   : 72.21   Mean   : 82.59                   \n#>  3rd Qu.: 80.00   3rd Qu.: 94.75                   \n#>  Max.   :120.00   Max.   :162.00"},{"path":"lecture-3-plots-and-table1.html","id":"table-1","chapter":"4 Lecture 3: Plots and table1","heading":"4.1 Table 1","text":"important table create table 1. Table 1 contains descriptive statistics cohort, preferably stratified case/control, necessarily. called table 1 often first table see paper.many ways create table 1 discuss table1 package. Using table1 pretty simple. begin writing variables interested tell function stratify group interest. us, intervention variable one want stratify .see good summary data: means, ranges, standard deviations, number observations, counts percentages. examining table get good feel data relatively short time. Can see difference two groups? see anything weird table?","code":"\ntable1(~ firstPulse + secondPulse + height + weight + age + sex + smokes + drinks | intervention, data = pulse)"},{"path":"lecture-3-plots-and-table1.html","id":"note-using-tidyverse-functions","chapter":"4 Lecture 3: Plots and table1","heading":"4.1.1 Note: using tidyverse functions","text":"actually possible recreate table group_by(), summarize(), . ’s quite involved want challenge encourage try .","code":""},{"path":"lecture-3-plots-and-table1.html","id":"summarizing-the-data-with-plots","chapter":"4 Lecture 3: Plots and table1","heading":"4.2 Summarizing the data with plots","text":"Tables good picture worth thousand words; ’s time create plots.","code":""},{"path":"lecture-3-plots-and-table1.html","id":"scatter-plot-or-geom_point","chapter":"4 Lecture 3: Plots and table1","heading":"4.2.1 Scatter plot or geom_point()","text":"seen one . Let’s plot values firstPulse secondPulse color points based whether observation active stationary group.lot information simple graph. see :stationary group, higher starting pulse, higher second pulse (general).stationary group, higher starting pulse, higher second pulse (general).active group much higher second pulse.active group much higher second pulse.One individual stationary group much higher second pulse relative starting pulse. ?One individual stationary group much higher second pulse relative starting pulse. ?plot nice invest time making aesthetically pleasing. pick sharper colors, relabel axes, move legend bottom graph. also load theme plot cowplots package.Note “aesthetically pleasing” relative.","code":"\npulse %>%\n    ggplot(aes(x = firstPulse, y = secondPulse, color = intervention)) +\n    geom_point()\npulse %>%\n    ggplot(aes(x = firstPulse, y = secondPulse, color = intervention)) +\n    geom_point() +\n    scale_color_brewer(type = 'seq', palette = 'Set1') + # Change color\n    labs(x = 'First measurement', \n         y = 'Second measurement', \n         color = 'Intervention') + # Change labels on axes, change legend\n    theme_cowplot() +    # change the theme\n    theme(legend.position = 'bottom')"},{"path":"lecture-3-plots-and-table1.html","id":"data-distribution-plots-or-geom_histogram","chapter":"4 Lecture 3: Plots and table1","heading":"4.2.2 Data distribution plots or geom_histogram()","text":"multiple continuous variables play . Let’s look distribution pulse measurements.look first picture see “mass” 60 80. agreement saw table 1. second picture shows distribution bimodal; , seem two peaks. also consistent seen two groups active group much larger second pulse measurement.Maybe color second histogram using intervention variable can better see two peaks.better invest time cleaning graph. repeat steps scatter plot one addition; make colors little transparent alpha parameter two histograms overlap .","code":"\npulse %>%\n    ggplot(aes(x = firstPulse)) +\n    geom_histogram()\n#> `stat_bin()` using `bins = 30`. Pick better value with\n#> `binwidth`.\npulse %>%\n    ggplot(aes(x = secondPulse)) +\n    geom_histogram()\n#> `stat_bin()` using `bins = 30`. Pick better value with\n#> `binwidth`.\npulse %>%\n    ggplot(aes(x = secondPulse, fill = intervention)) +\n    geom_histogram()\n#> `stat_bin()` using `bins = 30`. Pick better value with\n#> `binwidth`.\npulse %>%\n    ggplot(aes(x = secondPulse, fill = intervention)) +\n    geom_histogram(alpha = 0.5, position = 'identity') +\n    scale_fill_brewer(type = 'seq', palette = 'Set1') +\n    labs(x = 'Second measurement', \n         fill = 'Intervention') +\n    theme_cowplot() +\n    theme(legend.position = 'bottom')\n#> `stat_bin()` using `bins = 30`. Pick better value with\n#> `binwidth`."},{"path":"lecture-3-plots-and-table1.html","id":"geom_boxplot-as-an-alternative-to-geom_histogram","chapter":"4 Lecture 3: Plots and table1","heading":"4.2.3 geom_boxplot() as an alternative to geom_histogram()","text":"Another important plot compares distribution two variables boxplot. boxplot shows us range data, outliers, median.basically contains information histogram. Can see ?","code":"\npulse %>%\n    ggplot(aes(x = intervention, y = secondPulse)) +\n    geom_boxplot()"},{"path":"lecture-3-plots-and-table1.html","id":"geom_density-as-an-alternative-to-geom_histogram-and-geom_boxplot","chapter":"4 Lecture 3: Plots and table1","heading":"4.2.4 geom_density() as an alternative to geom_histogram() and geom_boxplot()","text":"another (essentially) version previous two plots.","code":"\npulse %>%\n    ggplot(aes(x = secondPulse, fill = intervention)) +\n    geom_density(alpha = 0.5) +\n    scale_fill_brewer(type = 'seq', palette = 'Set1') +\n    theme_cowplot() +\n    labs(x = 'Second measurement', \n         fill = 'Intervention') + \n    theme(legend.position = 'bottom')"},{"path":"lecture-3-plots-and-table1.html","id":"barplots-for-discrete-variables-geom_bar","chapter":"4 Lecture 3: Plots and table1","heading":"4.2.5 Barplots for discrete variables geom_bar()","text":"far paying attention continuous variable. Let’s turn attention discrete ones. discrete variables sex, smoker, drinks intervention.Let’s plot barplot sex:\nhonestly pretty boring plot. can spice things little adding intervention variable.bars stacked top annoying. better side--side.course can improve aesthetics graph like ones.","code":"\npulse %>%\n    ggplot(aes(x = sex)) +\n    geom_bar()\npulse %>%\n    ggplot(aes(x = sex, fill = intervention)) +\n    geom_bar()\npulse %>%\n    ggplot(aes(x = sex, fill = intervention)) +\n    geom_bar(position = 'dodge')"},{"path":"lecture-3-plots-and-table1.html","id":"splitting-plots-into-facets-with-facet_grid","chapter":"4 Lecture 3: Plots and table1","heading":"4.2.6 Splitting plots into facets with facet_grid()","text":"Recall scatter plot :Instead points static activity together can split plot two. code needs modified little ’s much work.group gets plot. Sometimes ’s easier compare plots way.","code":"\npulse %>%\n    ggplot(aes(x = firstPulse, y = secondPulse, color = intervention)) +\n    geom_point() +\n    scale_color_brewer(type = 'seq', palette = 'Set1') + # Change color\n    labs(x = 'First measurement', \n         y = 'Second measurement', \n         color = 'Intervention') + # Change labels on axes, change legend\n    theme_cowplot() +    # change the theme\n    theme(legend.position = 'bottom')\npulse %>%\n    ggplot(aes(x = firstPulse, y = secondPulse)) +\n    geom_point() +\n    labs(x = 'First measurement', \n         y = 'Second measurement') + # Change labels on axes, change legend +\n    facet_grid(~intervention, scales = 'free') +\n    theme_cowplot() +    # change the theme\n    theme(legend.position = 'bottom')"},{"path":"lecture-3-plots-and-table1.html","id":"saving-our-plots","chapter":"4 Lecture 3: Plots and table1","heading":"4.3 Saving our plots","text":"use ggsave() function save plots locally. save plot need store object memory. Let’s save scatter plot.","code":"\np <- \n    pulse %>%\n    ggplot(aes(x = firstPulse, y = secondPulse, color = intervention)) +\n    geom_point() +\n    scale_color_brewer(type = 'seq', palette = 'Set1') + # Change color\n    labs(x = 'First measurement', \n         y = 'Second measurement', \n         color = 'Intervention') + # Change labels on axes, change legend\n    theme_cowplot() +    # change the theme\n    theme(legend.position = 'bottom')\nggsave(filename = here('img', 'scatterPlot.png'), plot = p, width = 8, height = 6, dpi = 320)"},{"path":"lecture-3-plots-and-table1.html","id":"appendix-1","chapter":"4 Lecture 3: Plots and table1","heading":"4.4 Appendix","text":"","code":"\nlibrary(tidyverse)\nlibrary(here)\n\npuls <- read_csv2(\"https://edbook.hi.is/gogn/pulsAll.csv\")\n\npuls2 <-\n    puls %>%\n    rename(course = namskeid, \n           coin = kronukast, \n           height = haed, \n           weight = thyngd, \n           age = aldur,\n           sex = kyn, \n           smokes = reykir,\n           drinks = drekkur, \n           gym = likamsraekt, \n           firstPulse = fyrriPuls,\n           secondPulse = seinniPuls, \n           intervention = inngrip,\n           date = dagsetning) %>%\n    select(height, weight, age, sex, smokes, \n           drinks, firstPulse, secondPulse, intervention) %>%\n    mutate(smokes = case_when(smokes == 'ja' ~ 'yes', \n                              smokes == 'nei' ~ 'no'), \n           drinks = case_when(drinks == 'ja' ~ 'yes',\n                              drinks == 'nei' ~ 'no'), \n           intervention = case_when(intervention == 'hljop' ~ 'active', \n                                    intervention == 'sat_kyrr' ~ 'stationary'), \n           id = 1:nrow(puls)) %>%\n    relocate(id, .before = 'height')\n\nwrite.table(x = puls2, file = here('data', 'pulseEn.csv'), sep = ';', row.names = F)"},{"path":"lecture-4-probability-theory.html","id":"lecture-4-probability-theory","chapter":"5 Lecture 4: Probability theory","heading":"5 Lecture 4: Probability theory","text":"Statistics based probability theory. must therefore learn little probability theory can learn statistics.","code":""},{"path":"lecture-4-probability-theory.html","id":"the-sample-space","chapter":"5 Lecture 4: Probability theory","heading":"5.1 The sample space","text":"Imagine simple experiment toss coin . coin can come heads tails. therefore say experiment two possible outcomes, viz. heads tails. collection (set) possible outcomes experiment called sample space. coin experiment, sample space collection (set) two elements, heads tails. can denote sample space coin toss mathematically :\\[\nS = \\{\\mbox{Heads}, \\mbox{Tails}\\}.\n\\]Another experiment casting die . die can come 1, 2, 3, 4, 5, 6. sample space \\(S\\) can thus written:\\[\nS = \\{1, 2, 3, 4, 5, 6\\}.\n\\], \\(S\\) six elements. experiments, sample space finite. finite amount possible outcomes.","code":""},{"path":"lecture-4-probability-theory.html","id":"discrete-and-continuous","chapter":"5 Lecture 4: Probability theory","heading":"5.1.1 Discrete and continuous","text":"experiments sample space discrete. discrete can “enumerate” options - give indexes. coin experiment, enumeration follows:\\[\n\\begin{array}{c}\n\\mbox{Index} & \\mbox{Element} \\\\\n1 & H \\\\\n2 & T\n\\end{array}\n\\]die example even simpler.\\[\n\\begin{array}{c}\n\\mbox{Index} & \\mbox{Element} \\\\\n1 & 1 \\\\\n2 & 2 \\\\\n3 & 3 \\\\\n4 & 4 \\\\\n5 & 5 \\\\\n6 & 6\n\\end{array}\n\\]’s say discreteness implies finiteness. imagine die “infinite” faces can still enumerate options.Now, imagine another experiment want measure temperature glass water boiled. measure temperature given time temperature can anywhere 20\\(^\\circ\\)C 100\\(^\\circ\\)C. say, \\(S = [20, 100]\\). case actually can’t enumerate possible values may seem counterintuitive. case, say sample space continuous; exists continuum can smoothly go values.feels fuzzy unclear . continuity uncountability real numbers tough grasp required decades mathematical work made rigorous.","code":""},{"path":"lecture-4-probability-theory.html","id":"random-variables","chapter":"5 Lecture 4: Probability theory","heading":"5.2 Random variables","text":"need way talk outcome experiment perform . Let’s us denote outcome experiment perform \\(X\\). refer \\(X\\) random variable. make things conecrete let’s return coin-tossing experiment. can write:\\[\nX = \\mbox{outcome coin toss}.\n\\]Thinking back sample space \\(S = \\{H, T\\}\\) (H Heads, T Tails) see random variable \\(X\\) can take two possible values: \\(X = H\\) \\(X = T\\). therefore say random variable \\(X\\) discrete. \\(X\\) take value existing continuum, call \\(X\\) continuous random variable (think temperature experiment).","code":""},{"path":"lecture-4-probability-theory.html","id":"probability-of-an-outcome","chapter":"5 Lecture 4: Probability theory","heading":"5.3 Probability of an outcome","text":"defined random variables values can take finally position talk probability random variable \\(X\\) taking value \\(x\\). Note difference upper-case \\(X\\) lower-case \\(x\\). distinction important! \\(X\\) random variable \\(x\\) observe; \\(x\\) random variable. die-casting experiment \\(X\\) outcome toss die. toss die, let’s assume comes 6, say \\(X = 6\\).Now, likely see 6 outcome experiment? Assuming die unbiased, expect 1--6 chance observe 6. understand recall sample space experiment. \\(S = \\{1, 2, 3, 4, 5, 6\\}\\). six elements assume one likely appear . can written mathematicall :\\[\nP(X = x) = \\frac16.\n\\]equation can parsed English follows: probability (\\(P\\)) random varible \\(X\\) takes value \\(x\\) (\\(P(X = x)\\)) one six (\\(P(X = x) = 1/6\\)). Note doesn’t matter \\(x\\) ; probability always \\(1/6\\).“equal spreading” probability naive definition probability. give potential outcome equal weight. \\(|S|\\) number elements sample space (6 die) equal probability :\\[\nP(X = x) = \\frac{1}{|S|}\n\\]Note becomes problem soon allow infinite sample spaces. Thinking temperature example, number elements closed interval \\([20, 100]\\) infinite, probability picking specific value \\(x\\) random 0. level therefore stick two “rules”:\\(X\\) discrete, allow \\(P(X = x)\\).\\(X\\) discrete, allow \\(P(X = x)\\).\\(X\\) continuous need consider interval around \\(x\\). , \\(P(< X < b)\\) reads “probability \\(X\\) \\(\\) \\(b\\)”.\\(X\\) continuous need consider interval around \\(x\\). , \\(P(< X < b)\\) reads “probability \\(X\\) \\(\\) \\(b\\)”.","code":""},{"path":"lecture-4-probability-theory.html","id":"the-probability-massdensity-function","chapter":"5 Lecture 4: Probability theory","heading":"5.3.1 The probability mass/density function","text":"probability mass function \\(f(x)\\) gives us probability discrete random variable\\(X\\) takes value \\(x\\). already seen explicit form \\(f(x)\\):\\[\nf(x) = P(X = x).\n\\]probability mass function two properties: \\(f(x) \\geq 0\\) \\(\\sum _{x \\S} f(x) = 1\\). plot probability mass function die:bars height outcome just likely. involved example die biased. write:\\[\n\\begin{aligned}\n&P(X = 1) = 1/10, \\quad P(X = 2) = 2/10, \\quad P(X = 3) = 3/10 \\\\ \n&P(X = 4) = 2/10, \\quad P(X = 5) = 2/10, \\quad P(X = 6) = 0/10\n\\end{aligned}\n\\]can see, probabilities greater equal 0 add 1. Graphically, probability mass function :bars longer equal height probabilities longer equal.\\(X\\) continuous random variable speak probability density function.","code":""},{"path":"lecture-4-probability-theory.html","id":"the-cumulative-distribution-function","chapter":"5 Lecture 4: Probability theory","heading":"5.3.2 The cumulative distribution function","text":"cumulative distribution function random variable \\(X\\) :\\[\nF(x) = P(X \\leq x).\n\\]can parsed : probability random variable \\(X\\) taking value less equal \\(x\\).exists connection CDF density/mass functions. mass function gives us probability specific outcome CDF gives us cumulative probability specific event. Compare graph CDF probability mass function graph example .many continuous random variables exists explicit form CDF must thus rely density function.","code":""},{"path":"lecture-4-probability-theory.html","id":"properties-of-random-variables","chapter":"5 Lecture 4: Probability theory","heading":"5.4 Properties of random variables","text":"","code":""},{"path":"lecture-4-probability-theory.html","id":"independence","chapter":"5 Lecture 4: Probability theory","heading":"5.4.1 Independence","text":"say two random variables \\(X\\) \\(Y\\) independent outcome one random variable effect outcome random variable. example tossing two coins. outcome one toss influence outcome toss.two random variables independent say dependent.","code":""},{"path":"lecture-4-probability-theory.html","id":"identically-distributed","chapter":"5 Lecture 4: Probability theory","heading":"5.4.2 Identically distributed","text":"Let \\(X_1, X_2, \\ldots , X_n\\) collection random variables. say identically distributed probability distribution.","code":""},{"path":"lecture-4-probability-theory.html","id":"expected-value","chapter":"5 Lecture 4: Probability theory","heading":"5.4.3 Expected value","text":"expected value random variable \\(X\\) denoted \\(E(X)\\). can think average \\(X\\). calculate expected value \\(X\\) summing product potential outcomes \\(X\\) probability outcome. example make clearer. Let’s return die \\(X\\) outcome toss. Since outcome equally likely expected value \\(X\\) simply:\\[\n\\begin{aligned}\nE(X) &= \\sum _{x \\S} x P(X = x) \\\\\n&= 1 \\cdot P(X = 1) + 2 \\cdot P(X = 2) + \\ldots + 6 \\cdot P(X = 6) \\\\\n&= (1 + 2 + 3 + 4 + 5 + 6) \\cdot \\frac16 \\\\\n&=3.5\n\\end{aligned}\n\\]expected value linear operator. Let \\(X\\) \\(Y\\) random variables assume \\(E(X) = \\mu _X\\) \\(E(Y) = \\mu _Y\\). expected value \\(X\\) \\(Y\\) :\n\\[\nE(X + Y) = E(X) + E(Y) = \\mu _X + \\mu _Y\n\\]Furthermore, \\(\\) constant \\[\nE(aX) = aE(X) = \\mu_X.\n\\]important result expected value random variable \\(X\\). called law large numbers states sample \\(X\\) grows, closer mean sample expected value \\(X\\). Let’s toss die bunch times take average throws. plot shows average sample function sample size. red line expected value \\(X\\) calculated 3.5. can see, often throw die, closer calculated sample mean theoretical expected value.","code":""},{"path":"lecture-4-probability-theory.html","id":"variance","chapter":"5 Lecture 4: Probability theory","heading":"5.4.4 Variance","text":"variance random variable \\(X\\) tells us “dispersed” random variable expected value denoted \\(Var(X)\\). higher variance, bigger dispersion. plot two samples mean different variance.\nsquare root variance called standard deviation often denoted \\(\\sigma\\).","code":""},{"path":"lecture-4-probability-theory.html","id":"probability-distributions","chapter":"5 Lecture 4: Probability theory","heading":"5.5 Probability distributions","text":"probability distribution random variable \\(X\\) completely describes probabilities outcomes \\(X\\)","code":""},{"path":"lecture-4-probability-theory.html","id":"examples-of-discrete-distributions","chapter":"5 Lecture 4: Probability theory","heading":"5.6 Examples of discrete distributions","text":"","code":""},{"path":"lecture-4-probability-theory.html","id":"bernoulli-distribution","chapter":"5 Lecture 4: Probability theory","heading":"5.6.1 Bernoulli distribution","text":"Imagine coin-toss experiment. coin comes heads consider experiment success. can encode outcome experiment takes value 1 success (heads) 0 otherwise. random variable \\(X\\) thus 1 get heads 0 otherwise. thing missing probability success (symmetry probability failure) denote \\(p\\). coin unbiased, \\(p\\) 0.5 50/50 chance success failure.can describe experiment way say \\(X\\) Bernoulli random variable success parameter \\(p\\). experiments can reduced Bernoulli trial. Let’s take die . consider experiment success roll 6. 1/6 chance rolling 6, success parameter \\(p\\) 1/6. experiment failure get outcome; get 1, 2, 3, 4, 5. probability failure thus 5/6. general rule thumb, probability failure \\(1- p\\).expected value \\(X\\) \\(E(X) = p\\) variance \\(Var(X) = p(1-p)\\). probability mass function \\(X\\) :\\[\nf(x) = P(X = x) = p^x (1 - p)^{1 - x}\n\\]","code":""},{"path":"lecture-4-probability-theory.html","id":"binomial-distribution","chapter":"5 Lecture 4: Probability theory","heading":"5.6.2 Binomial distribution","text":"Let’s say toss unbiased coin 3 times. performing 3 Bernoulli trials, success parameter \\(p\\). consider outcome heads success. many possible permutations available:\\[\nHHH, HHT, HTH, THH, HTT, THT, TTH, TTT.\n\\]particular experiment aren’t interested get heads many heads get. Notice following:\\[\n\\begin{array}{c}\n\\mbox{Number heads} & \\mbox{Outcomes} \\\\\n0 & TTT \\\\\n1 & HTT, THT, TTH\\\\\n2 & HHT, HTH, THH\\\\\n3 & HHH\n\\end{array}\n\\]\nsee outcomes 0 3 heads equally likely (\\(P(X = 0) = P(X = 3) = 1/8\\)), \\(P(X = 1) = P(X = 2) = 3/8\\). can define random variable \\(X\\) number heads say binomially distributed parameters \\(n\\) \\(p\\). \\(n\\) number Bernoulli trials (number coin flips, die throws) \\(p\\) success probability. expected value \\(X\\) \\(E(X) = np\\) variance \\(X\\) \\(Var(X) = np(1-p)\\). probability mass function :\\[\nf(x) = P(X = x) = \\binom{n}{k} p^k (1-p)^{n - k},\n\\]\\(k\\) number successes. Note similarity Bernoulli distribution.Let’s simulate scenario . toss coin three times 10000 iterations iteration keep track number heads. can see result plot .can compute probabilities \\(P(X = x)\\) \\(P(X <= x)\\) dbinom() pbinom() functions.","code":"\n# P(X = 2)\ndbinom(x = 2, size = 3, prob = 0.5)\n#> [1] 0.375\n# P(X <= 2)\npbinom(q = 2, size = 3, prob = 0.5)\n#> [1] 0.875"},{"path":"lecture-4-probability-theory.html","id":"poisson-distribution","chapter":"5 Lecture 4: Probability theory","heading":"5.6.3 Poisson distribution","text":"third discrete distribution Poisson distribution. use binomial distribution model counts within interval. Unlike binomial distribution fixed upper bound \\(n\\) Poisson distribution ceiling. Poisson distribution single parameter \\(\\lambda\\) describes expected number outcomes within interval. defining characteristic Poisson distribution expected value equal variance. : \\(E(X) = \\lambda = Var(X)\\).plot 10000 draws Poisson distribution rate parameter \\(\\lambda = 5\\).can compute probabilities \\(P(X = x)\\) \\(P(X <= x)\\) dpois() ppois() functions.","code":"\n# P(X = 5)\ndpois(x = 5, lambda = 5)\n#> [1] 0.1754674\n# P(X <= 5)\nppois(q = 5, lambda = 5)\n#> [1] 0.6159607"},{"path":"lecture-4-probability-theory.html","id":"examples-of-continuous-distributions","chapter":"5 Lecture 4: Probability theory","heading":"5.7 Examples of continuous distributions","text":"","code":""},{"path":"lecture-4-probability-theory.html","id":"the-normal-distribution","chapter":"5 Lecture 4: Probability theory","heading":"5.7.1 The normal distribution","text":"normal distribution important probability distribution see course. Many continuous variables can described normal distribution. normal distribution completely described expected value variance (standard deviation). means know expected value variance can completely reconstruct distribution. Another property normal distribution symmetry. normal distribution expected value \\(\\mu\\) standard deviation \\(\\sigma\\), roughly 68%, 95%, 99.7% measurements within one, two, three standard deviations respectively.Let’s look density normally distributed random variable \\(Z\\) expected value 0 standard deviation 1.normal distribution doesn’t expected value 0 standard deviation 1 refer standard normal distribution. normal distribution can transformed standard normal distribution centering scaling random variable. example: imagine \\(X\\) normally distributed random variable parameters \\(\\mu = 10\\) \\(\\sigma = 5\\). density looks like:center \\(X\\) subtracting expected value, scale \\(X\\) dividing standard deviation:\\[\nZ = \\frac{X - \\mu}{\\sigma} = \\frac{X - 10}{5},\n\\]see \\(Z\\) normally distributed random variable expected value 0 standard deviation 1.Compared probability mass function spent little time discussing probability density function. probability density function little trickier use relies calculus. Say \\(Z\\), random variable standard normal distribution, want know probability \\(Z\\) -1, 1, \\(P(-1 \\leq Z \\leq 1)\\). working probability mass function enough us add probabilities respected values. since \\(Z\\) continuous, probability \\(Z\\) taking specific value 0. must thus look intervals. compute probability random variable within region requires integration, something won’t cover course.\\[\nP(-1 \\leq Z \\leq 1) \\approx 68\\%.\n\\]can calculate probabilities \\(P(Z > 2)\\) \\(P(Z < -0.5)\\). \\(P(Z > 2)\\) can use fact probabilities must add 1 can rewrite \\(P(Z > 2) = 1 - P(Z <= 2)\\).can use R calculate area red curves. must use pnorm() function. Let’s compute probabilities three cases:","code":"\npnorm(q = -0.5, mean = 0, sd = 1)\n#> [1] 0.3085375\n1 - pnorm(q = 2, mean = 0, sd = 1)\n#> [1] 0.02275013\npnorm(q = 1, mean = 0, sd = 1) - pnorm(q = -1, mean = 0, sd = 1)\n#> [1] 0.6826895"},{"path":"lecture-4-probability-theory.html","id":"the-t-distribution","chapter":"5 Lecture 4: Probability theory","heading":"5.7.2 The \\(t\\)-distribution","text":"Closely related normal distribution \\(t\\)-distribution. reminiscent normal distribution “heavier tails”. number degrees freedom determine shape distribution. discuss \\(t\\)-distribution better later.can compute probabilities \\(P(X < x)\\), \\(X\\) random variable \\(t\\)-distribution, pt() function. pt() function requires two inputs, \\(x\\) number degrees freedom:Compare value computed pnorm(q = 1, mean = 0, sd = 1). greater \\(t\\)-distribution consequence heavier tails.","code":"\n1 - pt(q = 2, df = 5)\n#> [1] 0.05096974"},{"path":"lecture-4-probability-theory.html","id":"the-chi-2-distribution","chapter":"5 Lecture 4: Probability theory","heading":"5.7.3 The \\(\\chi ^2\\) distribution","text":"\\(\\chi ^2\\) distribution based normal distribution. symmetrical like \\(t\\)-distribution single parameter, namely number degrees freedom.can compute probabilities \\(P(X < x)\\), \\(X\\) random variable \\(\\chi^2\\)-distribution, pchisq() function. pchisq() function requires two inputs, \\(x\\) number degrees freedom:","code":"\n1 - pchisq(q = 2, df = 1)\n#> [1] 0.1572992"},{"path":"lecture-4-probability-theory.html","id":"the-central-limit-theorem","chapter":"5 Lecture 4: Probability theory","heading":"5.8 The central limit theorem","text":"important result probability theory Central limit theorem. says mean sample resemble normal distribution closely sample grows. Furthermore, variance decreases sample size increases.Remember Poisson distribution ?Let’s draw random samples Poisson distribution . samples size 5, 25, 100, 500. sample compute mean store . repeating process 5000 times, plot results:","code":""},{"path":"lecture-5-inference.html","id":"lecture-5-inference","chapter":"6 Lecture 5: Inference","heading":"6 Lecture 5: Inference","text":"far descriptive statistics. computed stuff like means, standard deviations . purpose descriptive statistics allow us get feel data explore whether research question can answered data. done descriptives, must confirm statistical tests. Eyeballing enough. goal lecture give tools necessary answer simple research questions statistics.","code":""},{"path":"lecture-5-inference.html","id":"the-population-and-the-sample","chapter":"6 Lecture 5: Inference","heading":"6.1 The population and the sample","text":"’ve made cryptic references “populations” “samples”. population group interested investigating entirety. study average height Europeans population entire population Europe. Obviously impossible measure every single individual Europe instead make sample; randomly selected subgroup population. examine sample make inferences population.Note population unique can many samples population.","code":""},{"path":"lecture-5-inference.html","id":"sample-statistics","chapter":"6 Lecture 5: Inference","heading":"6.2 Sample statistics","text":"interested statistics. statistic function sample. example, assume measure height ten individuals compute mean measurements. , mean statistic function data (measurements). Since mean function sample, sample random, mean inherits property stochastic. careful! taken measurements deterministic stochastic.Let’s create example illustrate property. create “fake” population repeatedly sample compute mean sample.can see, samples gave different mean.","code":"\n# The population\npopulation <- c(181, 191, 171, 151, 161, 171, 172, \n                175, 165, 183, 201, 151, 159, 153, 165)\n# Let's draw 5 samples from the population, each of size 4 \n# and compute the mean. \nset.seed(7783)\nmeans <- colMeans(replicate(5, sample(population, size = 4)))\n# Make output more readable\nnames(means) <- c('mean1', 'mean2', 'mean3', 'mean4', 'mean5')\nmeans\n#>  mean1  mean2  mean3  mean4  mean5 \n#> 167.25 178.00 159.50 174.50 183.75"},{"path":"lecture-5-inference.html","id":"properties-of-the-mean-of-the-sample","chapter":"6 Lecture 5: Inference","heading":"6.3 Properties of the mean of the sample","text":"Let us denote mean sample observe sample \\(\\bar{X}\\) mean observed sample \\(\\bar{x}\\). \\(\\bar{X}\\) random variable \\(\\bar{x}\\) outcome. Since \\(\\bar{X}\\) random variable distributional properties: expected value variance.\\(X_1, X_2, \\ldots , X_n\\) independent identically distributed variables, expected value \\(E(X_i) = \\mu\\) variance \\(Var(X_i) = \\sigma ^2\\) :\\[\nE(\\bar{X}) = \\mu, \\quad Var(\\bar{X}) = \\frac{\\sigma ^2}{n}.\n\\]may heard standard error . standard error simply standard deviation random variable \\(\\bar{X}\\); namely, \\(SE(\\bar{X}) = \\sigma/\\sqrt{n}\\). gets name commonly used.Finally, assume sample \\(X_1, X_,2, \\ldots , X_n\\) comes normal distribution parameters \\(\\mu\\) \\(\\sigma\\) \\(\\bar{X}\\) also normally distributed parameters \\(\\mu\\) \\(\\sigma / \\sqrt{n}\\).Let’s create example like . draw sample size 25 standard normal distribution 5000 times. compute mean sample store . simulation run course, compute mean standard deviation 5000 means finally plot . expect sample mean 0, standard deviation \\(1/\\sqrt{25}\\). Finally, expect histogram means resemble normal distribution.","code":"\nnMeans <- colMeans(replicate(5000, rnorm(n = 25, mean = 0, sd = 1))) \ntibble(parameter = c('mean', 'standard deviation'),\n       theoretical = c(0, 1/sqrt(25)), \n       simulation = c(mean(nMeans), sd(nMeans))) %>%\n    kbl() %>%\n    kable_styling(full_width = F)\ntibble(means = nMeans) %>%\n    ggplot(aes(x = means)) +\n    geom_histogram() +\n    theme_cowplot()\n#> `stat_bin()` using `bins = 30`. Pick better value with\n#> `binwidth`."},{"path":"lecture-5-inference.html","id":"the-estimator","chapter":"6 Lecture 5: Inference","heading":"6.4 The estimator","text":"estimator statistic estimates parameters model. already seen parameters success probability \\(p\\) binomial distribution \\(\\mu\\) \\(\\sigma\\) normal distribution.","code":""},{"path":"lecture-5-inference.html","id":"proportions-and-means","chapter":"6 Lecture 5: Inference","heading":"6.4.1 Proportions and means","text":"estimator works \\(\\mu\\) \\(p\\):\\[\n\\hat{\\mu} = \\frac{\\sum _ {= 1} ^n X_ }{n} = \\hat{p}\n\\]example binomial distribution. Imagine population 50 people. person can diabetic . interest determine proportion people diabetic. Unfortunately secure enough grant money test 20 people.can see, estimate \\(p\\) 0.25 pretty close population proportion.","code":"\nset.seed(847)\ndiabetesPref <- rbinom(n = 50, size = 1, prob = 0.3)\n# This is the true population\ndiabetesPref\n#>  [1] 1 1 1 0 0 0 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 1 1 1 1 0 0\n#> [29] 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0\n# Sample 20 people\nsampleDia <- sample(diabetesPref, size = 20)\nmean(sampleDia)\n#> [1] 0.25"},{"path":"lecture-5-inference.html","id":"variance-1","chapter":"6 Lecture 5: Inference","heading":"6.4.2 Variance","text":"denote estimator variance \\(S^2\\). defined :\\[\nS^2 = \\frac{\\sum _ {= 1} ^n (X_i - \\bar{X})^2}{n - 1}.\n\\]seen data denote estimate \\(s^2\\). wanted estimate standard deviation simply take square root \\(s^2\\).","code":""},{"path":"lecture-5-inference.html","id":"confidence-intervals","chapter":"6 Lecture 5: Inference","heading":"6.5 Confidence intervals","text":"far considered point estimates. saw mean example, point estimates dependent sample. probability point estimator exactly equal parameter trying estimate 0 (remember discussion continuous random variables). also saw increased sample size, allowed repeat sampling, approached true parameter value. Now, impossible real life. sample size often restricted due external factors (often many) can’t easily repeat experiments.Luckily, solution problem: confidence intervals. construct confidence interval around point estimates get sense estimates likely. wider confidence intervals, likely catch true parameter. probably seen terms like 95% confidence interval 99.5% confidence interval. percentages reflect proportion times true parameter value contained interval repeat experiment infinitely often. sounds weird, ’s . Please note parameter never changes, estimates.generally speak \\(1 - \\alpha\\) confidence interval \\(\\alpha\\) accepted type error probability. example, interested 95% confidence interval, accept type error probability 5% \\(alpha = 0.05\\).type error probability? instance proportion confidence intervals construct around point estimates don’t contain true parameter value.create confidence intervals need compute test statistics. later.","code":""},{"path":"lecture-5-inference.html","id":"hypothesis-tests","chapter":"6 Lecture 5: Inference","heading":"6.6 Hypothesis tests","text":"alternative confidence intervals hypothesis tests. hypothesis test two components: null hypothesis \\(H_0\\) alternative hypothesis \\(H_1\\). null hypothesis often bland, boring scenario want reject. example, two groups, one placebo actual drug, null hypothesis difference groups. obviously boring result perspective pharmaceutical company wants demonstrate efficacy drug. alternative hypothesis scenario interested .e. difference groups.go ? defined hypothesis test assume null hypothesis true. use data construct test statistic test null hypothesis. Often test statistic well known probability distribution; test statistic “unlikely enough” given null hypothesis, use evidence reject null hypothesis.Important: never accept null hypothesis.","code":""},{"path":"lecture-5-inference.html","id":"the-orientation-of-the-hypothesis-test","chapter":"6 Lecture 5: Inference","heading":"6.6.1 The orientation of the hypothesis test","text":"hypothesis test can one-sided two-sided. example one-sided test :\\[\n\\begin{aligned}\nH_0&: \\mbox{mean cholesterol group equal mean group B} \\\\\nH_1&: \\mbox{mean cholesterol group smaller mean group B}\n\\end{aligned}\n\\]example two-sided test :\\[\n\\begin{aligned}\nH_0&: \\mbox{mean cholesterol group equal mean group B} \\\\\nH_1&: \\mbox{mean cholesterol group equal mean group B}\n\\end{aligned}\n\\]","code":""},{"path":"lecture-5-inference.html","id":"the-test-statistic-of-the-hypothesis-test","chapter":"6 Lecture 5: Inference","heading":"6.6.2 The test statistic of the hypothesis test","text":"test statistic statistic can use reject null hypothesis. example test statistic :\\[\nt = \\frac{\\bar{X} - \\mu}{S/\\sqrt{n}} \n\\]use perform \\(t\\)-tests. Note test statistic based sample thus random variable distribution. compare computed test statistic theoretical distribution (null).’s important know test statistic compute. looking means assume normal distribution, appropriate test \\(t\\)-test. need \\(t\\)-test don’t know standard deviation population distribution.","code":""},{"path":"lecture-5-inference.html","id":"the-rejection-region","chapter":"6 Lecture 5: Inference","heading":"6.6.3 The rejection region","text":"said , reject null test statistic unreasonable given null hypothesis. rejection region area distribution test statistic contains values lead rejection null hypothesis. rejection region always tails distribution.","code":""},{"path":"lecture-5-inference.html","id":"the-p-value","chapter":"6 Lecture 5: Inference","heading":"6.6.4 The \\(p\\)-value","text":"Assume \\(t\\) test statistic. \\(p\\)-value hypothesis test probability seeing larger test statistic \\(t\\) given null hypothesis true. \\(p\\)-value smaller predefined significance threshold \\(\\alpha\\) use evidence support decision reject null hypothesis.","code":""},{"path":"lecture-5-inference.html","id":"power-and-errors","chapter":"6 Lecture 5: Inference","heading":"6.6.5 Power and errors","text":"power test probability rejecting null hypothesis rejected. can increase statistical power increasing sample size.commit type error reject null hypothesis shouldn’t (false positive). Similarly, commit type II error reject null hypothesis (false negative).","code":""},{"path":"lecture-5-inference.html","id":"examples","chapter":"6 Lecture 5: Inference","heading":"6.7 Examples","text":"use pulseEn.csv data.","code":"\npulse <- \n    read_csv2('https://notendur.hi.is/thj73/data/pulseEn.csv') %>%\n    na.omit() %>% # Remove NA\n    filter(height < 300) # Remove two weird height values"},{"path":"lecture-5-inference.html","id":"the-simple-t-test","chapter":"6 Lecture 5: Inference","heading":"6.7.1 The simple \\(t\\)-test","text":"Let’s see average height pulseEn differs 150 cm significantly. use t.test() function. parameter value mu 0 default need change value want test.\\(p\\)-value tiny. strongly reject null hypothesis.test statistic test can computed :\\[\nt = \\frac{\\bar{x} - \\mu _ 0}{s/\\sqrt{n}}\n\\]","code":"\nt.test(pulse$height, mu = 150)\n#> \n#>  One Sample t-test\n#> \n#> data:  pulse$height\n#> t = 48.684, df = 423, p-value < 2.2e-16\n#> alternative hypothesis: true mean is not equal to 150\n#> 95 percent confidence interval:\n#>  172.3950 174.2795\n#> sample estimates:\n#> mean of x \n#>  173.3373"},{"path":"lecture-5-inference.html","id":"a-small-t-test-app","chapter":"6 Lecture 5: Inference","heading":"6.7.2 A small \\(t\\)-test app","text":"Click !","code":""},{"path":"lecture-5-inference.html","id":"the-t-test-for-groups","chapter":"6 Lecture 5: Inference","heading":"6.7.3 The \\(t\\)-test for groups","text":"variable interest height sample. hypothesis test follows:\\[\n\\begin{aligned}\nH_0:& \\mbox{mean height intervention groups } \\\\\nH_1:& \\mbox{mean height intervention groups }\n\\end{aligned}\n\\]begin inspecting variables graphically:can see difference groups little, least height-wise. makes us suspect unable reject null hypothesis mean height groups different. Let’s confirm performing \\(t\\)-test t.test() function:see \\(p\\)-value test 0.0829235 unable reject null hypothesis significance level \\(\\alpha = 0.05\\). Note also get confidence interval.test statistic can computed :\\[\nt = \\frac{(\\bar{x} - \\bar{y} - \\delta _ 0)}{\\sqrt{s_1^2/n + s_2^2/n}}\n\\]","code":"\npulse %>%\n    ggplot(aes(x = intervention, y = height)) +\n    geom_boxplot() +\n    theme_cowplot()\nt.test(height ~ intervention, data = pulse)\n#> \n#>  Welch Two Sample t-test\n#> \n#> data:  height by intervention\n#> t = 1.7387, df = 365.06, p-value = 0.08292\n#> alternative hypothesis: true difference in means between group active and group stationary is not equal to 0\n#> 95 percent confidence interval:\n#>  -0.221251  3.599599\n#> sample estimates:\n#>     mean in group active mean in group stationary \n#>                 174.3571                 172.6680"},{"path":"lecture-5-inference.html","id":"the-paired-t-test","chapter":"6 Lecture 5: Inference","heading":"6.7.4 The paired \\(t\\)-test","text":"Let us see difference first second pulse measurement control group. control group nothing measurements, expect significant difference measurements. always, begin plotting two measurements:\ntwo distributions seem identical. Let’s perform paired \\(t\\)-testThis time \\(p\\)-value quite large completely fail reject null hypothesis.test statistic can computed :\\[\nt = \\frac{\\bar{w} - \\Delta _ 0}{s_w /\\sqrt{n}}\n\\]\\[\n\\begin{aligned}\n\\bar{w} &= \\frac1n \\sum _ {= 1} ^n w_i, \\\\\nw_i &= x_i - y_i, \\\\ \ns_w &= \\sqrt{\\frac{1}{n - 1} \\sum _ {= 1} ^n (w_i - \\bar{w})}\n\\end{aligned}\n\\]","code":"\npulse %>%\n    filter(intervention == 'stationary') %>%\n    gather(measurement, value, firstPulse, secondPulse) %>% # convert to long\n    ggplot(aes(x = measurement, y = value)) +\n        geom_boxplot() +\n        theme_cowplot()\nstationary <- \n    pulse %>%\n    filter(intervention == 'stationary')\nt.test(stationary$firstPulse, stationary$secondPulse)\n#> \n#>  Welch Two Sample t-test\n#> \n#> data:  stationary$firstPulse and stationary$secondPulse\n#> t = -0.076792, df = 506.37, p-value = 0.9388\n#> alternative hypothesis: true difference in means is not equal to 0\n#> 95 percent confidence interval:\n#>  -2.076885  1.920635\n#> sample estimates:\n#> mean of x mean of y \n#>  71.04297  71.12109"},{"path":"lecture-5-inference.html","id":"a-test-for-proportions","chapter":"6 Lecture 5: Inference","heading":"6.7.5 A test for proportions","text":"Let’s see difference proportion smokers versus non-smokers. begin constructing table see many fall category.Using table see almost one smokes sample. Let’s perform test:test statistic can computed :\\[\nz = \\frac{\\hat{p} - p_0}{\\sqrt{p_0(1 - p_0)/n}}\n\\]\\(\\hat{p} = x/n\\), \\(x\\) number successes.","code":"\nprop.table(table(pulse$smokes))\n#> \n#>        no       yes \n#> 0.9009434 0.0990566\nbinom.test(table(pulse$smokes))\n#> \n#>  Exact binomial test\n#> \n#> data:  table(pulse$smokes)\n#> number of successes = 382, number of trials = 424,\n#> p-value < 2.2e-16\n#> alternative hypothesis: true probability of success is not equal to 0.5\n#> 95 percent confidence interval:\n#>  0.8684666 0.9276729\n#> sample estimates:\n#> probability of success \n#>              0.9009434"},{"path":"lecture-5-inference.html","id":"in-conclusion","chapter":"6 Lecture 5: Inference","heading":"6.8 In conclusion","text":"tests can perform tests independence contingency table tests variances talk discuss ANOVA. Also, today’s lecture heavy hypothesis tests without mention confidence intervals. discuss confidence intervals better talk linear models.","code":""},{"path":"lecture-6-anova.html","id":"lecture-6-anova","chapter":"7 Lecture 6: ANOVA","heading":"7 Lecture 6: ANOVA","text":"previous lecture used \\(t\\)-test compare means two groups. natural extension test ask multiple groups. Rather perform \\(t\\)-test every single group difference use staticial method called analysis variance ANOVA.","code":""},{"path":"lecture-6-anova.html","id":"the-assumptions-of-anova","chapter":"7 Lecture 6: ANOVA","heading":"7.1 The assumptions of ANOVA","text":"assumptions ANOVA follows:samples random samples.samples random samples.underlying populations normally distributed.underlying populations normally distributed.variances populations .variances populations .","code":""},{"path":"lecture-6-anova.html","id":"one-way-anova","chapter":"7 Lecture 6: ANOVA","heading":"7.2 One-way ANOVA","text":"begin short example. Imagine developed new drug reduces biomarker. interested testing two things:drug works (outperforms placebo).drug works (outperforms placebo).Compare efficacy drug another drug.Compare efficacy drug another drug.study randomly assign participants one three groups: P (placebo), O (old drug), N (new drug). draw blood twice, administered treatment measure biomarker. measured biomarkers compute difference two measurements participants. data looks like:Let’s plot data get better sense . red dots represent group means black dots represent data . broken line running across plot overall mean.plot immediately see difference placebo group two drug groups. clear whether difference two drugs. Finally, seems variance approximately groups. point know eyeballing sufficient.","code":"\nset.seed(12)\nbioData <- \n    tibble(P1 = rnorm(10, 7, 1), \n       P2 = rnorm(10, 7, 1),\n       O1 = rnorm(10, 7, 1), \n       O2 = rnorm(10, 5, 1), \n       N1 = rnorm(10, 7, 1), \n       N2 = rnorm(10, 4, 1)) %>%\n    mutate(P = P1 - P2, O = O1 - O2, N = N1 - N2) %>%\n    select(P, O, N)\nbioLong <- \n    bioData %>%\n    gather(treatment, value) %>%\n    mutate(treatment = factor(treatment, levels = c('P', 'O', 'N')))\nbioMeans <-\n    bioLong %>%\n    group_by(treatment) %>%\n    summarize(mean = mean(value))\nbioLong %>%\n    ggplot(aes(x = treatment, y = value)) +\n    geom_point() + \n    geom_point(data = bioMeans %>% rename(value = mean), col = 'red') +\n    geom_hline(yintercept = mean(bioLong$value), lty = 2) +\n    theme_cowplot() +\n    labs(x = 'Treatment groups', y = 'Values')"},{"path":"lecture-6-anova.html","id":"mathematical-notation-for-one-way-anova","chapter":"7 Lecture 6: ANOVA","heading":"7.3 Mathematical notation for one-way ANOVA","text":"\\(y_ {ij}\\) : \\(\\) index group \\(j\\) index measurement within group\\(y_ {ij}\\) : \\(\\) index group \\(j\\) index measurement within group\\(\\) : total number groups\\(\\) : total number groups\\(n_ \\) : total number measurements within group \\(\\)\\(n_ \\) : total number measurements within group \\(\\)\\(N\\) : total number measurements \n\\[\n  N = n_1 + n_2 + \\ldots + n_a\n\\]\\(N\\) : total number measurements \n\\[\n  N = n_1 + n_2 + \\ldots + n_a\n\\]\\(\\bar{y}_ {.}\\): within group mean:\n\\[\n  \\bar{y}_ {.} = \\frac{\\sum _ {j = 1} ^{n_i} y_ {ij}}{n_i}\n\\]\\(\\bar{y}_ {.}\\): within group mean:\\[\n  \\bar{y}_ {.} = \\frac{\\sum _ {j = 1} ^{n_i} y_ {ij}}{n_i}\n\\]\\(\\bar{y}_ {..}\\) : grand mean overall mean. mean entire data set.\n\\[\n  \\bar{y}_ {..} = \\frac{\\sum _{= 1} ^\\sum _{j = 1}^{n_i} y_ {ij}}{N}\n\\]\\(\\bar{y}_ {..}\\) : grand mean overall mean. mean entire data set.\\[\n  \\bar{y}_ {..} = \\frac{\\sum _{= 1} ^\\sum _{j = 1}^{n_i} y_ {ij}}{N}\n\\]\\(SS_T\\): measure total variation data:\n\\[\n  SS_T = \\sum _{= 1} ^\\sum _{j = 1} ^{n _i} (y _{ij} - \\bar{y} _{..})^2\n\\]\\(SS_T\\): measure total variation data:\\[\n  SS_T = \\sum _{= 1} ^\\sum _{j = 1} ^{n _i} (y _{ij} - \\bar{y} _{..})^2\n\\]\\(SS_{Tr}\\): measure variation treatments:\n\\[\n  SS_{Tr} = \\sum _{= 1} ^n_i (\\bar{y} _{.} - \\bar{y} _{..})^2\n\\]\\(SS_{Tr}\\): measure variation treatments:\\[\n  SS_{Tr} = \\sum _{= 1} ^n_i (\\bar{y} _{.} - \\bar{y} _{..})^2\n\\]\\(SS_E\\): measure variation within treatments:\n\\[\n  SS_E = \\sum ^_{= 1} \\sum _{j = 1} ^{n_i} (y_{ij} - \\bar{y}_{.})^2\n\\]\\(SS_E\\): measure variation within treatments:\\[\n  SS_E = \\sum ^_{= 1} \\sum _{j = 1} ^{n_i} (y_{ij} - \\bar{y}_{.})^2\n\\]","code":""},{"path":"lecture-6-anova.html","id":"the-one-way-anova-table","chapter":"7 Lecture 6: ANOVA","heading":"7.4 The one-way ANOVA table","text":"common put square sums variation \\(SS_T, SS_{Tr}, SS_E\\) table:\\[\n\\begin{array}{c}\n\\mbox{Square sums} & \\mbox{Degrees freedom} & \\mbox{Mean square sums} \\\\\nSS_{Tr} & - 1 & MS_{Tr} = SS_{Tr}/(- 1) \\\\\nSS_{E} & N - & MS_{E} = SS_{E}/(N - ) \\\\\nSS_{T} & N - 1 & \n\\end{array}\n\\]ANOVA table used perform hypothesis tests within ANOVA paradigm.","code":""},{"path":"lecture-6-anova.html","id":"hypothesis-tests-with-anova","chapter":"7 Lecture 6: ANOVA","heading":"7.5 Hypothesis tests with ANOVA","text":"null hypothesis one-way ANOVA :\\[\nH_0 = \\mu _ 1 = \\mu _ 2 = \\ldots = \\mu _ \n\\]alternative hypothesis :\\[\nH_1 = \\mbox{ least one mean differs others}\n\\]iThe test statistic :\n\\[\nF = \\frac{SS_{Tr}/(- 1)}{SS_E/(N - )} = \\frac{MS_{Tr}}{MS_E}\n\\]denote \\(F\\)-statistic \\(F\\) null hypothesis come \\(F\\) distribution \\(- 1\\) \\(N - \\) degrees freedom. successful rejecting null means evidence least one mean differing others.","code":""},{"path":"lecture-6-anova.html","id":"anova-in-r","chapter":"7 Lecture 6: ANOVA","heading":"7.6 ANOVA in R","text":"covered theort ’s time use R perform ANOVA analysis us. use biomarker drug . first thing create ANOVA object R aov function:next thing create ANOVA table anova() function.can see get first two rows ANOVA table. \\(p\\)-value small meaning reject null hypothesis \\(\\alpha = 0.05\\) significance level. value \\(F\\)-statistic 15.7226097.successfully rejected null hypothesis tells us exists difference difference lies. need post-hoc analyses Tukey’s multiple comparison test.see comparison tested us. see old new drug differ placebo. Unfortunately new drug compared old drug failed reject null hypothesis. can therefore claim drug improvement.can also graph comparisons:","code":"\nanovaObj <- aov(value ~ treatment, data = bioLong)\nanova(anovaObj)\n#> Analysis of Variance Table\n#> \n#> Response: value\n#>           Df Sum Sq Mean Sq F value    Pr(>F)    \n#> treatment  2 49.208 24.6040  15.723 2.967e-05 ***\n#> Residuals 27 42.252  1.5649                      \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nTukeyHSD(anovaObj)\n#>   Tukey multiple comparisons of means\n#>     95% family-wise confidence level\n#> \n#> Fit: aov(formula = value ~ treatment, data = bioLong)\n#> \n#> $treatment\n#>          diff        lwr      upr     p adj\n#> O-P 2.4941887  1.1070959 3.881282 0.0003727\n#> N-P 2.8949646  1.5078718 4.282057 0.0000552\n#> N-O 0.4007759 -0.9863169 1.787869 0.7560053\nplot(TukeyHSD(anovaObj))"},{"path":"lecture-6-anova.html","id":"a-glimpse-of-what-is-to-come","chapter":"7 Lecture 6: ANOVA","heading":"7.7 A glimpse of what is to come","text":"alternative way perform ANOVA analysis. can simply use linear regression function (subject next lecture) lm().lot information packed summary look bottom see \\(F\\)-statistic, degrees freedom \\(p\\)-value.","code":"\nlinearFit <- lm(value ~ treatment, data = bioLong)\nsummary(linearFit)\n#> \n#> Call:\n#> lm(formula = value ~ treatment, data = bioLong)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -3.2389 -0.6902 -0.1457  0.6638  3.1431 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)  -0.2720     0.3956  -0.688  0.49756    \n#> treatmentO    2.4942     0.5594   4.458  0.00013 ***\n#> treatmentN    2.8950     0.5594   5.175  1.9e-05 ***\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 1.251 on 27 degrees of freedom\n#> Multiple R-squared:  0.538,  Adjusted R-squared:  0.5038 \n#> F-statistic: 15.72 on 2 and 27 DF,  p-value: 2.967e-05"}]
