[{"path":"index.html","id":"general-information","chapter":"1 General information","heading":"1 General information","text":"","code":"\nlibrary(tidyverse)\nlibrary(knitr)\nlibrary(kableExtra)"},{"path":"index.html","id":"instructors","chapter":"1 General information","heading":"1.1 Instructors","text":"Lectures: Þórarinn Jónmundsson (thj73@hi.)Workshops:Supervision: Thor Aspelund (thor@hi.)","code":""},{"path":"index.html","id":"about-me","chapter":"1 General information","heading":"1.2 About me","text":"","code":""},{"path":"index.html","id":"education","chapter":"1 General information","heading":"1.2.1 Education","text":"B.Sc. economics [2017]; B.Sc. applied mathematics [2020]M.Sc. mathematical statistics [2020]Ph.D. biostatistics [current]Research project: Identifying causal candidate proteins cardiometabolic disease","code":""},{"path":"index.html","id":"previous-teaching-experience","chapter":"1 General information","heading":"1.2.2 Previous teaching experience","text":"Teacher assistant:\nIntroduction probability statistics (STÆ203G)\nLinear algebra (STÆ107G)\nBayesian data analysis (STÆ529M)\nApplied linear statistical models (STÆ312M)\nR beginners (MAS103M)\nStatistical consulting (MAS201M)\nIntroduction probability statistics (STÆ203G)Linear algebra (STÆ107G)Bayesian data analysis (STÆ529M)Applied linear statistical models (STÆ312M)R beginners (MAS103M)Statistical consulting (MAS201M)Lecturer\nApplied linear statistical models (STÆ312M)\nApplied linear statistical models (STÆ312M)","code":""},{"path":"index.html","id":"contact-information","chapter":"1 General information","heading":"1.2.3 Contact information","text":"Email: thj73@hi.isOffice: Læknagarður (Lg-306)Mobile: 698-2126","code":""},{"path":"index.html","id":"course-description","chapter":"1 General information","heading":"1.3 Course description","text":"Compulsory course. aim course provide post graduate students practical generic skills required research. Items covered course descriptive statistics, effect statistics, validity reliability, inferential statistics, common parametric nonparametric statistical tests multiple regression analysis. students introduced computer statistical analysis practical computer classes.","code":""},{"path":"index.html","id":"learning-outcomes","chapter":"1 General information","heading":"1.4 Learning outcomes","text":"students:Understand basic concepts statistics data analysis.Can apply basic statistical methods data analysed thesis.Can understand criticize statistical analysis data health research.literate R.Understand theory behind various statistical calculations, able apply knowledge analysis data.","code":""},{"path":"index.html","id":"textbooks","chapter":"1 General information","heading":"1.5 Textbooks","text":"lecture notes based following texts:Textbooks English:\nModern Dive (MD)\nR Data Science (R4DS)\nStatistical Thinking 21st Century (ST21)\nModern Dive (MD)R Data Science (R4DS)Statistical Thinking 21st Century (ST21)Textbooks Icelandic:\nTölfræði frá grunni (TG)\nR frá grunni (RG)\nTölfræði frá grunni (TG)R frá grunni (RG)sufficient read lecture notes interested can read books pace.","code":""},{"path":"index.html","id":"software","chapter":"1 General information","heading":"1.6 Software","text":"RRStudio","code":""},{"path":"index.html","id":"lectures-and-workshops","chapter":"1 General information","heading":"1.7 Lectures and workshops","text":"Lectures 9:10-11:30 workshops 12:30 14:00. lectures recorded streamed.. Lecture workshop attendance mandatory. Student’s strongly encouraged utilize workshops!.can see rough plan course. Depending course goes may subject change. Lectures marked open might used extra lecture workshop slots.","code":""},{"path":"index.html","id":"tentative-lecture-schedule","chapter":"1 General information","heading":"1.7.1 Tentative lecture schedule","text":"","code":""},{"path":"index.html","id":"workshop-schedule","chapter":"1 General information","heading":"1.7.2 Workshop schedule","text":"","code":""},{"path":"index.html","id":"grading","chapter":"1 General information","heading":"1.8 Grading","text":"final grade class based following partition:Four assignments R, worth 10% final grade.Two longer assignments, worth 10% final grade.final exam worth 40% final grade.must get score 5/10 higher final exam finish course.","code":""},{"path":"index.html","id":"assignments","chapter":"1 General information","heading":"1.9 Assignments","text":"schedule assignments. encouraged form groups 2-4 people solve assignments. choose , please hand one solution per group.","code":""},{"path":"index.html","id":"final-exam","chapter":"1 General information","heading":"1.10 Final exam","text":"final exam take-home exam handed March 1st.","code":""},{"path":"index.html","id":"final-exam-eligibility","chapter":"1 General information","heading":"1.10.1 Final exam eligibility","text":"need hand four R assignments home assignments eligible take final exam.","code":""},{"path":"lecture-1-r-and-r-basics.html","id":"lecture-1-r-and-r-basics","chapter":"2 Lecture 1: R and R basics","heading":"2 Lecture 1: R and R basics","text":"","code":""},{"path":"lecture-1-r-and-r-basics.html","id":"installing-r-and-rstudio","chapter":"2 Lecture 1: R and R basics","heading":"2.1 Installing R and RStudio","text":"install RStudio, need install R. Please visit link download R operating system. Afterwards, can download RStudio .Alternatively, can create account RStudio Cloud cloud-based version RStudio.","code":""},{"path":"lecture-1-r-and-r-basics.html","id":"what-is-the-difference-between-r-and-rstudio","chapter":"2 Lecture 1: R and R basics","heading":"2.2 What is the difference between R and RStudio?","text":"R programming language specifically designed statistics RStudio Integrated Development Environment (IDE). RStudio combines multiple tool single graphical user interface (GUI) ease development R programs. Examples tools source-code environment, file navigation, plot viewer.exist alternatives RStudio. personally use Nvim-R like many others started R journey RStudio.","code":""},{"path":"lecture-1-r-and-r-basics.html","id":"what-is-the-difference-between-.r-and-.rmd-files","chapter":"2 Lecture 1: R and R basics","heading":"2.3 What is the difference between .R and .Rmd files?","text":"file .R extension script file R. script file contains code written. .R file “stupid” sense follow instructions letter. tell computer run R scrip opening command prompt/terminal, navigating root folder, writing Rscript yourScript.R.file .Rmd extension markdown file. allows combine code text write detailed reports. example .Rmd file lecture.","code":""},{"path":"lecture-1-r-and-r-basics.html","id":"hello-world","chapter":"2 Lecture 1: R and R basics","heading":"2.4 Hello world","text":"simple piece R code inside .Rmd document:anatomy code follows:# Example embedded code. # symbol tells R ignore whatever comes # particular line. useful allows us comment code. Comments allow explain code . Writing informative, concise comments extremely important. good chance either share code collaborator revisit later time. cases, comments can quickly bring reader speed.# Example embedded code. # symbol tells R ignore whatever comes # particular line. useful allows us comment code. Comments allow explain code . Writing informative, concise comments extremely important. good chance either share code collaborator revisit later time. cases, comments can quickly bring reader speed.print('Hello world'). line composed two things: print() function string 'Hello world'. function like recipe; inputs outputs. recipe print() take input display command line. many predefined functions R can also create . string data type R. quotes tell R whatever considered string. remove quotes, R whine. can also tell R something string double quotes. Pick whichever prefer.print('Hello world'). line composed two things: print() function string 'Hello world'. function like recipe; inputs outputs. recipe print() take input display command line. many predefined functions R can also create . string data type R. quotes tell R whatever considered string. remove quotes, R whine. can also tell R something string double quotes. Pick whichever prefer.","code":"\n# Example of embedded code\nprint('Hello world')\n#> [1] \"Hello world\"\n# Function printMyString\n# Input: A string.\n# Output: The input string is printed out in the command line.\nprintMyString <- function(someString) {\n    print(someString)\n}\n# Test case\nprintMyString(\"This is a custom function I made. It's pretty useless.\")\n#> [1] \"This is a custom function I made. It's pretty useless.\""},{"path":"lecture-1-r-and-r-basics.html","id":"other-basic-objects","chapter":"2 Lecture 1: R and R basics","heading":"2.5 Other basic objects","text":"R many objects besides strings integers, doubles (reals), vectors, factors, logical, data frames.integer whole number. Examples integers 1, -2, 1000, 94.integer whole number. Examples integers 1, -2, 1000, 94.double real number. Examples real numbers 1, -2, 3.14, sin(0.5).double real number. Examples real numbers 1, -2, 3.14, sin(0.5).vector collection values. define vector c() function. example c(1, -2, 3.14, sin(0.5)) vector doubles c('', 'ab', 'bla bla') vector strings. Note elements vector data type. discrepancy, R try coerce elements data type.vector collection values. define vector c() function. example c(1, -2, 3.14, sin(0.5)) vector doubles c('', 'ab', 'bla bla') vector strings. Note elements vector data type. discrepancy, R try coerce elements data type.factor way store categorical values. example, imagine conducted study patients. One pieces information store smoking status patients. Patients can current smokers, previous smokers, never smokers. levels factor never, previous, current. discuss factor variables later.factor way store categorical values. example, imagine conducted study patients. One pieces information store smoking status patients. Patients can current smokers, previous smokers, never smokers. levels factor never, previous, current. discuss factor variables later.logical value either TRUE FALSE. Logical values can also encoded 1 0 TRUE FALSE respectively.logical value either TRUE FALSE. Logical values can also encoded 1 0 TRUE FALSE respectively.data frame collection vectors. can think data frame Excel spreadsheet columns rows. data frames later.data frame collection vectors. can think data frame Excel spreadsheet columns rows. data frames later.Note exhaustive list.","code":""},{"path":"lecture-1-r-and-r-basics.html","id":"storing-objects-in-memory","chapter":"2 Lecture 1: R and R basics","heading":"2.6 Storing objects in memory","text":"can store objects memory <- operator. Whatever right <- object want store memory whatever left <- name object. quick example:vector c(1, 2, 3) exists now memory named variable x rather typing c(1, 2, 3) whenever want work vector, can simply use x instead. example, imagine wanted add 1 vector c(1, 2, 3); since stored c(1, 2, 3) object x R, can simply write:careful! reuse variable names pre-existing object overwritten new object.","code":"\nx <- c(1, 2, 3)\nx\n#> [1] 1 2 3\nx + 1\n#> [1] 2 3 4\nx <- 'x is no longer a vector of numbers; it is a string'\nx\n#> [1] \"x is no longer a vector of numbers; it is a string\""},{"path":"lecture-1-r-and-r-basics.html","id":"packages","chapter":"2 Lecture 1: R and R basics","heading":"2.7 Packages","text":"R many built-functions data types sometimes need . Luckily, good chance someone R community already solved problem made work publicly available R package. R package thus simply collection functions data types (sometimes even data) can load R use.can download install package install.packages() function. load package use library() function. enough install package must load every time open RStudio (intend use ).","code":"\n# This can be run once\ninstall.packages('ggplot2')\n# This is something you have to run every time \n# you restart R\nlibrary('ggplot2')"},{"path":"lecture-1-r-and-r-basics.html","id":"the-mtcars-data","chapter":"2 Lecture 1: R and R basics","heading":"2.8 The mtcars data","text":"’s time start working data. Included R mtcars data frame:Note entire data set printed. annoying avoided. Functions like as_tibble() included tibble library converts data frame tibble, much nicer data type work (opinion). begin installing loading tibble library:use mtcars data frame input as_tibble() function.things worth mentioning :data truncated. see first ten rows however many columns fit page.data truncated. see first ten rows however many columns fit page.get information total number rows columns tibble.get information total number rows columns tibble.get data type column. Note dbl underneath column name.get data type column. Note dbl underneath column name.Since prefer using tibble format, going overwrite data frame mtcars tibble version [recall discussion ]:can refer specific variables tibbles (data frames) $ operator:Sometimes want access number rows /columns data. can use nrow() ncol() functions respectively. Alternatively, can use dim() function get simultaneously.","code":"\nmtcars\n#>                      mpg cyl  disp  hp drat    wt  qsec vs\n#> Mazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0\n#> Mazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0\n#> Datsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1\n#> Hornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1\n#> Hornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0\n#> Valiant             18.1   6 225.0 105 2.76 3.460 20.22  1\n#> Duster 360          14.3   8 360.0 245 3.21 3.570 15.84  0\n#> Merc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1\n#> Merc 230            22.8   4 140.8  95 3.92 3.150 22.90  1\n#> Merc 280            19.2   6 167.6 123 3.92 3.440 18.30  1\n#> Merc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1\n#> Merc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0\n#> Merc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0\n#> Merc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0\n#> Cadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0\n#> Lincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0\n#> Chrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0\n#> Fiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1\n#> Honda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1\n#> Toyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1\n#> Toyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1\n#> Dodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0\n#> AMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0\n#> Camaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0\n#> Pontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0\n#> Fiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1\n#> Porsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0\n#> Lotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1\n#> Ford Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0\n#> Ferrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0\n#> Maserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0\n#> Volvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1\n#>                     am gear carb\n#> Mazda RX4            1    4    4\n#> Mazda RX4 Wag        1    4    4\n#> Datsun 710           1    4    1\n#> Hornet 4 Drive       0    3    1\n#> Hornet Sportabout    0    3    2\n#> Valiant              0    3    1\n#> Duster 360           0    3    4\n#> Merc 240D            0    4    2\n#> Merc 230             0    4    2\n#> Merc 280             0    4    4\n#> Merc 280C            0    4    4\n#> Merc 450SE           0    3    3\n#> Merc 450SL           0    3    3\n#> Merc 450SLC          0    3    3\n#> Cadillac Fleetwood   0    3    4\n#> Lincoln Continental  0    3    4\n#> Chrysler Imperial    0    3    4\n#> Fiat 128             1    4    1\n#> Honda Civic          1    4    2\n#> Toyota Corolla       1    4    1\n#> Toyota Corona        0    3    1\n#> Dodge Challenger     0    3    2\n#> AMC Javelin          0    3    2\n#> Camaro Z28           0    3    4\n#> Pontiac Firebird     0    3    2\n#> Fiat X1-9            1    4    1\n#> Porsche 914-2        1    5    2\n#> Lotus Europa         1    5    2\n#> Ford Pantera L       1    5    4\n#> Ferrari Dino         1    5    6\n#> Maserati Bora        1    5    8\n#> Volvo 142E           1    4    2\n# Install tibble package\ninstall.packages('tibble')\n# Load tibble package\nlibrary(tibble)\nas_tibble(mtcars)\n#> # A tibble: 32 × 11\n#>      mpg   cyl  disp    hp  drat    wt  qsec    vs    am\n#>    <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n#>  1  21       6  160    110  3.9   2.62  16.5     0     1\n#>  2  21       6  160    110  3.9   2.88  17.0     0     1\n#>  3  22.8     4  108     93  3.85  2.32  18.6     1     1\n#>  4  21.4     6  258    110  3.08  3.22  19.4     1     0\n#>  5  18.7     8  360    175  3.15  3.44  17.0     0     0\n#>  6  18.1     6  225    105  2.76  3.46  20.2     1     0\n#>  7  14.3     8  360    245  3.21  3.57  15.8     0     0\n#>  8  24.4     4  147.    62  3.69  3.19  20       1     0\n#>  9  22.8     4  141.    95  3.92  3.15  22.9     1     0\n#> 10  19.2     6  168.   123  3.92  3.44  18.3     1     0\n#> # … with 22 more rows, and 2 more variables: gear <dbl>,\n#> #   carb <dbl>\nmtcars <- as_tibble(mtcars)\nmtcars$mpg\n#>  [1] 21.0 21.0 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 17.8\n#> [12] 16.4 17.3 15.2 10.4 10.4 14.7 32.4 30.4 33.9 21.5 15.5\n#> [23] 15.2 13.3 19.2 27.3 26.0 30.4 15.8 19.7 15.0 21.4\nnrow(mtcars)\n#> [1] 32\nncol(mtcars)\n#> [1] 11\ndim(mtcars)\n#> [1] 32 11"},{"path":"lecture-1-r-and-r-basics.html","id":"summarizing-data-frames","chapter":"2 Lecture 1: R and R basics","heading":"2.9 Summarizing data frames","text":"can quickly summarize data summary() function:nice way get quick feel data.","code":"\nsummary(mtcars)\n#>       mpg             cyl             disp      \n#>  Min.   :10.40   Min.   :4.000   Min.   : 71.1  \n#>  1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8  \n#>  Median :19.20   Median :6.000   Median :196.3  \n#>  Mean   :20.09   Mean   :6.188   Mean   :230.7  \n#>  3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0  \n#>  Max.   :33.90   Max.   :8.000   Max.   :472.0  \n#>        hp             drat             wt       \n#>  Min.   : 52.0   Min.   :2.760   Min.   :1.513  \n#>  1st Qu.: 96.5   1st Qu.:3.080   1st Qu.:2.581  \n#>  Median :123.0   Median :3.695   Median :3.325  \n#>  Mean   :146.7   Mean   :3.597   Mean   :3.217  \n#>  3rd Qu.:180.0   3rd Qu.:3.920   3rd Qu.:3.610  \n#>  Max.   :335.0   Max.   :4.930   Max.   :5.424  \n#>       qsec             vs               am        \n#>  Min.   :14.50   Min.   :0.0000   Min.   :0.0000  \n#>  1st Qu.:16.89   1st Qu.:0.0000   1st Qu.:0.0000  \n#>  Median :17.71   Median :0.0000   Median :0.0000  \n#>  Mean   :17.85   Mean   :0.4375   Mean   :0.4062  \n#>  3rd Qu.:18.90   3rd Qu.:1.0000   3rd Qu.:1.0000  \n#>  Max.   :22.90   Max.   :1.0000   Max.   :1.0000  \n#>       gear            carb      \n#>  Min.   :3.000   Min.   :1.000  \n#>  1st Qu.:3.000   1st Qu.:2.000  \n#>  Median :4.000   Median :2.000  \n#>  Mean   :3.688   Mean   :2.812  \n#>  3rd Qu.:4.000   3rd Qu.:4.000  \n#>  Max.   :5.000   Max.   :8.000"},{"path":"lecture-1-r-and-r-basics.html","id":"subsetting-our-data","chapter":"2 Lecture 1: R and R basics","heading":"2.10 Subsetting our data","text":"Let us assume interested specific subset mtcars data. can create subset mtcars columns interested using select() function dplyr package. tibble package, install load dplyr package, something hope grown comfortable point:now create subset tibble mtcars data consist rows variables mpg, hp, wt, representing miles per gallon, gross horsepower, weight (1000 lbs) respectively.Dissecting select() function, see first supply function data set want subset (case mtcars) list selected variables. can also use select() function throw variables - prefix.","code":"\n# Install dplyr\ninstall.packages('dplyr')\n# Load dplyr package\nlibrary(dplyr)\nsmallData <- select(mtcars, mpg, hp, wt)\nsmallData\n#> # A tibble: 32 × 3\n#>      mpg    hp    wt\n#>    <dbl> <dbl> <dbl>\n#>  1  21     110  2.62\n#>  2  21     110  2.88\n#>  3  22.8    93  2.32\n#>  4  21.4   110  3.22\n#>  5  18.7   175  3.44\n#>  6  18.1   105  3.46\n#>  7  14.3   245  3.57\n#>  8  24.4    62  3.19\n#>  9  22.8    95  3.15\n#> 10  19.2   123  3.44\n#> # … with 22 more rows\nthrowOutVariables <- select(mtcars, -mpg, -hp, -wt)\nthrowOutVariables\n#> # A tibble: 32 × 8\n#>      cyl  disp  drat  qsec    vs    am  gear  carb\n#>    <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n#>  1     6  160   3.9   16.5     0     1     4     4\n#>  2     6  160   3.9   17.0     0     1     4     4\n#>  3     4  108   3.85  18.6     1     1     4     1\n#>  4     6  258   3.08  19.4     1     0     3     1\n#>  5     8  360   3.15  17.0     0     0     3     2\n#>  6     6  225   2.76  20.2     1     0     3     1\n#>  7     8  360   3.21  15.8     0     0     3     4\n#>  8     4  147.  3.69  20       1     0     4     2\n#>  9     4  141.  3.92  22.9     1     0     4     2\n#> 10     6  168.  3.92  18.3     1     0     4     4\n#> # … with 22 more rows"},{"path":"lecture-1-r-and-r-basics.html","id":"creating-new-variables","chapter":"2 Lecture 1: R and R basics","heading":"2.11 Creating new variables","text":"two ways making new variables: $ operator mutate() function dplyr package. Let’s create two new variables randomly drawing numbers interval [0, 1].Note overwrite smallData used mutate() function.","code":"\nsmallData$var1 <- runif(n = nrow(smallData))\nsmallData <- mutate(smallData, var2 = runif(n = nrow(smallData)))\nsmallData\n#> # A tibble: 32 × 5\n#>      mpg    hp    wt   var1  var2\n#>    <dbl> <dbl> <dbl>  <dbl> <dbl>\n#>  1  21     110  2.62 0.190  0.334\n#>  2  21     110  2.88 0.0457 0.192\n#>  3  22.8    93  2.32 0.473  0.131\n#>  4  21.4   110  3.22 0.462  0.597\n#>  5  18.7   175  3.44 0.137  0.607\n#>  6  18.1   105  3.46 0.989  0.813\n#>  7  14.3   245  3.57 0.141  0.550\n#>  8  24.4    62  3.19 0.661  0.218\n#>  9  22.8    95  3.15 0.340  0.428\n#> 10  19.2   123  3.44 0.248  0.374\n#> # … with 22 more rows"},{"path":"lecture-1-r-and-r-basics.html","id":"plotting-our-data","chapter":"2 Lecture 1: R and R basics","heading":"2.12 Plotting our data","text":"create plots use ggplot2 package installed earlier. cover plots detail later now example create scatter plot using ggplot() geom_point() functions.","code":"\nggplot(data = smallData, aes(x = var1, y = var2)) + \n    geom_point()"},{"path":"lecture-1-r-and-r-basics.html","id":"chaining-multiple-functions-together","chapter":"2 Lecture 1: R and R basics","heading":"2.13 Chaining multiple functions together","text":"far used select(), mutate(), ggplot() separately can actually chain together pipe operator %>% package magrittr. use pipe became ubiquitous got added R built-operator version 4.1. “base” R pipe |>.Let’s recreate plot previous section pipes.","code":"\ninstall.packages('magrittr')\nlibrary(magrittr)\nmtcars %>%\n    mutate(var1 = runif(nrow(mtcars)), \n           var2 = runif(nrow(mtcars))) %>% \n    select(var1, var2) %>%  # this step is unnecessary\n    ggplot(aes(x = var1, y = var2)) + \n        geom_point()"},{"path":"lecture-1-r-and-r-basics.html","id":"the-tidyverse-package","chapter":"2 Lecture 1: R and R basics","heading":"2.14 The tidyverse package","text":"course lecture installed loaded ggplot2, tibble, dplyr, magrittr. packages () belong tidyverse collection. loading tidyverse load ggplot2, dplyr, tidyr, readr, purrr, tibble, stringr, forcats. can read package : https://www.tidyverse.org/packages/install load tidyverse simply run:","code":"\ninstall.packages('tidyverse')\nlibrary(tidyverse)"},{"path":"lecture-2-data-wrangling.html","id":"lecture-2-data-wrangling","chapter":"3 Lecture 2: Data wrangling","heading":"3 Lecture 2: Data wrangling","text":"","code":"\nlibrary(tidyverse)\nlibrary(here) # relative paths\nlibrary(readxl)"},{"path":"lecture-2-data-wrangling.html","id":"r-projects-and-workflows","chapter":"3 Lecture 2: Data wrangling","heading":"3.1 R Projects and workflows","text":"Last week used mtcars data set included R. want load external data tell R find . tell R files , need supply R file paths. good opportunity introduce basic workflow.Let us assume tasked performing preliminary analysis data set. supervisor expects us :Present results couple days.Present results couple days.Hand preliminary analysis resident data scientist downstream modeling.Hand preliminary analysis resident data scientist downstream modeling.thus two things keep mind; need perform preliminary analysis need way data scientist can continue work painlessly possible. need get organized. means:project work , create dedicated folder specific project. files relevant project placed project folder.project work , create dedicated folder specific project. files relevant project placed project folder.project folder compartmentalized! , multiple subdirectories within project folder, serving specific purpose.project folder compartmentalized! , multiple subdirectories within project folder, serving specific purpose.example layout hypothetical scenario:created project folder relevant structure next thing create R project file. creating R project, telling R base operations (working directory). means can use relative paths, namely, instead writing full file path names like C:\\Documents\\ourProject\\data\\someData.csv suffices write data\\someData.csv.Finally, recommend use package .Rmd files use location working directory. Using file structure example, since .Rmd file saved within report try point data file someData.csv data/someData.csv met error. looks location R project file .Rproj sets file paths relative .Rproj.completing preliminary analysis can simply hand ourProject directory data scientist. reproducible research basic form.","code":"Within folder ourProject:\n-code/      code to wrangle original data\n-data/      output of code\n-raw/       original data files\n-report/    our .Rmd file"},{"path":"lecture-2-data-wrangling.html","id":"more-intricate-workflow","chapter":"3 Lecture 2: Data wrangling","heading":"3.1.1 More intricate workflow","text":"Look Snakemake, renv, github, DiagrammeR, Docker.","code":""},{"path":"lecture-2-data-wrangling.html","id":"the-anatomy-of-a-file-and-external-data","chapter":"3 Lecture 2: Data wrangling","heading":"3.2 The anatomy of a file and external data","text":"Consider following data:first line called header. names columns (sometimes referred fields) stated. Note data comes header line. functions read data memory assume header line don’t explicitly state file one. lines follow header store data values. semicolon ; value acts delimiter. delimiter helps R map values respective columns. delimiter necessarily ;. non exhaustive list:Comma ,. Data comma delimiter typically stored .csv files. .csv stands Comma-Separated Values. can read .csv files read_csv() function readr.Comma ,. Data comma delimiter typically stored .csv files. .csv stands Comma-Separated Values. can read .csv files read_csv() function readr.Semicolon ;. Data semicolon delimiter typically stored .csv files well. can read data semicolon-separated values read_csv2() function readr.Semicolon ;. Data semicolon delimiter typically stored .csv files well. can read data semicolon-separated values read_csv2() function readr.Tab \\t. Data \\t (tab) delimiter typically stored .csv .tsv files. .tsv stands Tab-Separated Values. can read .tsv files read_tsv() function readr.Tab \\t. Data \\t (tab) delimiter typically stored .csv .tsv files. .tsv stands Tab-Separated Values. can read .tsv files read_tsv() function readr.delimiters. can use general function read_delim() specify (ones mentioned ). Finally, Excel spreadsheets .xls .xlsx quite common well. read R can use readxl package.examples use , readr, readxl read dummy data created (see Appendix).","code":"id;type;age\n1;a;30\n2;b;37\n3;a;42\ndummyTSV <- read_tsv(here('data', 'l2_data_tsv.txt'))\n#> Rows: 10000 Columns: 4\n#> ── Column specification ──────────────────────────────────────────────────\n#> Delimiter: \"\\t\"\n#> chr (1): type\n#> dbl (3): id, age, metric1\n#> \n#> ℹ Use `spec()` to retrieve the full column specification for this data.\n#> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\ndummyCSV <- read_csv(here('data', 'l2_data_csv.txt'))\n#> Rows: 10000 Columns: 4\n#> ── Column specification ──────────────────────────────────────────────────\n#> Delimiter: \",\"\n#> chr (1): type\n#> dbl (3): id, age, metric1\n#> \n#> ℹ Use `spec()` to retrieve the full column specification for this data.\n#> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\ndumyCSV2 <- read_csv2(here('data', 'l2_data_csv2.txt'))\n#> ℹ Using \"','\" as decimal and \"'.'\" as grouping mark. Use `read_delim()` for more control.\n#> Rows: 10000 Columns: 4\n#> ── Column specification ──────────────────────────────────────────────────\n#> Delimiter: \";\"\n#> chr (1): type\n#> dbl (2): id, age\n#> \n#> ℹ Use `spec()` to retrieve the full column specification for this data.\n#> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\ndummyXLSX <- read_xlsx(here('data', 'l2_data_excel.xlsx'))\ndummyXLSX\n#> # A tibble: 10,000 × 4\n#>       id type    age metric1\n#>    <dbl> <chr> <dbl>   <dbl>\n#>  1     1 a        32    31.4\n#>  2     2 c        33    42.4\n#>  3     3 a        41    40.4\n#>  4     4 b        37    75.7\n#>  5     5 a        35    66.9\n#>  6     6 c        37    64.8\n#>  7     7 c        40    52.0\n#>  8     8 b        34    35.7\n#>  9     9 b        36    30.2\n#> 10    10 c        36    34.5\n#> # … with 9,990 more rows"},{"path":"lecture-2-data-wrangling.html","id":"factors","chapter":"3 Lecture 2: Data wrangling","heading":"3.3 Factors","text":"continue use dummyXLSX. Let’s get overview data glimpse() function dplyr:glimpse() see four variables, three type double one character. Let’s use summary() function last lecture probe data:looks well except type variable takes values , b, c. like see just many observations fall type. good opportunity discuss factor variables. factor variable variable levels labels. levels type aforementioned , b, c, (values type takes) labels contains “name” level. example, participant diabetes, b participant heart disease c controls. One levels factor variable baseline. Unless told otherwise, R arrange levels alphabetical order type, baseline since first letter alphabet.Let’s create new variable based schema .","code":"\nglimpse(dummyXLSX)\n#> Rows: 10,000\n#> Columns: 4\n#> $ id      <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,…\n#> $ type    <chr> \"a\", \"c\", \"a\", \"b\", \"a\", \"c\", \"c\", \"b\", \"b…\n#> $ age     <dbl> 32, 33, 41, 37, 35, 37, 40, 34, 36, 36, 37…\n#> $ metric1 <dbl> 31.41350, 42.44945, 40.40657, 75.74094, 66…\nsummary(dummyXLSX)\n#>        id            type                age       \n#>  Min.   :    1   Length:10000       Min.   :23.00  \n#>  1st Qu.: 2501   Class :character   1st Qu.:32.00  \n#>  Median : 5000   Mode  :character   Median :35.00  \n#>  Mean   : 5000                      Mean   :34.51  \n#>  3rd Qu.: 7500                      3rd Qu.:37.00  \n#>  Max.   :10000                      Max.   :45.00  \n#>     metric1      \n#>  Min.   :-13.54  \n#>  1st Qu.: 39.62  \n#>  Median : 49.94  \n#>  Mean   : 49.82  \n#>  3rd Qu.: 59.91  \n#>  Max.   :102.87\ndummyXLSX <- \n    dummyXLSX %>% \n    mutate(factorType = factor(type, \n                               levels = c('a', 'b', 'c'), \n                               labels = c('Diabetes', 'Heart disease', 'Control')))\nsummary(dummyXLSX)\n#>        id            type                age       \n#>  Min.   :    1   Length:10000       Min.   :23.00  \n#>  1st Qu.: 2501   Class :character   1st Qu.:32.00  \n#>  Median : 5000   Mode  :character   Median :35.00  \n#>  Mean   : 5000                      Mean   :34.51  \n#>  3rd Qu.: 7500                      3rd Qu.:37.00  \n#>  Max.   :10000                      Max.   :45.00  \n#>     metric1               factorType  \n#>  Min.   :-13.54   Diabetes     :3424  \n#>  1st Qu.: 39.62   Heart disease:3292  \n#>  Median : 49.94   Control      :3284  \n#>  Mean   : 49.82                       \n#>  3rd Qu.: 59.91                       \n#>  Max.   :102.87"},{"path":"lecture-2-data-wrangling.html","id":"filter","chapter":"3 Lecture 2: Data wrangling","heading":"3.4 Filter","text":"Now, say want restrict analysis participants scored 50 “test” dummyXLSX (variable metric1). useful tool filter() function dplyr allows us subset data based logic operations.Note half observations gone. can filter data even , example restricting analysis participants Diabetes Heart disease scored higher 50 metric1.resulting data even smaller. list operators can use compare values:>: Used \\(x > y\\) read x greater y.>: Used \\(x > y\\) read x greater y.<: Used \\(x < y\\) read x lesser y.<: Used \\(x < y\\) read x lesser y.>=: Used \\(x \\geq y\\) read x greater equal y>=: Used \\(x \\geq y\\) read x greater equal y<=: Used \\(x \\leq y\\) read x lesser equal y.<=: Used \\(x \\leq y\\) read x lesser equal y.==: Used \\(x = y\\) read x y. double == mistake.==: Used \\(x = y\\) read x y. double == mistake.small example ==:","code":"\ndummyXLSX %>%\n    filter(metric1 > 50)\n#> # A tibble: 4,978 × 5\n#>       id type    age metric1 factorType   \n#>    <dbl> <chr> <dbl>   <dbl> <fct>        \n#>  1     4 b        37    75.7 Heart disease\n#>  2     5 a        35    66.9 Diabetes     \n#>  3     6 c        37    64.8 Control      \n#>  4     7 c        40    52.0 Control      \n#>  5    13 a        33    62.7 Diabetes     \n#>  6    15 b        37    60.4 Heart disease\n#>  7    17 b        38    75.3 Heart disease\n#>  8    19 c        38    61.5 Control      \n#>  9    20 a        32    76.1 Diabetes     \n#> 10    21 c        32    61.0 Control      \n#> # … with 4,968 more rows\ndummyXLSX %>%\n    filter(metric1 > 50, type %in% c('a', 'b'))\n#> # A tibble: 3,328 × 5\n#>       id type    age metric1 factorType   \n#>    <dbl> <chr> <dbl>   <dbl> <fct>        \n#>  1     4 b        37    75.7 Heart disease\n#>  2     5 a        35    66.9 Diabetes     \n#>  3    13 a        33    62.7 Diabetes     \n#>  4    15 b        37    60.4 Heart disease\n#>  5    17 b        38    75.3 Heart disease\n#>  6    20 a        32    76.1 Diabetes     \n#>  7    23 a        31    54.4 Diabetes     \n#>  8    25 a        34    51.7 Diabetes     \n#>  9    26 b        29    57.0 Heart disease\n#> 10    30 b        32    54.7 Heart disease\n#> # … with 3,318 more rows\ndummyXLSX %>%\n    filter(factorType == 'Diabetes')\n#> # A tibble: 3,424 × 5\n#>       id type    age metric1 factorType\n#>    <dbl> <chr> <dbl>   <dbl> <fct>     \n#>  1     1 a        32    31.4 Diabetes  \n#>  2     3 a        41    40.4 Diabetes  \n#>  3     5 a        35    66.9 Diabetes  \n#>  4    12 a        30    34.8 Diabetes  \n#>  5    13 a        33    62.7 Diabetes  \n#>  6    14 a        35    49.6 Diabetes  \n#>  7    20 a        32    76.1 Diabetes  \n#>  8    22 a        36    49.0 Diabetes  \n#>  9    23 a        31    54.4 Diabetes  \n#> 10    24 a        35    33.2 Diabetes  \n#> # … with 3,414 more rows"},{"path":"lecture-2-data-wrangling.html","id":"ifelse-and-case_when","chapter":"3 Lecture 2: Data wrangling","heading":"3.5 ifelse and case_when","text":"Assume want create new variable participants scored higher 50 metric1 labeled high scored lower equal 50 labeled low. can use ifelse() function end. First example dissection.ifelse() work? function takes three inputs: test want perform (metric1 score higher 50?), action test true (High), action test false (metric1 \\(\\leq\\) 50; Low). can create complicated tests & | operators symbolize respectively.Assume want create variable combines multiple scenarios. possible nest ifelse() within () quickly becomes obnoxious confusing. case_when() comes allows us strictly define value new variable takes based scenario. one:","code":"\ndummyXLSX <-\n    dummyXLSX %>%\n    mutate(scoreCat = ifelse(metric1 > 50, 'High', 'Low'))\ndummyXLSX\n#> # A tibble: 10,000 × 6\n#>       id type    age metric1 factorType    scoreCat\n#>    <dbl> <chr> <dbl>   <dbl> <fct>         <chr>   \n#>  1     1 a        32    31.4 Diabetes      Low     \n#>  2     2 c        33    42.4 Control       Low     \n#>  3     3 a        41    40.4 Diabetes      Low     \n#>  4     4 b        37    75.7 Heart disease High    \n#>  5     5 a        35    66.9 Diabetes      High    \n#>  6     6 c        37    64.8 Control       High    \n#>  7     7 c        40    52.0 Control       High    \n#>  8     8 b        34    35.7 Heart disease Low     \n#>  9     9 b        36    30.2 Heart disease Low     \n#> 10    10 c        36    34.5 Control       Low     \n#> # … with 9,990 more rows\ndummyXLSX <-\n    dummyXLSX %>%\n    mutate(scoreCatAge = ifelse(metric1 > 50 & age > 30, 'Both true', 'One or both false'))\ndummyXLSX %>%\n    select(age, metric1, scoreCatAge)\n#> # A tibble: 10,000 × 3\n#>      age metric1 scoreCatAge      \n#>    <dbl>   <dbl> <chr>            \n#>  1    32    31.4 One or both false\n#>  2    33    42.4 One or both false\n#>  3    41    40.4 One or both false\n#>  4    37    75.7 Both true        \n#>  5    35    66.9 Both true        \n#>  6    37    64.8 Both true        \n#>  7    40    52.0 Both true        \n#>  8    34    35.7 One or both false\n#>  9    36    30.2 One or both false\n#> 10    36    34.5 One or both false\n#> # … with 9,990 more rows\ndummyXLSX %>%\n    mutate(orStatement = ifelse(metric1 > 50 | age < 30, 'One or both true', 'Both false')) %>%\n    select(age, metric1, orStatement)\n#> # A tibble: 10,000 × 3\n#>      age metric1 orStatement     \n#>    <dbl>   <dbl> <chr>           \n#>  1    32    31.4 Both false      \n#>  2    33    42.4 Both false      \n#>  3    41    40.4 Both false      \n#>  4    37    75.7 One or both true\n#>  5    35    66.9 One or both true\n#>  6    37    64.8 One or both true\n#>  7    40    52.0 One or both true\n#>  8    34    35.7 Both false      \n#>  9    36    30.2 Both false      \n#> 10    36    34.5 Both false      \n#> # … with 9,990 more rows\ndummyXLSX %>%\n    mutate(scenarios = case_when(age > 35 & metric1 < 50 & type == 'c' ~ 'Control, > 35, < 50',\n                                 age > 35 & metric1 > 50 & type == 'b' ~ 'Heart, > 35, > 50',\n                                 age <= 35 & metric1 <= 50 & type == 'a' ~ 'Diabetes, <= 35, <= 50',\n                                 TRUE ~ 'Whatever does not fit')) %>% \n    select(age, metric1, type, scenarios)\n#> # A tibble: 10,000 × 4\n#>      age metric1 type  scenarios             \n#>    <dbl>   <dbl> <chr> <chr>                 \n#>  1    32    31.4 a     Diabetes, <= 35, <= 50\n#>  2    33    42.4 c     Whatever does not fit \n#>  3    41    40.4 a     Whatever does not fit \n#>  4    37    75.7 b     Heart, > 35, > 50     \n#>  5    35    66.9 a     Whatever does not fit \n#>  6    37    64.8 c     Whatever does not fit \n#>  7    40    52.0 c     Whatever does not fit \n#>  8    34    35.7 b     Whatever does not fit \n#>  9    36    30.2 b     Whatever does not fit \n#> 10    36    34.5 c     Control, > 35, < 50   \n#> # … with 9,990 more rows"},{"path":"lecture-2-data-wrangling.html","id":"summarize-and-group_by","chapter":"3 Lecture 2: Data wrangling","heading":"3.6 summarize and group_by","text":"preliminary analysis can divided two (roughly) parts: numerical analysis graphical analysis.\nCommon statistics asked compute sample size, mean, standard deviation, minimum, maximum, quantiles (typically 2.5% 97.5% percentiles) sample. end following functions: n(), mean(), sd(), min(), max(), quantile(). functions take numerical vector return statistic interest. discuss statistics detail next week.Let’s compute statistics variable age:alternative way use summarize() function:methods yield answer. Now, imagine want compute statistics stratified type patient. can done base R function recommend using summarize() function group_by() function. put simply, group_by() function allows us define groups “wrangle” based groups. Let’s see happens.code basically save one additional line. Pretty convenient, right?","code":"\nnrow(dummyXLSX)\n#> [1] 10000\nmean(dummyXLSX$age)\n#> [1] 34.5138\nsd(dummyXLSX$age)\n#> [1] 3.182487\nmin(dummyXLSX$age)\n#> [1] 23\nmax(dummyXLSX$age)\n#> [1] 45\nquantile(dummyXLSX$age, probs = 0.025)\n#> 2.5% \n#>   28\nquantile(dummyXLSX$age, probs = 0.975)\n#> 97.5% \n#>    41\ndummyXLSX %>%\n    summarize(sampleSize = n(), \n              mean = mean(age), \n              standardDev = sd(age), \n              mini = min(age), \n              maxi = max(age), \n              q025 = quantile(age, 0.025), \n              q975 = quantile(age, 0.975))\n#> # A tibble: 1 × 7\n#>   sampleSize  mean standardDev  mini  maxi  q025  q975\n#>        <int> <dbl>       <dbl> <dbl> <dbl> <dbl> <dbl>\n#> 1      10000  34.5        3.18    23    45    28    41\ndummyXLSX %>% \n    group_by(type) %>%\n    summarize(sampleSize = n(), \n              mean = mean(age), \n              standardDev = sd(age), \n              mini = min(age), \n              maxi = max(age), \n              q025 = quantile(age, 0.025), \n              q975 = quantile(age, 0.975))\n#> # A tibble: 3 × 8\n#>   type  sampleSize  mean standardDev  mini  maxi  q025  q975\n#>   <chr>      <int> <dbl>       <dbl> <dbl> <dbl> <dbl> <dbl>\n#> 1 a           3424  34.5        3.17    23    45    28    41\n#> 2 b           3292  34.5        3.16    24    45    28    41\n#> 3 c           3284  34.5        3.21    24    45    28    41"},{"path":"lecture-2-data-wrangling.html","id":"gather-and-spread","chapter":"3 Lecture 2: Data wrangling","heading":"3.7 gather and spread","text":"Data can long wide. data wide one column variable. Imagine conducted study took measurements twice research period. data look something like :Now, might actually beneficial measurement row. measurement row say data long. convert wide long use gather() function. actually superseeded pivot_longer() ’m used gather(). function gather() takes : names two new columns creating (“key”, “value”) selection columns. want measurement line selection measure1 measure2.Alternatively can just use - prefix tell R columns ignore:go long wide function spread() pivot_wider(). encourage try .","code":"\nwideExample <- \n    tibble(id = 1:3, \n       measure1 = rnorm(3), \n       measure2 = rnorm(3))\nwideExample\n#> # A tibble: 3 × 3\n#>      id measure1 measure2\n#>   <int>    <dbl>    <dbl>\n#> 1     1    0.883   -0.338\n#> 2     2    0.683   -0.931\n#> 3     3    1.30    -0.239\nwideExample %>%\n    gather(measurement, value, measure1, measure2)\n#> # A tibble: 6 × 3\n#>      id measurement  value\n#>   <int> <chr>        <dbl>\n#> 1     1 measure1     0.883\n#> 2     2 measure1     0.683\n#> 3     3 measure1     1.30 \n#> 4     1 measure2    -0.338\n#> 5     2 measure2    -0.931\n#> 6     3 measure2    -0.239\nwideExample %>%\n    gather(measurement, value, -id)\n#> # A tibble: 6 × 3\n#>      id measurement  value\n#>   <int> <chr>        <dbl>\n#> 1     1 measure1     0.883\n#> 2     2 measure1     0.683\n#> 3     3 measure1     1.30 \n#> 4     1 measure2    -0.338\n#> 5     2 measure2    -0.931\n#> 6     3 measure2    -0.239"},{"path":"lecture-2-data-wrangling.html","id":"why-this-is-useful","chapter":"3 Lecture 2: Data wrangling","heading":"3.7.1 Why this is useful","text":"long format often useful want fit models repeated measurements. ’s also useful want summarize multiple variables simultaneously. Recall summarize() section summarized age. many variables might interested summarizing metric1. example combine gather(), group_by(), summarize().another example include type participants.","code":"\n# Let's add another variable to dummyXLSX\ndummyXLSX %>%\n    mutate(someVar = rnorm(n = n())) %>%\n    gather(variables, values, age, metric1, someVar) %>%\n    group_by(variables) %>%\n    summarize(mean = mean(values), \n              sd = sd(values),\n              median = median(values))\n#> # A tibble: 3 × 4\n#>   variables    mean     sd  median\n#>   <chr>       <dbl>  <dbl>   <dbl>\n#> 1 age       34.5     3.18  35     \n#> 2 metric1   49.8    15.0   49.9   \n#> 3 someVar    0.0160  0.995  0.0306\ndummyXLSX %>%\n    mutate(someVar = rnorm(n = n())) %>%\n    gather(variables, values, age, metric1, someVar) %>%\n    group_by(variables, type) %>%\n    summarize(mean = mean(values), \n              sd = sd(values),\n              median = median(values)) %>%\n    arrange(type)\n#> `summarise()` has grouped output by 'variables'. You can override using the `.groups` argument.\n#> # A tibble: 9 × 5\n#> # Groups:   variables [3]\n#>   variables type      mean     sd  median\n#>   <chr>     <chr>    <dbl>  <dbl>   <dbl>\n#> 1 age       a     34.5      3.17  35     \n#> 2 metric1   a     49.5     14.9   49.5   \n#> 3 someVar   a      0.00236  0.997  0.0144\n#> 4 age       b     34.5      3.16  34     \n#> 5 metric1   b     50.1     15.1   50.2   \n#> 6 someVar   b      0.00308  1.02   0.0126\n#> 7 age       c     34.5      3.21  35     \n#> 8 metric1   c     49.8     15.2   50.1   \n#> 9 someVar   c      0.0200   1.00   0.0336"},{"path":"lecture-2-data-wrangling.html","id":"appendix","chapter":"3 Lecture 2: Data wrangling","heading":"3.8 Appendix","text":"","code":"\nlibrary(here)\nlibrary(xlsx)\n\n# Set seed for reproducibility\nset.seed(1)\n\nn <- 10000\n\n# Create fake data with research ID, \n# participant type, age, and performance\n# metric score\n\nid <- 1:n\ntype <- sample(x = c('a', 'b', 'c'), size = n, replace = T)\nage <- floor(rnorm(n = n, mean = 35, sd = sqrt(10)))\nmetric1 <- rnorm(n = n, mean = 50, sd = 15)\n\nd <- data.frame(id, type, age, metric1)\n\n# CSV\nwrite.table(x = d, file = here('data', 'l2_data_csv.txt'), \n            row.names = F, quote = F, sep = ',')\n# CSV2\nwrite.table(x = d, file = here('data', 'l2_data_csv2.txt'), \n            row.names = F, quote = F, sep = ';')\n# TSV\nwrite.table(x = d, file = here('data', 'l2_data_tsv.txt'), \n            row.names = F, quote = F, sep = '\\t')\n# Excel\nwrite.xlsx(x = d, file = here('data', 'l2_data_excel.xlsx'), \n           row.names = F, sheetName = 'DummyData')"},{"path":"lecture-3-plots-and-table1.html","id":"lecture-3-plots-and-table1","chapter":"4 Lecture 3: Plots and table1","heading":"4 Lecture 3: Plots and table1","text":"begin loading data:data pulse based Icelandic data set pulse students measured two different time points. two groups; control group stationary two measurements, case group active two measurements. original data Icelandic translate columns levels factor variables. also omitted variables simplicity’s sake. curious translation can look Appendix bottom document.goal today’s lecture compare two groups. graphically tables. begin using glimpse() function get brief overview data.see three chr variables (smokes, drinks, intervention) want cast factor variables. Furthermore, even though sex dbl cast factor variable well.done , now use summary() function get quick overview data:first time see missing values. Note NA’s height, weight, firstPulse, secondPulse, . Missing values can imputed beyond scope course. Instead, just going remove data na.omit() function.Let’s use summary() function see changed:NA’s gone cost reduced data set. conduct research important try minimize missing values affects power study.","code":"\nlibrary(tidyverse)  # General data wrangling & plots\nlibrary(knitr)      # Tables \nlibrary(kableExtra) # Fancy tables\nlibrary(cowplot)    # For pretty plots\nlibrary(table1)     # Table1\nlibrary(here)       # paths\npulse <- read_csv2(\"https://notendur.hi.is/thj73/data/pulseEn.csv\")\npulse\n#> # A tibble: 471 × 10\n#>       id height weight   age   sex smokes drinks firstPulse\n#>    <dbl>  <dbl>  <dbl> <dbl> <dbl> <chr>  <chr>       <dbl>\n#>  1     1    161     60    23     1 no     no             83\n#>  2     2    185    115    52     2 <NA>   yes            80\n#>  3     3    167     NA    22     1 no     yes            43\n#>  4     4    174     67    21     1 no     yes            76\n#>  5     5    163     57    20     1 no     yes            71\n#>  6     6    175     59    20     1 no     yes            65\n#>  7     7    178     70    39     1 <NA>   yes            77\n#>  8     8    191     94    21     2 no     yes            79\n#>  9     9    176     68    20     1 no     yes            73\n#> 10    10    176     82    70     2 no     yes            65\n#> # … with 461 more rows, and 2 more variables:\n#> #   secondPulse <dbl>, intervention <chr>\nglimpse(pulse)\n#> Rows: 471\n#> Columns: 10\n#> $ id           <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12…\n#> $ height       <dbl> 161, 185, 167, 174, 163, 175, 178, 19…\n#> $ weight       <dbl> 60, 115, NA, 67, 57, 59, 70, 94, 68, …\n#> $ age          <dbl> 23, 52, 22, 21, 20, 20, 39, 21, 20, 7…\n#> $ sex          <dbl> 1, 2, 1, 1, 1, 1, 1, 2, 1, 2, 2, 2, 1…\n#> $ smokes       <chr> \"no\", NA, \"no\", \"no\", \"no\", \"no\", NA,…\n#> $ drinks       <chr> \"no\", \"yes\", \"yes\", \"yes\", \"yes\", \"ye…\n#> $ firstPulse   <dbl> 83, 80, 43, 76, 71, 65, 77, 79, 73, 6…\n#> $ secondPulse  <dbl> 84, 103, 52, 105, 68, 65, 75, 83, 90,…\n#> $ intervention <chr> \"stationary\", \"active\", \"stationary\",…\npulse <-\n    pulse %>% \n    mutate(sex = factor(sex), \n           smokes = factor(smokes), \n           drinks = factor(drinks), \n           intervention = factor(intervention, \n                                 levels = c('stationary', 'active')))\nsummary(pulse)\n#>        id            height           weight      \n#>  Min.   :  1.0   Min.   : 150.0   Min.   : 40.00  \n#>  1st Qu.:118.5   1st Qu.: 166.0   1st Qu.: 60.00  \n#>  Median :236.0   Median : 172.0   Median : 70.00  \n#>  Mean   :236.0   Mean   : 179.6   Mean   : 85.38  \n#>  3rd Qu.:353.5   3rd Qu.: 181.0   3rd Qu.: 81.00  \n#>  Max.   :471.0   Max.   :1725.0   Max.   :846.00  \n#>                  NA's   :1        NA's   :10      \n#>       age        sex      smokes     drinks   \n#>  Min.   :19.00   1:307   no  :412   no  : 76  \n#>  1st Qu.:20.00   2:164   yes : 46   yes :389  \n#>  Median :22.00           NA's: 13   NA's:  6  \n#>  Mean   :24.24                                \n#>  3rd Qu.:25.00                                \n#>  Max.   :70.00                                \n#>                                               \n#>    firstPulse      secondPulse         intervention\n#>  Min.   : 42.00   Min.   : 42.00   stationary:286  \n#>  1st Qu.: 64.00   1st Qu.: 68.00   active    :183  \n#>  Median : 71.50   Median : 77.00   NA's      :  2  \n#>  Mean   : 71.98   Mean   : 82.21                   \n#>  3rd Qu.: 80.00   3rd Qu.: 93.00                   \n#>  Max.   :120.00   Max.   :162.00                   \n#>  NA's   :17       NA's   :14\npulse <- \n    pulse %>%\n    na.omit()\nsummary(pulse)\n#>        id            height           weight      \n#>  Min.   :  1.0   Min.   : 150.0   Min.   : 40.00  \n#>  1st Qu.:116.2   1st Qu.: 165.0   1st Qu.: 60.00  \n#>  Median :237.5   Median : 172.0   Median : 70.00  \n#>  Mean   :237.1   Mean   : 180.3   Mean   : 84.89  \n#>  3rd Qu.:358.8   3rd Qu.: 181.0   3rd Qu.: 81.00  \n#>  Max.   :471.0   Max.   :1725.0   Max.   :846.00  \n#>       age        sex     smokes    drinks   \n#>  Min.   :19.00   1:275   no :384   no : 65  \n#>  1st Qu.:20.00   2:151   yes: 42   yes:361  \n#>  Median :22.00                              \n#>  Mean   :23.92                              \n#>  3rd Qu.:25.00                              \n#>  Max.   :70.00                              \n#>    firstPulse      secondPulse         intervention\n#>  Min.   : 43.00   Min.   : 45.00   stationary:257  \n#>  1st Qu.: 64.00   1st Qu.: 68.00   active    :169  \n#>  Median : 72.00   Median : 78.00                   \n#>  Mean   : 72.21   Mean   : 82.59                   \n#>  3rd Qu.: 80.00   3rd Qu.: 94.75                   \n#>  Max.   :120.00   Max.   :162.00"},{"path":"lecture-3-plots-and-table1.html","id":"table-1","chapter":"4 Lecture 3: Plots and table1","heading":"4.1 Table 1","text":"important table create table 1. Table 1 contains descriptive statistics cohort, preferably stratified case/control, necessarily. called table 1 often first table see paper.many ways create table 1 discuss table1 package. Using table1 pretty simple. begin writing variables interested tell function stratify group interest. us, intervention variable one want stratify .see good summary data: means, ranges, standard deviations, number observations, counts percentages. examining table get good feel data relatively short time. Can see difference two groups? see anything weird table?","code":"\ntable1(~ firstPulse + secondPulse + height + weight + age + sex + smokes + drinks | intervention, data = pulse)"},{"path":"lecture-3-plots-and-table1.html","id":"note-using-tidyverse-functions","chapter":"4 Lecture 3: Plots and table1","heading":"4.1.1 Note: using tidyverse functions","text":"actually possible recreate table group_by(), summarize(), . ’s quite involved want challenge encourage try .","code":""},{"path":"lecture-3-plots-and-table1.html","id":"summarizing-the-data-with-plots","chapter":"4 Lecture 3: Plots and table1","heading":"4.2 Summarizing the data with plots","text":"Tables good picture worth thousand words; ’s time create plots.","code":""},{"path":"lecture-3-plots-and-table1.html","id":"scatter-plot-or-geom_point","chapter":"4 Lecture 3: Plots and table1","heading":"4.2.1 Scatter plot or geom_point()","text":"seen one . Let’s plot values firstPulse secondPulse color points based whether observation active stationary group.lot information simple graph. see :stationary group, higher starting pulse, higher second pulse (general).stationary group, higher starting pulse, higher second pulse (general).active group much higher second pulse.active group much higher second pulse.One individual stationary group much higher second pulse relative starting pulse. ?One individual stationary group much higher second pulse relative starting pulse. ?plot nice invest time making aesthetically pleasing. pick sharper colors, relabel axes, move legend bottom graph. also load theme plot cowplots package.Note “aesthetically pleasing” relative.","code":"\npulse %>%\n    ggplot(aes(x = firstPulse, y = secondPulse, color = intervention)) +\n    geom_point()\npulse %>%\n    ggplot(aes(x = firstPulse, y = secondPulse, color = intervention)) +\n    geom_point() +\n    scale_color_brewer(type = 'seq', palette = 'Set1') + # Change color\n    labs(x = 'First measurement', \n         y = 'Second measurement', \n         color = 'Intervention') + # Change labels on axes, change legend\n    theme_cowplot() +    # change the theme\n    theme(legend.position = 'bottom')"},{"path":"lecture-3-plots-and-table1.html","id":"data-distribution-plots-or-geom_histogram","chapter":"4 Lecture 3: Plots and table1","heading":"4.2.2 Data distribution plots or geom_histogram()","text":"multiple continuous variables play . Let’s look distribution pulse measurements.look first picture see “mass” 60 80. agreement saw table 1. second picture shows distribution bimodal; , seem two peaks. also consistent seen two groups active group much larger second pulse measurement.Maybe color second histogram using intervention variable can better see two peaks.better invest time cleaning graph. repeat steps scatter plot one addition; make colors little transparent alpha parameter two histograms overlap .","code":"\npulse %>%\n    ggplot(aes(x = firstPulse)) +\n    geom_histogram()\n#> `stat_bin()` using `bins = 30`. Pick better value with\n#> `binwidth`.\npulse %>%\n    ggplot(aes(x = secondPulse)) +\n    geom_histogram()\n#> `stat_bin()` using `bins = 30`. Pick better value with\n#> `binwidth`.\npulse %>%\n    ggplot(aes(x = secondPulse, fill = intervention)) +\n    geom_histogram()\n#> `stat_bin()` using `bins = 30`. Pick better value with\n#> `binwidth`.\npulse %>%\n    ggplot(aes(x = secondPulse, fill = intervention)) +\n    geom_histogram(alpha = 0.5, position = 'identity') +\n    scale_fill_brewer(type = 'seq', palette = 'Set1') +\n    labs(x = 'Second measurement', \n         fill = 'Intervention') +\n    theme_cowplot() +\n    theme(legend.position = 'bottom')\n#> `stat_bin()` using `bins = 30`. Pick better value with\n#> `binwidth`."},{"path":"lecture-3-plots-and-table1.html","id":"geom_boxplot-as-an-alternative-to-geom_histogram","chapter":"4 Lecture 3: Plots and table1","heading":"4.2.3 geom_boxplot() as an alternative to geom_histogram()","text":"Another important plot compares distribution two variables boxplot. boxplot shows us range data, outliers, median.basically contains information histogram. Can see ?","code":"\npulse %>%\n    ggplot(aes(x = intervention, y = secondPulse)) +\n    geom_boxplot()"},{"path":"lecture-3-plots-and-table1.html","id":"geom_density-as-an-alternative-to-geom_histogram-and-geom_boxplot","chapter":"4 Lecture 3: Plots and table1","heading":"4.2.4 geom_density() as an alternative to geom_histogram() and geom_boxplot()","text":"another (essentially) version previous two plots.","code":"\npulse %>%\n    ggplot(aes(x = secondPulse, fill = intervention)) +\n    geom_density(alpha = 0.5) +\n    scale_fill_brewer(type = 'seq', palette = 'Set1') +\n    theme_cowplot() +\n    labs(x = 'Second measurement', \n         fill = 'Intervention') + \n    theme(legend.position = 'bottom')"},{"path":"lecture-3-plots-and-table1.html","id":"barplots-for-discrete-variables-geom_bar","chapter":"4 Lecture 3: Plots and table1","heading":"4.2.5 Barplots for discrete variables geom_bar()","text":"far paying attention continuous variable. Let’s turn attention discrete ones. discrete variables sex, smoker, drinks intervention.Let’s plot barplot sex:\nhonestly pretty boring plot. can spice things little adding intervention variable.bars stacked top annoying. better side--side.course can improve aesthetics graph like ones.","code":"\npulse %>%\n    ggplot(aes(x = sex)) +\n    geom_bar()\npulse %>%\n    ggplot(aes(x = sex, fill = intervention)) +\n    geom_bar()\npulse %>%\n    ggplot(aes(x = sex, fill = intervention)) +\n    geom_bar(position = 'dodge')"},{"path":"lecture-3-plots-and-table1.html","id":"splitting-plots-into-facets-with-facet_grid","chapter":"4 Lecture 3: Plots and table1","heading":"4.2.6 Splitting plots into facets with facet_grid()","text":"Recall scatter plot :Instead points static activity together can split plot two. code needs modified little ’s much work.group gets plot. Sometimes ’s easier compare plots way.","code":"\npulse %>%\n    ggplot(aes(x = firstPulse, y = secondPulse, color = intervention)) +\n    geom_point() +\n    scale_color_brewer(type = 'seq', palette = 'Set1') + # Change color\n    labs(x = 'First measurement', \n         y = 'Second measurement', \n         color = 'Intervention') + # Change labels on axes, change legend\n    theme_cowplot() +    # change the theme\n    theme(legend.position = 'bottom')\npulse %>%\n    ggplot(aes(x = firstPulse, y = secondPulse)) +\n    geom_point() +\n    labs(x = 'First measurement', \n         y = 'Second measurement') + # Change labels on axes, change legend +\n    facet_grid(~intervention, scales = 'free') +\n    theme_cowplot() +    # change the theme\n    theme(legend.position = 'bottom')"},{"path":"lecture-3-plots-and-table1.html","id":"saving-our-plots","chapter":"4 Lecture 3: Plots and table1","heading":"4.3 Saving our plots","text":"use ggsave() function save plots locally. save plot need store object memory. Let’s save scatter plot.","code":"\np <- \n    pulse %>%\n    ggplot(aes(x = firstPulse, y = secondPulse, color = intervention)) +\n    geom_point() +\n    scale_color_brewer(type = 'seq', palette = 'Set1') + # Change color\n    labs(x = 'First measurement', \n         y = 'Second measurement', \n         color = 'Intervention') + # Change labels on axes, change legend\n    theme_cowplot() +    # change the theme\n    theme(legend.position = 'bottom')\nggsave(filename = here('img', 'scatterPlot.png'), plot = p, width = 8, height = 6, dpi = 320)"},{"path":"lecture-3-plots-and-table1.html","id":"appendix-1","chapter":"4 Lecture 3: Plots and table1","heading":"4.4 Appendix","text":"","code":"\nlibrary(tidyverse)\nlibrary(here)\n\npuls <- read_csv2(\"https://edbook.hi.is/gogn/pulsAll.csv\")\n\npuls2 <-\n    puls %>%\n    rename(course = namskeid, \n           coin = kronukast, \n           height = haed, \n           weight = thyngd, \n           age = aldur,\n           sex = kyn, \n           smokes = reykir,\n           drinks = drekkur, \n           gym = likamsraekt, \n           firstPulse = fyrriPuls,\n           secondPulse = seinniPuls, \n           intervention = inngrip,\n           date = dagsetning) %>%\n    select(height, weight, age, sex, smokes, \n           drinks, firstPulse, secondPulse, intervention) %>%\n    mutate(smokes = case_when(smokes == 'ja' ~ 'yes', \n                              smokes == 'nei' ~ 'no'), \n           drinks = case_when(drinks == 'ja' ~ 'yes',\n                              drinks == 'nei' ~ 'no'), \n           intervention = case_when(intervention == 'hljop' ~ 'active', \n                                    intervention == 'sat_kyrr' ~ 'stationary'), \n           id = 1:nrow(puls)) %>%\n    relocate(id, .before = 'height')\n\nwrite.table(x = puls2, file = here('data', 'pulseEn.csv'), sep = ';', row.names = F)"},{"path":"lecture-4-probability-theory.html","id":"lecture-4-probability-theory","chapter":"5 Lecture 4: Probability theory","heading":"5 Lecture 4: Probability theory","text":"Statistics based probability theory. must therefore learn little probability theory can learn statistics.","code":""},{"path":"lecture-4-probability-theory.html","id":"the-sample-space","chapter":"5 Lecture 4: Probability theory","heading":"5.1 The sample space","text":"Imagine simple experiment toss coin . coin can come heads tails. therefore say experiment two possible outcomes, viz. heads tails. collection (set) possible outcomes experiment called sample space. coin experiment, sample space collection (set) two elements, heads tails. can denote sample space coin toss mathematically :\\[\nS = \\{\\mbox{Heads}, \\mbox{Tails}\\}.\n\\]Another experiment casting die . die can come 1, 2, 3, 4, 5, 6. sample space \\(S\\) can thus written:\\[\nS = \\{1, 2, 3, 4, 5, 6\\}.\n\\], \\(S\\) six elements. experiments, sample space finite. finite amount possible outcomes.","code":""},{"path":"lecture-4-probability-theory.html","id":"discrete-and-continuous","chapter":"5 Lecture 4: Probability theory","heading":"5.1.1 Discrete and continuous","text":"experiments sample space discrete. discrete can “enumerate” options - give indexes. coin experiment, enumeration follows:\\[\n\\begin{array}{c}\n\\mbox{Index} & \\mbox{Element} \\\\\n1 & H \\\\\n2 & T\n\\end{array}\n\\]die example even simpler.\\[\n\\begin{array}{c}\n\\mbox{Index} & \\mbox{Element} \\\\\n1 & 1 \\\\\n2 & 2 \\\\\n3 & 3 \\\\\n4 & 4 \\\\\n5 & 5 \\\\\n6 & 6\n\\end{array}\n\\]’s say discreteness implies finiteness. imagine die “infinite” faces can still enumerate options.Now, imagine another experiment want measure temperature glass water boiled. measure temperature given time temperature can anywhere 20\\(^\\circ\\)C 100\\(^\\circ\\)C. say, \\(S = [20, 100]\\). case actually can’t enumerate possible values may seem counterintuitive. case, say sample space continuous; exists continuum can smoothly go values.feels fuzzy unclear . continuity uncountability real numbers tough grasp required decades mathematical work made rigorous.","code":""},{"path":"lecture-4-probability-theory.html","id":"random-variables","chapter":"5 Lecture 4: Probability theory","heading":"5.2 Random variables","text":"need way talk outcome experiment perform . Let’s us denote outcome experiment perform \\(X\\). refer \\(X\\) random variable. make things conecrete let’s return coin-tossing experiment. can write:\\[\nX = \\mbox{outcome coin toss}.\n\\]Thinking back sample space \\(S = \\{H, T\\}\\) (H Heads, T Tails) see random variable \\(X\\) can take two possible values: \\(X = H\\) \\(X = T\\). therefore say random variable \\(X\\) discrete. \\(X\\) take value existing continuum, call \\(X\\) continuous random variable (think temperature experiment).","code":""},{"path":"lecture-4-probability-theory.html","id":"probability-of-an-outcome","chapter":"5 Lecture 4: Probability theory","heading":"5.3 Probability of an outcome","text":"defined random variables values can take finally position talk probability random variable \\(X\\) taking value \\(x\\). Note difference upper-case \\(X\\) lower-case \\(x\\). distinction important! \\(X\\) random variable \\(x\\) observe; \\(x\\) random variable. die-casting experiment \\(X\\) outcome toss die. toss die, let’s assume comes 6, say \\(X = 6\\).Now, likely see 6 outcome experiment? Assuming die unbiased, expect 1--6 chance observe 6. understand recall sample space experiment. \\(S = \\{1, 2, 3, 4, 5, 6\\}\\). six elements assume one likely appear . can written mathematicall :\\[\nP(X = x) = \\frac16.\n\\]equation can parsed English follows: probability (\\(P\\)) random varible \\(X\\) takes value \\(x\\) (\\(P(X = x)\\)) one six (\\(P(X = x) = 1/6\\)). Note doesn’t matter \\(x\\) ; probability always \\(1/6\\).“equal spreading” probability naive definition probability. give potential outcome equal weight. \\(|S|\\) number elements sample space (6 die) equal probability :\\[\nP(X = x) = \\frac{1}{|S|}\n\\]Note becomes problem soon allow infinite sample spaces. Thinking temperature example, number elements closed interval \\([20, 100]\\) infinite, probability picking specific value \\(x\\) random 0. level therefore stick two “rules”:\\(X\\) discrete, allow \\(P(X = x)\\).\\(X\\) discrete, allow \\(P(X = x)\\).\\(X\\) continuous need consider interval around \\(x\\). , \\(P(< X < b)\\) reads “probability \\(X\\) \\(\\) \\(b\\)”.\\(X\\) continuous need consider interval around \\(x\\). , \\(P(< X < b)\\) reads “probability \\(X\\) \\(\\) \\(b\\)”.","code":""},{"path":"lecture-4-probability-theory.html","id":"the-probability-massdensity-function","chapter":"5 Lecture 4: Probability theory","heading":"5.3.1 The probability mass/density function","text":"probability mass function \\(f(x)\\) gives us probability discrete random variable\\(X\\) takes value \\(x\\). already seen explicit form \\(f(x)\\):\\[\nf(x) = P(X = x).\n\\]probability mass function two properties: \\(f(x) \\geq 0\\) \\(\\sum _{x \\S} f(x) = 1\\). plot probability mass function die:bars height outcome just likely. involved example die biased. write:\\[\n\\begin{aligned}\n&P(X = 1) = 1/10, \\quad P(X = 2) = 2/10, \\quad P(X = 3) = 3/10 \\\\ \n&P(X = 4) = 2/10, \\quad P(X = 5) = 2/10, \\quad P(X = 6) = 0/10\n\\end{aligned}\n\\]can see, probabilities greater equal 0 add 1. Graphically, probability mass function :bars longer equal height probabilities longer equal.\\(X\\) continuous random variable speak probability density function.","code":""},{"path":"lecture-4-probability-theory.html","id":"the-cumulative-distribution-function","chapter":"5 Lecture 4: Probability theory","heading":"5.3.2 The cumulative distribution function","text":"cumulative distribution function random variable \\(X\\) :\\[\nF(x) = P(X \\leq x).\n\\]can parsed : probability random variable \\(X\\) taking value less equal \\(x\\).exists connection CDF density/mass functions. mass function gives us probability specific outcome CDF gives us cumulative probability specific event. Compare graph CDF probability mass function graph example .many continuous random variables exists explicit form CDF must thus rely density function.","code":""},{"path":"lecture-4-probability-theory.html","id":"properties-of-random-variables","chapter":"5 Lecture 4: Probability theory","heading":"5.4 Properties of random variables","text":"","code":""},{"path":"lecture-4-probability-theory.html","id":"independence","chapter":"5 Lecture 4: Probability theory","heading":"5.4.1 Independence","text":"say two random variables \\(X\\) \\(Y\\) independent outcome one random variable effect outcome random variable. example tossing two coins. outcome one toss influence outcome toss.two random variables independent say dependent.","code":""},{"path":"lecture-4-probability-theory.html","id":"identically-distributed","chapter":"5 Lecture 4: Probability theory","heading":"5.4.2 Identically distributed","text":"Let \\(X_1, X_2, \\ldots , X_n\\) collection random variables. say identically distributed probability distribution.","code":""},{"path":"lecture-4-probability-theory.html","id":"expected-value","chapter":"5 Lecture 4: Probability theory","heading":"5.4.3 Expected value","text":"expected value random variable \\(X\\) denoted \\(E(X)\\). can think average \\(X\\). calculate expected value \\(X\\) summing product potential outcomes \\(X\\) probability outcome. example make clearer. Let’s return die \\(X\\) outcome toss. Since outcome equally likely expected value \\(X\\) simply:\\[\n\\begin{aligned}\nE(X) &= \\sum _{x \\S} x P(X = x) \\\\\n&= 1 \\cdot P(X = 1) + 2 \\cdot P(X = 2) + \\ldots + 6 \\cdot P(X = 6) \\\\\n&= (1 + 2 + 3 + 4 + 5 + 6) \\cdot \\frac16 \\\\\n&=3.5\n\\end{aligned}\n\\]expected value linear operator. Let \\(X\\) \\(Y\\) random variables assume \\(E(X) = \\mu _X\\) \\(E(Y) = \\mu _Y\\). expected value \\(X\\) \\(Y\\) :\n\\[\nE(X + Y) = E(X) + E(Y) = \\mu _X + \\mu _Y\n\\]Furthermore, \\(\\) constant \\[\nE(aX) = aE(X) = \\mu_X.\n\\]important result expected value random variable \\(X\\). called law large numbers states sample \\(X\\) grows, closer mean sample expected value \\(X\\). Let’s toss die bunch times take average throws. plot shows average sample function sample size. red line expected value \\(X\\) calculated 3.5. can see, often throw die, closer calculated sample mean theoretical expected value.","code":""},{"path":"lecture-4-probability-theory.html","id":"variance","chapter":"5 Lecture 4: Probability theory","heading":"5.4.4 Variance","text":"variance random variable \\(X\\) tells us “dispersed” random variable expected value denoted \\(Var(X)\\). higher variance, bigger dispersion. plot two samples mean different variance.\nsquare root variance called standard deviation often denoted \\(\\sigma\\).","code":""},{"path":"lecture-4-probability-theory.html","id":"probability-distributions","chapter":"5 Lecture 4: Probability theory","heading":"5.5 Probability distributions","text":"probability distribution random variable \\(X\\) completely describes probabilities outcomes \\(X\\)","code":""},{"path":"lecture-4-probability-theory.html","id":"examples-of-discrete-distributions","chapter":"5 Lecture 4: Probability theory","heading":"5.6 Examples of discrete distributions","text":"","code":""},{"path":"lecture-4-probability-theory.html","id":"bernoulli-distribution","chapter":"5 Lecture 4: Probability theory","heading":"5.6.1 Bernoulli distribution","text":"Imagine coin-toss experiment. coin comes heads consider experiment success. can encode outcome experiment takes value 1 success (heads) 0 otherwise. random variable \\(X\\) thus 1 get heads 0 otherwise. thing missing probability success (symmetry probability failure) denote \\(p\\). coin unbiased, \\(p\\) 0.5 50/50 chance success failure.can describe experiment way say \\(X\\) Bernoulli random variable success parameter \\(p\\). experiments can reduced Bernoulli trial. Let’s take die . consider experiment success roll 6. 1/6 chance rolling 6, success parameter \\(p\\) 1/6. experiment failure get outcome; get 1, 2, 3, 4, 5. probability failure thus 5/6. general rule thumb, probability failure \\(1- p\\).expected value \\(X\\) \\(E(X) = p\\) variance \\(Var(X) = p(1-p)\\). probability mass function \\(X\\) :\\[\nf(x) = P(X = x) = p^x (1 - p)^{1 - x}\n\\]","code":""},{"path":"lecture-4-probability-theory.html","id":"binomial-distribution","chapter":"5 Lecture 4: Probability theory","heading":"5.6.2 Binomial distribution","text":"Let’s say toss unbiased coin 3 times. performing 3 Bernoulli trials, success parameter \\(p\\). consider outcome heads success. many possible permutations available:\\[\nHHH, HHT, HTH, THH, HTT, THT, TTH, TTT.\n\\]particular experiment aren’t interested get heads many heads get. Notice following:\\[\n\\begin{array}{c}\n\\mbox{Number heads} & \\mbox{Outcomes} \\\\\n0 & TTT \\\\\n1 & HTT, THT, TTH\\\\\n2 & HHT, HTH, THH\\\\\n3 & HHH\n\\end{array}\n\\]\nsee outcomes 0 3 heads equally likely (\\(P(X = 0) = P(X = 3) = 1/8\\)), \\(P(X = 1) = P(X = 2) = 3/8\\). can define random variable \\(X\\) number heads say binomially distributed parameters \\(n\\) \\(p\\). \\(n\\) number Bernoulli trials (number coin flips, die throws) \\(p\\) success probability. expected value \\(X\\) \\(E(X) = np\\) variance \\(X\\) \\(Var(X) = np(1-p)\\). probability mass function :\\[\nf(x) = P(X = x) = \\binom{n}{k} p^k (1-p)^{n - k},\n\\]\\(k\\) number successes. Note similarity Bernoulli distribution.Let’s simulate scenario . toss coin three times 10000 iterations iteration keep track number heads. can see result plot .can compute probabilities \\(P(X = x)\\) \\(P(X <= x)\\) dbinom() pbinom() functions.","code":"\n# P(X = 2)\ndbinom(x = 2, size = 3, prob = 0.5)\n#> [1] 0.375\n# P(X <= 2)\npbinom(q = 2, size = 3, prob = 0.5)\n#> [1] 0.875"},{"path":"lecture-4-probability-theory.html","id":"poisson-distribution","chapter":"5 Lecture 4: Probability theory","heading":"5.6.3 Poisson distribution","text":"third discrete distribution Poisson distribution. use binomial distribution model counts within interval. Unlike binomial distribution fixed upper bound \\(n\\) Poisson distribution ceiling. Poisson distribution single parameter \\(\\lambda\\) describes expected number outcomes within interval. defining characteristic Poisson distribution expected value equal variance. : \\(E(X) = \\lambda = Var(X)\\).plot 10000 draws Poisson distribution rate parameter \\(\\lambda = 5\\).can compute probabilities \\(P(X = x)\\) \\(P(X <= x)\\) dpois() ppois() functions.","code":"\n# P(X = 5)\ndpois(x = 5, lambda = 5)\n#> [1] 0.1754674\n# P(X <= 5)\nppois(q = 5, lambda = 5)\n#> [1] 0.6159607"},{"path":"lecture-4-probability-theory.html","id":"examples-of-continuous-distributions","chapter":"5 Lecture 4: Probability theory","heading":"5.7 Examples of continuous distributions","text":"","code":""},{"path":"lecture-4-probability-theory.html","id":"the-normal-distribution","chapter":"5 Lecture 4: Probability theory","heading":"5.7.1 The normal distribution","text":"normal distribution important probability distribution see course. Many continuous variables can described normal distribution. normal distribution completely described expected value variance (standard deviation). means know expected value variance can completely reconstruct distribution. Another property normal distribution symmetry. normal distribution expected value \\(\\mu\\) standard deviation \\(\\sigma\\), roughly 68%, 95%, 99.7% measurements within one, two, three standard deviations respectively.Let’s look density normally distributed random variable \\(Z\\) expected value 0 standard deviation 1.normal distribution doesn’t expected value 0 standard deviation 1 refer standard normal distribution. normal distribution can transformed standard normal distribution centering scaling random variable. example: imagine \\(X\\) normally distributed random variable parameters \\(\\mu = 10\\) \\(\\sigma = 5\\). density looks like:center \\(X\\) subtracting expected value, scale \\(X\\) dividing standard deviation:\\[\nZ = \\frac{X - \\mu}{\\sigma} = \\frac{X - 10}{5},\n\\]see \\(Z\\) normally distributed random variable expected value 0 standard deviation 1.Compared probability mass function spent little time discussing probability density function. probability density function little trickier use relies calculus. Say \\(Z\\), random variable standard normal distribution, want know probability \\(Z\\) -1, 1, \\(P(-1 \\leq Z \\leq 1)\\). working probability mass function enough us add probabilities respected values. since \\(Z\\) continuous, probability \\(Z\\) taking specific value 0. must thus look intervals. compute probability random variable within region requires integration, something won’t cover course.\\[\nP(-1 \\leq Z \\leq 1) \\approx 68\\%.\n\\]can calculate probabilities \\(P(Z > 2)\\) \\(P(Z < -0.5)\\). \\(P(Z > 2)\\) can use fact probabilities must add 1 can rewrite \\(P(Z > 2) = 1 - P(Z <= 2)\\).can use R calculate area red curves. must use pnorm() function. Let’s compute probabilities three cases:","code":"\npnorm(q = -0.5, mean = 0, sd = 1)\n#> [1] 0.3085375\n1 - pnorm(q = 2, mean = 0, sd = 1)\n#> [1] 0.02275013\npnorm(q = 1, mean = 0, sd = 1) - pnorm(q = -1, mean = 0, sd = 1)\n#> [1] 0.6826895"},{"path":"lecture-4-probability-theory.html","id":"the-t-distribution","chapter":"5 Lecture 4: Probability theory","heading":"5.7.2 The \\(t\\)-distribution","text":"Closely related normal distribution \\(t\\)-distribution. reminiscent normal distribution “heavier tails”. number degrees freedom determine shape distribution. discuss \\(t\\)-distribution better later.can compute probabilities \\(P(X < x)\\), \\(X\\) random variable \\(t\\)-distribution, pt() function. pt() function requires two inputs, \\(x\\) number degrees freedom:Compare value computed pnorm(q = 1, mean = 0, sd = 1). greater \\(t\\)-distribution consequence heavier tails.","code":"\n1 - pt(q = 2, df = 5)\n#> [1] 0.05096974"},{"path":"lecture-4-probability-theory.html","id":"the-chi-2-distribution","chapter":"5 Lecture 4: Probability theory","heading":"5.7.3 The \\(\\chi ^2\\) distribution","text":"\\(\\chi ^2\\) distribution based normal distribution. symmetrical like \\(t\\)-distribution single parameter, namely number degrees freedom.can compute probabilities \\(P(X < x)\\), \\(X\\) random variable \\(\\chi^2\\)-distribution, pchisq() function. pchisq() function requires two inputs, \\(x\\) number degrees freedom:","code":"\n1 - pchisq(q = 2, df = 1)\n#> [1] 0.1572992"},{"path":"lecture-4-probability-theory.html","id":"the-central-limit-theorem","chapter":"5 Lecture 4: Probability theory","heading":"5.8 The central limit theorem","text":"important result probability theory Central limit theorem. says mean sample resemble normal distribution closely sample grows. Furthermore, variance decreases sample size increases.Remember Poisson distribution ?Let’s draw random samples Poisson distribution . samples size 5, 25, 100, 500. sample compute mean store . repeating process 5000 times, plot results:","code":""},{"path":"lecture-5-inference.html","id":"lecture-5-inference","chapter":"6 Lecture 5: Inference","heading":"6 Lecture 5: Inference","text":"far descriptive statistics. computed stuff like means, standard deviations . purpose descriptive statistics allow us get feel data explore whether research question can answered data. done descriptives, must confirm statistical tests. Eyeballing enough. goal lecture give tools necessary answer simple research questions statistics.","code":""},{"path":"lecture-5-inference.html","id":"the-population-and-the-sample","chapter":"6 Lecture 5: Inference","heading":"6.1 The population and the sample","text":"’ve made cryptic references “populations” “samples”. population group interested investigating entirety. study average height Europeans population entire population Europe. Obviously impossible measure every single individual Europe instead make sample; randomly selected subgroup population. examine sample make inferences population.Note population unique can many samples population.","code":""},{"path":"lecture-5-inference.html","id":"sample-statistics","chapter":"6 Lecture 5: Inference","heading":"6.2 Sample statistics","text":"interested statistics. statistic function sample. example, assume measure height ten individuals compute mean measurements. , mean statistic function data (measurements). Since mean function sample, sample random, mean inherits property stochastic. careful! taken measurements deterministic stochastic.Let’s create example illustrate property. create “fake” population repeatedly sample compute mean sample.can see, samples gave different mean.","code":"\n# The population\npopulation <- c(181, 191, 171, 151, 161, 171, 172, \n                175, 165, 183, 201, 151, 159, 153, 165)\n# Let's draw 5 samples from the population, each of size 4 \n# and compute the mean. \nset.seed(7783)\nmeans <- colMeans(replicate(5, sample(population, size = 4)))\n# Make output more readable\nnames(means) <- c('mean1', 'mean2', 'mean3', 'mean4', 'mean5')\nmeans\n#>  mean1  mean2  mean3  mean4  mean5 \n#> 167.25 178.00 159.50 174.50 183.75"},{"path":"lecture-5-inference.html","id":"properties-of-the-mean-of-the-sample","chapter":"6 Lecture 5: Inference","heading":"6.3 Properties of the mean of the sample","text":"Let us denote mean sample observe sample \\(\\bar{X}\\) mean observed sample \\(\\bar{x}\\). \\(\\bar{X}\\) random variable \\(\\bar{x}\\) outcome. Since \\(\\bar{X}\\) random variable distributional properties: expected value variance.\\(X_1, X_2, \\ldots , X_n\\) independent identically distributed variables, expected value \\(E(X_i) = \\mu\\) variance \\(Var(X_i) = \\sigma ^2\\) :\\[\nE(\\bar{X}) = \\mu, \\quad Var(\\bar{X}) = \\frac{\\sigma ^2}{n}.\n\\]may heard standard error . standard error simply standard deviation random variable \\(\\bar{X}\\); namely, \\(SE(\\bar{X}) = \\sigma/\\sqrt{n}\\). gets name commonly used.Finally, assume sample \\(X_1, X_,2, \\ldots , X_n\\) comes normal distribution parameters \\(\\mu\\) \\(\\sigma\\) \\(\\bar{X}\\) also normally distributed parameters \\(\\mu\\) \\(\\sigma / \\sqrt{n}\\).Let’s create example like . draw sample size 25 standard normal distribution 5000 times. compute mean sample store . simulation run course, compute mean standard deviation 5000 means finally plot . expect sample mean 0, standard deviation \\(1/\\sqrt{25}\\). Finally, expect histogram means resemble normal distribution.","code":"\nnMeans <- colMeans(replicate(5000, rnorm(n = 25, mean = 0, sd = 1))) \ntibble(parameter = c('mean', 'standard deviation'),\n       theoretical = c(0, 1/sqrt(25)), \n       simulation = c(mean(nMeans), sd(nMeans))) %>%\n    kbl() %>%\n    kable_styling(full_width = F)\ntibble(means = nMeans) %>%\n    ggplot(aes(x = means)) +\n    geom_histogram() +\n    theme_cowplot()\n#> `stat_bin()` using `bins = 30`. Pick better value with\n#> `binwidth`."},{"path":"lecture-5-inference.html","id":"the-estimator","chapter":"6 Lecture 5: Inference","heading":"6.4 The estimator","text":"estimator statistic estimates parameters model. already seen parameters success probability \\(p\\) binomial distribution \\(\\mu\\) \\(\\sigma\\) normal distribution.","code":""},{"path":"lecture-5-inference.html","id":"proportions-and-means","chapter":"6 Lecture 5: Inference","heading":"6.4.1 Proportions and means","text":"estimator works \\(\\mu\\) \\(p\\):\\[\n\\hat{\\mu} = \\frac{\\sum _ {= 1} ^n X_ }{n} = \\hat{p}\n\\]example binomial distribution. Imagine population 50 people. person can diabetic . interest determine proportion people diabetic. Unfortunately secure enough grant money test 20 people.can see, estimate \\(p\\) 0.25 pretty close population proportion.","code":"\nset.seed(847)\ndiabetesPref <- rbinom(n = 50, size = 1, prob = 0.3)\n# This is the true population\ndiabetesPref\n#>  [1] 1 1 1 0 0 0 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 1 1 1 1 0 0\n#> [29] 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0\n# Sample 20 people\nsampleDia <- sample(diabetesPref, size = 20)\nmean(sampleDia)\n#> [1] 0.25"},{"path":"lecture-5-inference.html","id":"variance-1","chapter":"6 Lecture 5: Inference","heading":"6.4.2 Variance","text":"denote estimator variance \\(S^2\\). defined :\\[\nS^2 = \\frac{\\sum _ {= 1} ^n (X_i - \\bar{X})^2}{n - 1}.\n\\]seen data denote estimate \\(s^2\\). wanted estimate standard deviation simply take square root \\(s^2\\).","code":""},{"path":"lecture-5-inference.html","id":"confidence-intervals","chapter":"6 Lecture 5: Inference","heading":"6.5 Confidence intervals","text":"far considered point estimates. saw mean example, point estimates dependent sample. probability point estimator exactly equal parameter trying estimate 0 (remember discussion continuous random variables). also saw increased sample size, allowed repeat sampling, approached true parameter value. Now, impossible real life. sample size often restricted due external factors (often many) can’t easily repeat experiments.Luckily, solution problem: confidence intervals. construct confidence interval around point estimates get sense estimates likely. wider confidence intervals, likely catch true parameter. probably seen terms like 95% confidence interval 99.5% confidence interval. percentages reflect proportion times true parameter value contained interval repeat experiment infinitely often. sounds weird, ’s . Please note parameter never changes, estimates.generally speak \\(1 - \\alpha\\) confidence interval \\(\\alpha\\) accepted type error probability. example, interested 95% confidence interval, accept type error probability 5% \\(alpha = 0.05\\).type error probability? instance proportion confidence intervals construct around point estimates don’t contain true parameter value.create confidence intervals need compute test statistics. later.","code":""},{"path":"lecture-5-inference.html","id":"hypothesis-tests","chapter":"6 Lecture 5: Inference","heading":"6.6 Hypothesis tests","text":"alternative confidence intervals hypothesis tests. hypothesis test two components: null hypothesis \\(H_0\\) alternative hypothesis \\(H_1\\). null hypothesis often bland, boring scenario want reject. example, two groups, one placebo actual drug, null hypothesis difference groups. obviously boring result perspective pharmaceutical company wants demonstrate efficacy drug. alternative hypothesis scenario interested .e. difference groups.go ? defined hypothesis test assume null hypothesis true. use data construct test statistic test null hypothesis. Often test statistic well known probability distribution; test statistic “unlikely enough” given null hypothesis, use evidence reject null hypothesis.Important: never accept null hypothesis.","code":""},{"path":"lecture-5-inference.html","id":"the-orientation-of-the-hypothesis-test","chapter":"6 Lecture 5: Inference","heading":"6.6.1 The orientation of the hypothesis test","text":"hypothesis test can one-sided two-sided. example one-sided test :\\[\n\\begin{aligned}\nH_0&: \\mbox{mean cholesterol group equal mean group B} \\\\\nH_1&: \\mbox{mean cholesterol group smaller mean group B}\n\\end{aligned}\n\\]example two-sided test :\\[\n\\begin{aligned}\nH_0&: \\mbox{mean cholesterol group equal mean group B} \\\\\nH_1&: \\mbox{mean cholesterol group equal mean group B}\n\\end{aligned}\n\\]","code":""},{"path":"lecture-5-inference.html","id":"the-test-statistic-of-the-hypothesis-test","chapter":"6 Lecture 5: Inference","heading":"6.6.2 The test statistic of the hypothesis test","text":"test statistic statistic can use reject null hypothesis. example test statistic :\\[\nt = \\frac{\\bar{X} - \\mu}{S/\\sqrt{n}} \n\\]use perform \\(t\\)-tests. Note test statistic based sample thus random variable distribution. compare computed test statistic theoretical distribution (null).’s important know test statistic compute. looking means assume normal distribution, appropriate test \\(t\\)-test. need \\(t\\)-test don’t know standard deviation population distribution.","code":""},{"path":"lecture-5-inference.html","id":"the-rejection-region","chapter":"6 Lecture 5: Inference","heading":"6.6.3 The rejection region","text":"said , reject null test statistic unreasonable given null hypothesis. rejection region area distribution test statistic contains values lead rejection null hypothesis. rejection region always tails distribution.","code":""},{"path":"lecture-5-inference.html","id":"the-p-value","chapter":"6 Lecture 5: Inference","heading":"6.6.4 The \\(p\\)-value","text":"Assume \\(t\\) test statistic. \\(p\\)-value hypothesis test probability seeing larger test statistic \\(t\\) given null hypothesis true. \\(p\\)-value smaller predefined significance threshold \\(\\alpha\\) use evidence support decision reject null hypothesis.","code":""},{"path":"lecture-5-inference.html","id":"power-and-errors","chapter":"6 Lecture 5: Inference","heading":"6.6.5 Power and errors","text":"power test probability rejecting null hypothesis rejected. can increase statistical power increasing sample size.commit type error reject null hypothesis shouldn’t (false positive). Similarly, commit type II error reject null hypothesis (false negative).","code":""},{"path":"lecture-5-inference.html","id":"examples","chapter":"6 Lecture 5: Inference","heading":"6.7 Examples","text":"use pulseEn.csv data.","code":"\npulse <- \n    read_csv2('https://notendur.hi.is/thj73/data/pulseEn.csv') %>%\n    na.omit() %>% # Remove NA\n    filter(height < 300) # Remove two weird height values"},{"path":"lecture-5-inference.html","id":"the-simple-t-test","chapter":"6 Lecture 5: Inference","heading":"6.7.1 The simple \\(t\\)-test","text":"Let’s see average height pulseEn differs 150 cm significantly. use t.test() function. parameter value mu 0 default need change value want test.\\(p\\)-value tiny. strongly reject null hypothesis.test statistic test can computed :\\[\nt = \\frac{\\bar{x} - \\mu _ 0}{s/\\sqrt{n}}\n\\]","code":"\nt.test(pulse$height, mu = 150)\n#> \n#>  One Sample t-test\n#> \n#> data:  pulse$height\n#> t = 48.684, df = 423, p-value < 2.2e-16\n#> alternative hypothesis: true mean is not equal to 150\n#> 95 percent confidence interval:\n#>  172.3950 174.2795\n#> sample estimates:\n#> mean of x \n#>  173.3373"},{"path":"lecture-5-inference.html","id":"a-small-t-test-app","chapter":"6 Lecture 5: Inference","heading":"6.7.2 A small \\(t\\)-test app","text":"Click !","code":""},{"path":"lecture-5-inference.html","id":"the-t-test-for-groups","chapter":"6 Lecture 5: Inference","heading":"6.7.3 The \\(t\\)-test for groups","text":"variable interest height sample. hypothesis test follows:\\[\n\\begin{aligned}\nH_0:& \\mbox{mean height intervention groups } \\\\\nH_1:& \\mbox{mean height intervention groups }\n\\end{aligned}\n\\]begin inspecting variables graphically:can see difference groups little, least height-wise. makes us suspect unable reject null hypothesis mean height groups different. Let’s confirm performing \\(t\\)-test t.test() function:see \\(p\\)-value test 0.0829235 unable reject null hypothesis significance level \\(\\alpha = 0.05\\). Note also get confidence interval.test statistic can computed :\\[\nt = \\frac{(\\bar{x} - \\bar{y} - \\delta _ 0)}{\\sqrt{s_1^2/n + s_2^2/n}}\n\\]","code":"\npulse %>%\n    ggplot(aes(x = intervention, y = height)) +\n    geom_boxplot() +\n    theme_cowplot()\nt.test(height ~ intervention, data = pulse)\n#> \n#>  Welch Two Sample t-test\n#> \n#> data:  height by intervention\n#> t = 1.7387, df = 365.06, p-value = 0.08292\n#> alternative hypothesis: true difference in means between group active and group stationary is not equal to 0\n#> 95 percent confidence interval:\n#>  -0.221251  3.599599\n#> sample estimates:\n#>     mean in group active mean in group stationary \n#>                 174.3571                 172.6680"},{"path":"lecture-5-inference.html","id":"the-paired-t-test","chapter":"6 Lecture 5: Inference","heading":"6.7.4 The paired \\(t\\)-test","text":"Let us see difference first second pulse measurement control group. control group nothing measurements, expect significant difference measurements. always, begin plotting two measurements:\ntwo distributions seem identical. Let’s perform paired \\(t\\)-testThis time \\(p\\)-value quite large completely fail reject null hypothesis.test statistic can computed :\\[\nt = \\frac{\\bar{w} - \\Delta _ 0}{s_w /\\sqrt{n}}\n\\]\\[\n\\begin{aligned}\n\\bar{w} &= \\frac1n \\sum _ {= 1} ^n w_i, \\\\\nw_i &= x_i - y_i, \\\\ \ns_w &= \\sqrt{\\frac{1}{n - 1} \\sum _ {= 1} ^n (w_i - \\bar{w})}\n\\end{aligned}\n\\]","code":"\npulse %>%\n    filter(intervention == 'stationary') %>%\n    gather(measurement, value, firstPulse, secondPulse) %>% # convert to long\n    ggplot(aes(x = measurement, y = value)) +\n        geom_boxplot() +\n        theme_cowplot()\nstationary <- \n    pulse %>%\n    filter(intervention == 'stationary')\nt.test(stationary$firstPulse, stationary$secondPulse)\n#> \n#>  Welch Two Sample t-test\n#> \n#> data:  stationary$firstPulse and stationary$secondPulse\n#> t = -0.076792, df = 506.37, p-value = 0.9388\n#> alternative hypothesis: true difference in means is not equal to 0\n#> 95 percent confidence interval:\n#>  -2.076885  1.920635\n#> sample estimates:\n#> mean of x mean of y \n#>  71.04297  71.12109"},{"path":"lecture-5-inference.html","id":"a-test-for-proportions","chapter":"6 Lecture 5: Inference","heading":"6.7.5 A test for proportions","text":"Let’s see difference proportion smokers versus non-smokers. begin constructing table see many fall category.Using table see almost one smokes sample. Let’s perform test:test statistic can computed :\\[\nz = \\frac{\\hat{p} - p_0}{\\sqrt{p_0(1 - p_0)/n}}\n\\]\\(\\hat{p} = x/n\\), \\(x\\) number successes.","code":"\nprop.table(table(pulse$smokes))\n#> \n#>        no       yes \n#> 0.9009434 0.0990566\nbinom.test(table(pulse$smokes))\n#> \n#>  Exact binomial test\n#> \n#> data:  table(pulse$smokes)\n#> number of successes = 382, number of trials = 424,\n#> p-value < 2.2e-16\n#> alternative hypothesis: true probability of success is not equal to 0.5\n#> 95 percent confidence interval:\n#>  0.8684666 0.9276729\n#> sample estimates:\n#> probability of success \n#>              0.9009434"},{"path":"lecture-5-inference.html","id":"in-conclusion","chapter":"6 Lecture 5: Inference","heading":"6.8 In conclusion","text":"tests can perform tests independence contingency table tests variances talk discuss ANOVA. Also, today’s lecture heavy hypothesis tests without mention confidence intervals. discuss confidence intervals better talk linear models.","code":""},{"path":"lecture-6-anova.html","id":"lecture-6-anova","chapter":"7 Lecture 6: ANOVA","heading":"7 Lecture 6: ANOVA","text":"previous lecture used \\(t\\)-test compare means two groups. natural extension test ask multiple groups. Rather perform \\(t\\)-test every single group difference use staticial method called analysis variance ANOVA.","code":""},{"path":"lecture-6-anova.html","id":"the-assumptions-of-anova","chapter":"7 Lecture 6: ANOVA","heading":"7.1 The assumptions of ANOVA","text":"assumptions ANOVA follows:samples random samples.samples random samples.underlying populations normally distributed.underlying populations normally distributed.variances populations .variances populations .","code":""},{"path":"lecture-6-anova.html","id":"one-way-anova","chapter":"7 Lecture 6: ANOVA","heading":"7.2 One-way ANOVA","text":"begin short example. Imagine developed new drug reduces biomarker. interested testing two things:drug works (outperforms placebo).drug works (outperforms placebo).Compare efficacy drug another drug.Compare efficacy drug another drug.study randomly assign participants one three groups: P (placebo), O (old drug), N (new drug). draw blood twice, administered treatment measure biomarker. measured biomarkers compute difference two measurements participants. data looks like:Let’s plot data get better sense . red dots represent group means black dots represent data . broken line running across plot overall mean.plot immediately see difference placebo group two drug groups. clear whether difference two drugs. Finally, seems variance approximately groups. point know eyeballing sufficient.","code":"\nset.seed(12)\nbioData <- \n    tibble(P1 = rnorm(10, 7, 1), \n       P2 = rnorm(10, 7, 1),\n       O1 = rnorm(10, 7, 1), \n       O2 = rnorm(10, 5, 1), \n       N1 = rnorm(10, 7, 1), \n       N2 = rnorm(10, 4, 1)) %>%\n    mutate(P = P1 - P2, O = O1 - O2, N = N1 - N2) %>%\n    select(P, O, N)\nbioLong <- \n    bioData %>%\n    gather(treatment, value) %>%\n    mutate(treatment = factor(treatment, levels = c('P', 'O', 'N')))\nbioMeans <-\n    bioLong %>%\n    group_by(treatment) %>%\n    summarize(mean = mean(value))\nbioLong %>%\n    ggplot(aes(x = treatment, y = value)) +\n    geom_point() + \n    geom_point(data = bioMeans %>% rename(value = mean), col = 'red') +\n    geom_hline(yintercept = mean(bioLong$value), lty = 2) +\n    theme_cowplot() +\n    labs(x = 'Treatment groups', y = 'Values')"},{"path":"lecture-6-anova.html","id":"mathematical-notation-for-one-way-anova","chapter":"7 Lecture 6: ANOVA","heading":"7.3 Mathematical notation for one-way ANOVA","text":"\\(y_ {ij}\\) : \\(\\) index group \\(j\\) index measurement within group\\(y_ {ij}\\) : \\(\\) index group \\(j\\) index measurement within group\\(\\) : total number groups\\(\\) : total number groups\\(n_ \\) : total number measurements within group \\(\\)\\(n_ \\) : total number measurements within group \\(\\)\\(N\\) : total number measurements \n\\[\n  N = n_1 + n_2 + \\ldots + n_a\n\\]\\(N\\) : total number measurements \n\\[\n  N = n_1 + n_2 + \\ldots + n_a\n\\]\\(\\bar{y}_ {.}\\): within group mean:\n\\[\n  \\bar{y}_ {.} = \\frac{\\sum _ {j = 1} ^{n_i} y_ {ij}}{n_i}\n\\]\\(\\bar{y}_ {.}\\): within group mean:\\[\n  \\bar{y}_ {.} = \\frac{\\sum _ {j = 1} ^{n_i} y_ {ij}}{n_i}\n\\]\\(\\bar{y}_ {..}\\) : grand mean overall mean. mean entire data set.\n\\[\n  \\bar{y}_ {..} = \\frac{\\sum _{= 1} ^\\sum _{j = 1}^{n_i} y_ {ij}}{N}\n\\]\\(\\bar{y}_ {..}\\) : grand mean overall mean. mean entire data set.\\[\n  \\bar{y}_ {..} = \\frac{\\sum _{= 1} ^\\sum _{j = 1}^{n_i} y_ {ij}}{N}\n\\]\\(SS_T\\): measure total variation data:\n\\[\n  SS_T = \\sum _{= 1} ^\\sum _{j = 1} ^{n _i} (y _{ij} - \\bar{y} _{..})^2\n\\]\\(SS_T\\): measure total variation data:\\[\n  SS_T = \\sum _{= 1} ^\\sum _{j = 1} ^{n _i} (y _{ij} - \\bar{y} _{..})^2\n\\]\\(SS_{Tr}\\): measure variation treatments:\n\\[\n  SS_{Tr} = \\sum _{= 1} ^n_i (\\bar{y} _{.} - \\bar{y} _{..})^2\n\\]\\(SS_{Tr}\\): measure variation treatments:\\[\n  SS_{Tr} = \\sum _{= 1} ^n_i (\\bar{y} _{.} - \\bar{y} _{..})^2\n\\]\\(SS_E\\): measure variation within treatments:\n\\[\n  SS_E = \\sum ^_{= 1} \\sum _{j = 1} ^{n_i} (y_{ij} - \\bar{y}_{.})^2\n\\]\\(SS_E\\): measure variation within treatments:\\[\n  SS_E = \\sum ^_{= 1} \\sum _{j = 1} ^{n_i} (y_{ij} - \\bar{y}_{.})^2\n\\]","code":""},{"path":"lecture-6-anova.html","id":"the-one-way-anova-table","chapter":"7 Lecture 6: ANOVA","heading":"7.4 The one-way ANOVA table","text":"common put square sums variation \\(SS_T, SS_{Tr}, SS_E\\) table:\\[\n\\begin{array}{c}\n\\mbox{Square sums} & \\mbox{Degrees freedom} & \\mbox{Mean square sums} \\\\\nSS_{Tr} & - 1 & MS_{Tr} = SS_{Tr}/(- 1) \\\\\nSS_{E} & N - & MS_{E} = SS_{E}/(N - ) \\\\\nSS_{T} & N - 1 & \n\\end{array}\n\\]ANOVA table used perform hypothesis tests within ANOVA paradigm.","code":""},{"path":"lecture-6-anova.html","id":"hypothesis-tests-with-anova","chapter":"7 Lecture 6: ANOVA","heading":"7.5 Hypothesis tests with ANOVA","text":"null hypothesis one-way ANOVA :\\[\nH_0 = \\mu _ 1 = \\mu _ 2 = \\ldots = \\mu _ \n\\]alternative hypothesis :\\[\nH_1 = \\mbox{ least one mean differs others}\n\\]iThe test statistic :\n\\[\nF = \\frac{SS_{Tr}/(- 1)}{SS_E/(N - )} = \\frac{MS_{Tr}}{MS_E}\n\\]denote \\(F\\)-statistic \\(F\\) null hypothesis come \\(F\\) distribution \\(- 1\\) \\(N - \\) degrees freedom. successful rejecting null means evidence least one mean differing others.","code":""},{"path":"lecture-6-anova.html","id":"anova-in-r","chapter":"7 Lecture 6: ANOVA","heading":"7.6 ANOVA in R","text":"covered theort ’s time use R perform ANOVA analysis us. use biomarker drug . first thing create ANOVA object R aov function:next thing create ANOVA table anova() function.can see get first two rows ANOVA table. \\(p\\)-value small meaning reject null hypothesis \\(\\alpha = 0.05\\) significance level. value \\(F\\)-statistic 15.7226097.successfully rejected null hypothesis tells us exists difference difference lies. need post-hoc analyses Tukey’s multiple comparison test.see comparison tested us. see old new drug differ placebo. Unfortunately new drug compared old drug failed reject null hypothesis. can therefore claim drug improvement.can also graph comparisons:","code":"\nanovaObj <- aov(value ~ treatment, data = bioLong)\nanova(anovaObj)\n#> Analysis of Variance Table\n#> \n#> Response: value\n#>           Df Sum Sq Mean Sq F value    Pr(>F)    \n#> treatment  2 49.208 24.6040  15.723 2.967e-05 ***\n#> Residuals 27 42.252  1.5649                      \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nTukeyHSD(anovaObj)\n#>   Tukey multiple comparisons of means\n#>     95% family-wise confidence level\n#> \n#> Fit: aov(formula = value ~ treatment, data = bioLong)\n#> \n#> $treatment\n#>          diff        lwr      upr     p adj\n#> O-P 2.4941887  1.1070959 3.881282 0.0003727\n#> N-P 2.8949646  1.5078718 4.282057 0.0000552\n#> N-O 0.4007759 -0.9863169 1.787869 0.7560053\nplot(TukeyHSD(anovaObj))"},{"path":"lecture-6-anova.html","id":"a-glimpse-of-what-is-to-come","chapter":"7 Lecture 6: ANOVA","heading":"7.7 A glimpse of what is to come","text":"alternative way perform ANOVA analysis. can simply use linear regression function (subject next lecture) lm().lot information packed summary look bottom see \\(F\\)-statistic, degrees freedom \\(p\\)-value.","code":"\nlinearFit <- lm(value ~ treatment, data = bioLong)\nsummary(linearFit)\n#> \n#> Call:\n#> lm(formula = value ~ treatment, data = bioLong)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -3.2389 -0.6902 -0.1457  0.6638  3.1431 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)  -0.2720     0.3956  -0.688  0.49756    \n#> treatmentO    2.4942     0.5594   4.458  0.00013 ***\n#> treatmentN    2.8950     0.5594   5.175  1.9e-05 ***\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 1.251 on 27 degrees of freedom\n#> Multiple R-squared:  0.538,  Adjusted R-squared:  0.5038 \n#> F-statistic: 15.72 on 2 and 27 DF,  p-value: 2.967e-05"},{"path":"lecture-7-simple-linear-regression.html","id":"lecture-7-simple-linear-regression","chapter":"8 Lecture 7: Simple linear regression","heading":"8 Lecture 7: Simple linear regression","text":"","code":"\nlibrary(tidyverse)\nlibrary(cowplot)\nlibrary(knitr)\nlibrary(kableExtra)"},{"path":"lecture-7-simple-linear-regression.html","id":"the-model","chapter":"8 Lecture 7: Simple linear regression","heading":"8.1 The model","text":"working simple linear regression model dependent variable \\(Y\\), independent variable \\(x\\), error \\(\\varepsilon\\). error \\(\\varepsilon\\) stochastic assume comes normal distribution parameters \\(\\mu = 0\\) \\(\\sigma ^2\\), \\(\\sigma ^2\\) unknown. can also write \\(\\varepsilon \\sim N(0, \\sigma ^2)\\).\nassume following relationship variables:\\[\nY = \\beta _ 0 + \\beta _ 1 x + \\varepsilon \n\\]call model simple linear regression model one independent variable. goal linear regression estimate parameters \\(\\beta _ 0\\), \\(\\beta _ 1\\), \\(\\sigma\\). Sometimes refer \\(\\beta _ 0\\) intercept \\(\\beta_1\\) slope. measured \\(Y\\) denote \\(y\\). Note: \\(Y\\) random variable \\(y\\) observed value \\(Y\\). Since \\(\\varepsilon\\) normally distributed \\(Y\\). fact, \\(Y \\sim (\\beta _ 0 + \\beta _ 1 x, \\sigma ^2)\\).","code":""},{"path":"lecture-7-simple-linear-regression.html","id":"interlude-pulse-data","chapter":"8 Lecture 7: Simple linear regression","heading":"8.2 Interlude: pulse data","text":"Let’s load pulse data filter observations stationary group.Let’s plot first pulse measurement agains second pulse measurement.guys can see seems exists linear relationship two variables. goal linear regression find best line points. Unfortunately, can draw infinite lines plane.find “best” line need come measure can use compare lines.","code":"\npulse <- read_csv2('https://notendur.hi.is/thj73/data/pulseEn.csv') %>%\n    filter(intervention == 'stationary') %>%\n    na.omit()\npulse %>%\n    ggplot(aes(x = firstPulse, y = secondPulse)) +\n    geom_point() +\n    theme_cowplot()\npulse %>%\n    ggplot(aes(x = firstPulse, y = secondPulse)) +\n    geom_point() +\n    geom_abline(slope = 0, intercept = 70, lty = 2) + \n    geom_abline(slope = 1, intercept = 4, lty = 3) +\n    geom_abline(slope = 1.2, intercept = -4, lty = 4) +\n    geom_abline(slope = 1.4, intercept = -20, lty = 5) +\n    geom_abline(slope = 1.6, intercept = -40, lty = 6) +\n    geom_abline(slope = 1, intercept = 3, col = 'red') +\n    geom_abline(slope = 1, intercept = 6, col = 'blue') + \n    theme_cowplot()"},{"path":"lecture-7-simple-linear-regression.html","id":"the-least-squares-method","chapter":"8 Lecture 7: Simple linear regression","heading":"8.3 The least squares method","text":"complete information find best line easily. Consider following line:\n\\[\ny = 1 + 2x\n\\]\ncan completely reconstruct line two measurements. random element model (\\(\\varepsilon\\)) measurements “corrupted” need “guess”. example “guess”:\\[\n\\tilde{y} = \\tilde{\\beta} _ 0 + \\tilde{\\beta} _ 1 x.\n\\]\\(\\tilde{\\beta} _ 0\\) \\(\\tilde{\\beta} _ 1\\) “guesses”. Luckily, don’t guess randomly exists closed-form solution problem. turns best combination \\(\\tilde{\\beta} _ 0\\) \\(\\tilde{\\beta} _ 1\\) combination minimizes following expression:\\[\n\\begin{aligned}\n\\sum _ {= 1}^n (y_ -(\\tilde{\\beta} _ 0 + \\tilde{\\beta} _ 1x_i))^2 & = \\sum _ {= 1} ^n (y_i - \\tilde{y}_ )^2 \\\\\n&= \\sum _ {= 1}^n e_i ^2.\n\\end{aligned}\n\\]trying minimize square sum . method gets name; measure looking .quantity \\(e_i\\) called \\(\\)-th residual quantifies much guess diverges observed value. best guesses \\(\\beta _ 0\\) \\(\\beta _ 1\\) found customary denote \\(\\hat{\\beta} _ 0\\) \\(\\hat{\\beta} _ 1\\).","code":""},{"path":"lecture-7-simple-linear-regression.html","id":"finding-the-best-line-in-r","chapter":"8 Lecture 7: Simple linear regression","heading":"8.4 Finding the best line in R","text":"can use lm() function R fit linear model pulse data.see results model use summary() function:lot unpack now concentrate “Coefficients” part output.output see \\(\\hat{\\beta} _ 0\\) 3.487 \\(\\hat{\\beta} _ 1\\) 0.952. Let’s plot best line measurements.","code":"\nlinearModel <- lm(secondPulse ~ firstPulse, data = pulse)\nsummary(linearModel)\n#> \n#> Call:\n#> lm(formula = secondPulse ~ firstPulse, data = pulse)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -24.123  -3.082   0.301   2.582  53.678 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)    3.487      2.371   1.471    0.143    \n#> firstPulse     0.952      0.033  28.852   <2e-16 ***\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 5.809 on 255 degrees of freedom\n#> Multiple R-squared:  0.7655, Adjusted R-squared:  0.7646 \n#> F-statistic: 832.4 on 1 and 255 DF,  p-value: < 2.2e-16\nbroom::tidy(linearModel) %>%\n    kbl(booktabs = T) %>%\n    kable_styling(full_width = F)\npulse %>%\n    ggplot(aes(x = firstPulse, y = secondPulse)) +\n    geom_point() +\n    stat_smooth(method = 'lm', se = F) +\n    theme_cowplot()"},{"path":"lecture-7-simple-linear-regression.html","id":"hypothesis-tests-1","chapter":"8 Lecture 7: Simple linear regression","heading":"8.5 Hypothesis tests","text":"estimate standard error, \\(t\\)-statistic \\(p\\)-value. hypothesis test :\\[\n\\begin{aligned}\nH_0&: \\mbox{ parameter } \\beta _ = 0 \\\\\nH_1&: \\mbox{ parameter } \\beta _ \\neq 0\n\\end{aligned}\n\\]test statistic hypothesis :\n\\[\nt = \\frac{\\hat{\\beta} _ }{se(\\hat{\\beta} _ )}\n\\]Looking \\(p\\)-value column can see can comfortably reject null hypothesis (\\(\\alpha = 0.05\\)) slope, intercept. can get confidence intervals parameters using conf.int() function:Sometimes want plot coefficients confidence intervals. can done quite easily conf.int option tidy() function package broom.","code":"\nconfint(linearModel, level = 0.95)\n#>                  2.5 %   97.5 %\n#> (Intercept) -1.1818366 8.156766\n#> firstPulse   0.8870579 1.017021\nbroom::tidy(linearModel, conf.int = T) %>%\n    ggplot(aes(x = estimate, y = term)) +\n    geom_errorbarh(aes(xmin = conf.low, xmax = conf.high), height = 0) +\n    geom_point(col = 'red', size = 2) +\n    theme_cowplot() "},{"path":"lecture-7-simple-linear-regression.html","id":"what-about-sigma-2","chapter":"8 Lecture 7: Simple linear regression","heading":"8.6 What about \\(\\sigma ^2\\)?","text":"denote estimate \\(\\sigma ^2\\) \\(\\hat{\\sigma}^2\\) define :\n\\[\n\\hat{\\sigma}^2 = \\frac{1}{n - k}\\sum _ {= 1} ^n (y_i - \\hat{y}_ )^2,\n\\]\n\\(k\\) number parameters model (excluding \\(\\sigma ^2\\)). estimate \\(\\hat{\\sigma}\\) can seen output summary() function. Alternatively can use glance() function broom package.see \\(\\hat{\\sigma}\\) roughly 5.8087.","code":"\nbroom::glance(linearModel) %>%\n    kbl(booktabs = T) %>%\n    kable_styling(full_width = F)"},{"path":"lecture-7-simple-linear-regression.html","id":"the-coefficient-of-determination-r2","chapter":"8 Lecture 7: Simple linear regression","heading":"8.7 The coefficient of determination \\(R^2\\)","text":"One way evaluate goodness fit compute coefficient determination \\(R^2\\):\\[\nR^2 = 1 - \\frac{\\sum _ {= 1} ^n (y_i - \\hat{y}_ )^2}{\\sum _ {= 1} ^n (y_i - \\bar{y}_ )^2}\n\\]denominator \\(R^2\\) variation data numerator sum squares residuals. value \\(R^2\\) represents amount variation data explained model. model, \\(R^2\\) value 0.7655. , model explains 76.5% variation data. ’s pretty decent.simple linear regression model coefficient correlation \\(r\\) coefficient determination \\(R^2\\) related. can compute correlation two variables cor() function.Observe happens raise correlation coefficient power two:","code":"\ncor(pulse$firstPulse, pulse$secondPulse)\n#> [1] 0.8749319\ncor(pulse$firstPulse, pulse$secondPulse)^2\n#> [1] 0.7655058"},{"path":"lecture-7-simple-linear-regression.html","id":"prediction","chapter":"8 Lecture 7: Simple linear regression","heading":"8.8 Prediction","text":"fitted model can now use make predictions. two types prediction: interpolation extrapolation.Interpolation: use model predict value new observation falls within range independent variable.Interpolation: use model predict value new observation falls within range independent variable.Extrapolation: use model predict value new observation falls outside range independent variable.Extrapolation: use model predict value new observation falls outside range independent variable.Interpolation usually works well model extrapolation can dangerous.use use predict() function make predictions.","code":"\nnewData <- data.frame(firstPulse = c(78.7674, 200))\npredict(linearModel, newData)\n#>         1         2 \n#>  78.47714 193.89537"},{"path":"lecture-8-multiple-regression.html","id":"lecture-8-multiple-regression","chapter":"9 Lecture 8: Multiple regression","heading":"9 Lecture 8: Multiple regression","text":"","code":"\nlibrary(tidyverse)\nlibrary(cowplot)    # Theme\nlibrary(faraway)    # Data\nlibrary(table1)     # Table1\nlibrary(knitr)      # kable\nlibrary(kableExtra) # Pretty kables"},{"path":"lecture-8-multiple-regression.html","id":"ancova","chapter":"9 Lecture 8: Multiple regression","heading":"9.1 ANCOVA","text":"analysis covariance (ANCOVA) linear model one continuous variables one categorical (factor) variable. use cathedral data faraway package demonstrate ANCOVA model. Let’s summarize quickly.data consists 25 observations 3 variables. variables style, x, y.style: architectural style cathedral. two levels: r Romanesque g Gothic.style: architectural style cathedral. two levels: r Romanesque g Gothic.x: nave height cathedral. Measured feet.x: nave height cathedral. Measured feet.y: length cathedral. Measured feet.y: length cathedral. Measured feet.Let’s plot data see looks like:many models can fit data interested three:model uses x: one slope, one intercept.model uses x: one slope, one intercept.model uses x style: one slope, two intercepts.model uses x style: one slope, two intercepts.model uses x style: two slopes, two intercepts.model uses x style: two slopes, two intercepts.","code":"\n# Load data\ndata('cathedral')\nglimpse(cathedral)\n#> Rows: 25\n#> Columns: 3\n#> $ style <fct> r, r, r, r, r, r, r, r, r, g, g, g, g, g, g,…\n#> $ x     <dbl> 75, 80, 68, 64, 83, 80, 70, 76, 74, 100, 75,…\n#> $ y     <dbl> 502, 522, 425, 344, 407, 451, 551, 530, 547,…\ncathedral %>%\n    ggplot(aes(x = x, y = y, color = style)) +\n    geom_point() +\n    scale_color_brewer(type = 'seq', palette = 'Set1') +\n    theme_cowplot() +\n    theme(legend.position = 'bottom')"},{"path":"lecture-8-multiple-regression.html","id":"the-first-model-one-intercept-one-slope","chapter":"9 Lecture 8: Multiple regression","heading":"9.1.1 The first model: one intercept, one slope","text":"first model uses x assumes data explained single line. Let’s fit model look results.Let’s add regression line plot data:","code":"\nancovM1 <- lm(y ~ x, data = cathedral)\nsummary(ancovM1)\n#> \n#> Call:\n#> lm(formula = y ~ x, data = cathedral)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -201.601  -31.241    4.378   52.097  147.745 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)   76.420     89.258   0.856 0.400739    \n#> x              4.669      1.172   3.985 0.000584 ***\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 85.83 on 23 degrees of freedom\n#> Multiple R-squared:  0.4084, Adjusted R-squared:  0.3827 \n#> F-statistic: 15.88 on 1 and 23 DF,  p-value: 0.0005838\ncathedral %>%\n    ggplot(aes(x = x, y = y)) +\n    geom_point(aes(color = style)) +\n    stat_smooth(method = 'lm', se = F) +\n    scale_color_brewer(type = 'seq', palette = 'Set1') +\n    theme_cowplot() +\n    theme(legend.position = 'bottom')\n#> `geom_smooth()` using formula 'y ~ x'"},{"path":"lecture-8-multiple-regression.html","id":"the-second-model-same-slope-different-intercepts","chapter":"9 Lecture 8: Multiple regression","heading":"9.1.2 The second model: same slope, different intercepts","text":"Looking plot might ask maybe two lines describe data better. Let’s test adding style model.Look output. coefficient x still slope now two intercept. actual reported intercept term (Intercept) baseline level g part intercept term Romanesque style. get actual intercept term level r need add two terms together.useful function fortify() function ggplot2 package included tidyverse package.fortify() function gives us data frame independent dependent variables along predicted values diagnostics quantities. lecture concern independent dependent variables well .fitted column.can now use fortified data frame plot results model:now see two lines. parallel make sense slope. difference two intercept.","code":"\nancovM2 <- lm(y ~ x + style, data = cathedral)\nsummary(ancovM2)\n#> \n#> Call:\n#> lm(formula = y ~ x + style, data = cathedral)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -172.67  -30.44   20.38   55.02   96.50 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)   44.298     81.648   0.543   0.5929    \n#> x              4.712      1.058   4.452   0.0002 ***\n#> styler        80.393     32.306   2.488   0.0209 *  \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 77.53 on 22 degrees of freedom\n#> Multiple R-squared:  0.5384, Adjusted R-squared:  0.4964 \n#> F-statistic: 12.83 on 2 and 22 DF,  p-value: 0.0002028\nsum(coefficients(ancovM2)[c(1, 3)])\n#> [1] 124.6905\nfortM2 <- fortify(ancovM2)\nfortM2 %>% \n    as_tibble()\n#> # A tibble: 25 × 9\n#>        y     x style  .hat .sigma  .cooksd .fitted  .resid\n#>    <dbl> <dbl> <fct> <dbl>  <dbl>    <dbl>   <dbl>   <dbl>\n#>  1   502    75 r     0.111   79.2 0.00447     478.   23.9 \n#>  2   522    80 r     0.117   79.2 0.00345     502.   20.4 \n#>  3   425    68 r     0.119   79.2 0.00342     445.  -20.1 \n#>  4   344    64 r     0.131   77.0 0.0653      426.  -82.2 \n#>  5   407    83 r     0.125   75.2 0.107       516. -109.  \n#>  6   451    80 r     0.117   78.5 0.0213      502.  -50.6 \n#>  7   551    70 r     0.115   76.1 0.0757      455.   96.5 \n#>  8   530    76 r     0.112   78.6 0.0175      483.   47.2 \n#>  9   547    74 r     0.111   77.5 0.0423      473.   73.6 \n#> 10   519   100 g     0.180   79.3 0.000185    515.    3.54\n#> # … with 15 more rows, and 1 more variable: .stdresid <dbl>\nfortM2 <- \n    fortM2 %>%\n    select(y, x, style, .fitted)\nfortM2 %>%\n    ggplot(aes(x = x, color = style)) +\n    geom_point(aes(y = y)) +\n    geom_line(aes(y = .fitted)) +\n    scale_color_brewer(type = 'seq', palette = 'Set1') +\n    theme_cowplot() + \n    theme(legend.position = 'bottom') "},{"path":"lecture-8-multiple-regression.html","id":"the-third-model-two-slopes-two-intercepts","chapter":"9 Lecture 8: Multiple regression","heading":"9.1.3 The third model: two slopes, two intercepts","text":"Allowing two intercept seems improved model. allow second slope well?now four coefficients. intercept (Intercept) slope x regression line Gothic-style cathedral intercept styler slope x:styler regression line Romanesque-style cathedral. get true intercept slope Romanesque style need add intercept terms together slope terms:plot regression lines data using fortify() version data:","code":"\nancovM3 <- lm(y ~ x * style, data = cathedral)\nsummary(ancovM3)\n#> \n#> Call:\n#> lm(formula = y ~ x * style, data = cathedral)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -172.68  -30.22   23.75   55.78   89.50 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)   37.111     85.675   0.433 0.669317    \n#> x              4.808      1.112   4.322 0.000301 ***\n#> styler       204.722    347.207   0.590 0.561733    \n#> x:styler      -1.669      4.641  -0.360 0.722657    \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 79.11 on 21 degrees of freedom\n#> Multiple R-squared:  0.5412, Adjusted R-squared:  0.4757 \n#> F-statistic: 8.257 on 3 and 21 DF,  p-value: 0.0008072\nsum(coefficients(ancovM3)[c(1, 3)])\n#> [1] 241.8327\nsum(coefficients(ancovM3)[c(2, 4)])\n#> [1] 3.138068\nfortify(ancovM3) %>%\n    ggplot(aes(x = x, y = y, color = style)) +\n    geom_point() +\n    geom_line(aes(y = .fitted)) +\n    scale_color_brewer(type = 'seq', palette = 'Set1') +\n    theme_cowplot() +\n    theme(legend.position = 'bottom')"},{"path":"lecture-8-multiple-regression.html","id":"three-models-but-which-model","chapter":"9 Lecture 8: Multiple regression","heading":"9.1.4 Three models, but which model?","text":"fitted three models data. now need way select best model three, based measure. Let’s get coefficient determination \\(R^2\\) adjusted version model compare values:can see \\(R^2\\) increased added variables model. inherit property \\(R^2\\) reason can’t rely blindly judging quality model. adjusted \\(R^2\\) fluctuates however. reason fluctuation adjusted \\(R^2\\) penalizes addition variables model. Based table , model two looks like candidate model. can actually test anova() function.Notice first second model “embedded” sense third model. say third model contains variables used first second model. test whether “reduced” model performs better “full” model use anova() function:null hypothesis test adding variable (variables) improve fit. model three outperformed model two able reject null hypothesis unsuccessful. results ANOVA test come surprise think adjusted \\(R^2\\) values output summaries models.Bottom line: seems like model two intercepts one slope fits data best three models constructed.","code":"\ntibble(model = c('Model 1', 'Model 2', 'Model 3'), \n       r2 = c(summary(ancovM1)$r.squared, \n              summary(ancovM2)$r.squared, \n              summary(ancovM3)$r.squared), \n       r2adj = c(summary(ancovM1)$adj.r.squared, \n              summary(ancovM2)$adj.r.squared, \n              summary(ancovM3)$adj.r.squared)) %>%\n    kbl() %>%\n    kable_styling(full_width = F)\nanova(ancovM2, ancovM3)\n#> Analysis of Variance Table\n#> \n#> Model 1: y ~ x + style\n#> Model 2: y ~ x * style\n#>   Res.Df    RSS Df Sum of Sq      F Pr(>F)\n#> 1     22 132223                           \n#> 2     21 131413  1     809.7 0.1294 0.7227"},{"path":"lecture-8-multiple-regression.html","id":"two-way-anova","chapter":"9 Lecture 8: Multiple regression","heading":"9.2 Two-way ANOVA","text":"two-way ANOVA linear model two categorical (factor variable) continuous response. use rats data faraway package.data consists 48 lines 3 columns. variables data set time, poison, treat:time: survival time rats. Measured tens hours.time: survival time rats. Measured tens hours.poison: type poison rats subjected . variable three levels: , II, III.poison: type poison rats subjected . variable three levels: , II, III.treat: treatment rats subject . variable four levels: , B, C, D.treat: treatment rats subject . variable four levels: , B, C, D.Let’s plot data:\nLooking plots seems poison III potent. Similarly, treatment B seems greatest efficacy. interaction two variables? , effect one variable depend ? interaction, simply use additive model; add effects variable \\(x\\) variable \\(y\\). interaction, things can get complicated.check interactions graphically create interaction plot:\\(x\\)-axis poison type, \\(y\\)-axis mean survival time. lines start crossing, expect interaction effect. plot shows may interactions, graphical evidence unconvincing. Let’s formally test interaction effect.see treatment poison seem significant interaction two variables treat:poison significant. therefore conclude interactions significant.","code":"\ndata('rats')\nglimpse(rats)\n#> Rows: 48\n#> Columns: 3\n#> $ time   <dbl> 0.31, 0.82, 0.43, 0.45, 0.45, 1.10, 0.45, 0…\n#> $ poison <fct> I, I, I, I, I, I, I, I, I, I, I, I, I, I, I…\n#> $ treat  <fct> A, B, C, D, A, B, C, D, A, B, C, D, A, B, C…\nrats %>%\n    ggplot(aes(x = poison, y = time)) +\n    geom_boxplot() +\n    facet_wrap(~treat) +\n    theme_cowplot()\nrats %>%\n    ggplot(aes(x = poison, y = time, group = treat)) +\n    stat_summary(aes(color = treat), fun.y = mean, geom = 'line') +\n    scale_color_brewer(type = 'seq', palette = 'Set1') +\n    theme_cowplot() +\n    theme(legend.position = 'bottom')\n#> Warning: `fun.y` is deprecated. Use `fun` instead.\nanov2w <- lm(time ~ treat * poison, data = rats)\nanova(anov2w)\n#> Analysis of Variance Table\n#> \n#> Response: time\n#>              Df  Sum Sq Mean Sq F value    Pr(>F)    \n#> treat         3 0.92121 0.30707 13.8056 3.777e-06 ***\n#> poison        2 1.03301 0.51651 23.2217 3.331e-07 ***\n#> treat:poison  6 0.25014 0.04169  1.8743    0.1123    \n#> Residuals    36 0.80073 0.02224                      \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"lecture-8-multiple-regression.html","id":"same-method-different-data-set","chapter":"9 Lecture 8: Multiple regression","heading":"9.2.1 Same method, different data set","text":"Let’s look another example. use ToothGrowth data included R.data consists 60 lines 3 columns. variables data set len, supp, dose:len: length tooth.len: length tooth.supp: Supplement type. variable two levels: OJ (orange juice) VC (vitamin C).supp: Supplement type. variable two levels: OJ (orange juice) VC (vitamin C).dose: dose milligrams. levels 0.5, 1.0, 2.0.dose: dose milligrams. levels 0.5, 1.0, 2.0.data set much bigger rats one summarize quickly summary() function.Notice dose treated continuous variable. something don’t want. numbers represent category. therefore recast dose factor variable.Let’s plot data:Let’s also plot interactions.see VC mean length grows nicely increase dose. OJ group see shift going dose 1.0 dose 2.0. two treatment lines cross dose 2.0. Let’s formally test interaction:time see supplement, dose interaction two variables significant.","code":"\nglimpse(ToothGrowth)\n#> Rows: 60\n#> Columns: 3\n#> $ len  <dbl> 4.2, 11.5, 7.3, 5.8, 6.4, 10.0, 11.2, 11.2, 5…\n#> $ supp <fct> VC, VC, VC, VC, VC, VC, VC, VC, VC, VC, VC, V…\n#> $ dose <dbl> 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, …\nsummary(ToothGrowth)\n#>       len        supp         dose      \n#>  Min.   : 4.20   OJ:30   Min.   :0.500  \n#>  1st Qu.:13.07   VC:30   1st Qu.:0.500  \n#>  Median :19.25           Median :1.000  \n#>  Mean   :18.81           Mean   :1.167  \n#>  3rd Qu.:25.27           3rd Qu.:2.000  \n#>  Max.   :33.90           Max.   :2.000\nToothGrowth <- \n    ToothGrowth %>%\n    mutate(dose = factor(dose))\nToothGrowth %>%\n    ggplot(aes(x = dose, y = len, fill = supp)) +\n    geom_boxplot() +\n    scale_fill_brewer(type = 'seq', palette = 'Set1') +\n    theme_cowplot() +\n    theme(legend.position = 'bottom')\nToothGrowth %>%\n    ggplot(aes(x = dose, y = len, group = supp)) +\n    stat_summary(aes(color = supp), fun.y = mean, geom = 'line') +\n    scale_color_brewer(type = 'seq', palette = 'Set1') +\n    theme_cowplot() +\n    theme(legend.position = 'bottom')\n#> Warning: `fun.y` is deprecated. Use `fun` instead.\nanov2wtooth <- lm(len ~ supp * dose, data = ToothGrowth)\nanova(anov2wtooth)\n#> Analysis of Variance Table\n#> \n#> Response: len\n#>           Df  Sum Sq Mean Sq F value    Pr(>F)    \n#> supp       1  205.35  205.35  15.572 0.0002312 ***\n#> dose       2 2426.43 1213.22  92.000 < 2.2e-16 ***\n#> supp:dose  2  108.32   54.16   4.107 0.0218603 *  \n#> Residuals 54  712.11   13.19                      \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"lecture-8-multiple-regression.html","id":"multiple-linear-regression","chapter":"9 Lecture 8: Multiple regression","heading":"9.3 Multiple linear regression","text":"reason stop three variables. can continuously add variables model R happily fit us.’s say always better. general rule thumb parsimony smaller better. want model generalize data hasn’t seen (predict). tailor model closely data use construct run risk overfitting.","code":"\npulse <- read_csv2('https://notendur.hi.is/thj73/data/pulseEn.csv')\nsummary(lm(secondPulse ~ firstPulse*intervention + sex + height + weight, data = pulse))\n#> \n#> Call:\n#> lm(formula = secondPulse ~ firstPulse * intervention + sex + \n#>     height + weight, data = pulse)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -29.622  -5.599  -0.322   3.788  53.701 \n#> \n#> Coefficients:\n#>                                     Estimate Std. Error\n#> (Intercept)                        33.936496   6.050011\n#> firstPulse                          0.880819   0.075194\n#> interventionstationary            -31.353887   7.469346\n#> sex                                -0.752416   1.227185\n#> height                              0.008287   0.006409\n#> weight                              0.002131   0.006963\n#> firstPulse:interventionstationary   0.074302   0.101535\n#>                                   t value Pr(>|t|)    \n#> (Intercept)                         5.609 3.61e-08 ***\n#> firstPulse                         11.714  < 2e-16 ***\n#> interventionstationary             -4.198 3.27e-05 ***\n#> sex                                -0.613    0.540    \n#> height                              1.293    0.197    \n#> weight                              0.306    0.760    \n#> firstPulse:interventionstationary   0.732    0.465    \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 12.18 on 436 degrees of freedom\n#>   (28 observations deleted due to missingness)\n#> Multiple R-squared:  0.6795, Adjusted R-squared:  0.6751 \n#> F-statistic:   154 on 6 and 436 DF,  p-value: < 2.2e-16"},{"path":"lecture-8-multiple-regression.html","id":"the-problem-of-overfitting","chapter":"9 Lecture 8: Multiple regression","heading":"9.3.1 The problem of overfitting","text":"going create fake data set demonstrate danger overfitting. First create “truth”. want dependent variable \\(y\\) created independent variable \\(x\\). want related following way:\\[\ny = 1 + 2 \\cdot x = \\beta _ 0 + \\beta _ 1 x.\n\\]course needs sort randomness model otherwise point analysis. therefore add \\(\\varepsilon \\sim N(0, \\sigma ^2) = N(0, 3^2)\\) data create fluctuations.Let’s fit linear model data see get:results kinda poor ’s OK; just example. Let’s add variables data.Notice now happens fit model includes variables.Notice large \\(R^2\\). ’s almost 1 good can get! Let’s plot two regression “lines” get fit see actually going . begin adding predicted model values data reduce data set pivot long format.Now let’s plot lines:full model goes way close data point can. may seem nice paper full model completely useless make prediction; even interpolation useless. Let’s see happens. range x roughly 0 4. Let’s see happens make model predict x = 3.model predicts 32.4370343 terrible estimate. model useless. doesn’t generalize.finish section ’m going fit ever-increasing models data better show \\(R^2\\) adjusted \\(R^2\\) behave. include code don’t worry don’t understand .can see, \\(R^2\\) increases add variables \\(R^2\\) adjusted fluctuates. \\(R^2\\) adjusted typically used model performance evaluated.","code":"\n# Create data\nset.seed(1)\nn <- 10\nx <- rnorm(n = n, mean = 1, sd = 2)\ny <- 1 + 2 * x + rnorm(n = n, mean = 0, sd = 3)\noverData <- data.frame(x, y)\nlmSmall <- lm(y ~ x, data = overData)\nsummary(lmSmall)\n#> \n#> Call:\n#> lm(formula = y ~ x, data = overData)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -5.1252 -1.8286  0.4301  2.5616  3.0235 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)  \n#> (Intercept)   2.7254     1.3108   2.079   0.0712 .\n#> x             1.2258     0.6731   1.821   0.1061  \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 3.152 on 8 degrees of freedom\n#> Multiple R-squared:  0.2931, Adjusted R-squared:  0.2047 \n#> F-statistic: 3.316 on 1 and 8 DF,  p-value: 0.1061\noverData <- \n    overData %>%\n    mutate(x2 = x^2, x3 = x^3, \n           x4 = x^4, x5 = x^5, \n           x6 = x^6, x7 = x^7, \n           x8 = x^8)\nlmFull <- lm(y ~ ., data = overData)\nsummary(lmFull)\n#> \n#> Call:\n#> lm(formula = y ~ ., data = overData)\n#> \n#> Residuals:\n#>          1          2          3          4          5 \n#> -7.619e-02 -5.659e-01 -2.188e-01  7.279e-05  1.308e+00 \n#>          6          7          8          9         10 \n#>  2.595e-01 -1.734e+00 -1.127e-01  1.058e+00  8.274e-02 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)\n#> (Intercept)   4.6104     5.3893   0.855    0.549\n#> x            -3.3422     7.7297  -0.432    0.740\n#> x2           -1.6993    61.1088  -0.028    0.982\n#> x3           10.8544    51.8809   0.209    0.869\n#> x4          -12.8163    78.8381  -0.163    0.897\n#> x5           11.0604   122.0096   0.091    0.942\n#> x6           -6.2146    68.9239  -0.090    0.943\n#> x7            1.7458    17.3631   0.101    0.936\n#> x8           -0.1794     1.6103  -0.111    0.929\n#> \n#> Residual standard error: 2.509 on 1 degrees of freedom\n#> Multiple R-squared:  0.944,  Adjusted R-squared:  0.4961 \n#> F-statistic: 2.108 on 8 and 1 DF,  p-value: 0.4896\noverDataLong <- \n    overData %>%\n    mutate(smallPred = predict(lmSmall), \n           fullPred = predict(lmFull)) %>%\n    select(x, y, smallPred, fullPred) %>%\n    gather(regression, value, -x, -y)\noverDataLong\n#>             x          y regression      value\n#> 1  -0.2529076  5.0295283  smallPred  2.4154335\n#> 2   1.3672866  4.9041030  smallPred  4.4014570\n#> 3  -0.6712572 -2.2062362  smallPred  1.9026233\n#> 4   4.1905616  2.7370235  smallPred  7.8622089\n#> 5   1.6590155  7.6928238  smallPred  4.7590564\n#> 6  -0.6409368 -0.4166744  smallPred  1.9397899\n#> 7   1.9748581  4.9011454  smallPred  5.1462141\n#> 8   2.4766494  8.7848075  smallPred  5.7613066\n#> 9   2.1515627  7.7667890  smallPred  5.3628174\n#> 10  0.3892232  3.5601504  smallPred  3.2025533\n#> 11 -0.2529076  5.0295283   fullPred  5.1057217\n#> 12  1.3672866  4.9041030   fullPred  5.4700126\n#> 13 -0.6712572 -2.2062362   fullPred -1.9874255\n#> 14  4.1905616  2.7370235   fullPred  2.7369508\n#> 15  1.6590155  7.6928238   fullPred  6.3851049\n#> 16 -0.6409368 -0.4166744   fullPred -0.6761837\n#> 17  1.9748581  4.9011454   fullPred  6.6351488\n#> 18  2.4766494  8.7848075   fullPred  8.8975269\n#> 19  2.1515627  7.7667890   fullPred  6.7091933\n#> 20  0.3892232  3.5601504   fullPred  3.4774106\noverDataLong %>%\n    ggplot(aes(x = x)) +\n    geom_point(aes(y = y)) +\n    geom_line(aes(y = value, color = regression)) +\n    scale_color_brewer(type = 'seq', palette = 'Set1') +\n    theme_cowplot() +\n    theme(legend.position = 'bottom')\nnewPredict <- data.frame(x = 3, x2 = 3^2, x3 = 3^3, \n                         x4 = 3^4, x5 = 3^5, x6 = 3^6, \n                         x7 = 3^7, x8 = 3^8)\npredict(lmFull, newPredict)\n#>        1 \n#> 32.43703\nX <- model.matrix(lmFull)[, -1]\nlmNull <- lm(y ~ 1, data = overData)\nr2Data <- list()\nfor(i in 1:ncol(X)) {\n    lmTmp <- update(lmNull, . ~ . + X[, 1:i])\n    tmp <- data.frame(pred = (i + 1), \n                      r2 = summary(lmTmp)$r.squared, \n                      r2adj = summary(lmTmp)$adj.r.squared)\n    r2Data[[i]] <- tmp\n}\ndo.call(rbind, r2Data) %>%\n    gather(type, val, -pred) %>%\n    mutate(pred = factor(pred)) %>%\n    ggplot(aes(x = pred, y = val, color = type, group = type, lty = type)) +\n    geom_point() +\n    geom_line() +\n    scale_color_brewer(type = 'seq', palette = 'Set1') +\n    theme_cowplot() +\n    labs(x = 'Number of predictors', y = 'Value') +\n    theme(legend.position = 'bottom')"},{"path":"lecture-9-diagnostics.html","id":"lecture-9-diagnostics","chapter":"10 Lecture 9: Diagnostics","heading":"10 Lecture 9: Diagnostics","text":"fitting model important diagnose . Things need examine :High leverage points, outliers, influential points.High leverage points, outliers, influential points.assumptions linear regression: linearity, normality, heteroskedasticity.assumptions linear regression: linearity, normality, heteroskedasticity.using pulse data fit model diagnose.model fit following:Let’s look summary model:can see, coefficients highly significant adjusted \\(R^2\\) value 0.688974. model ANCOVA model (one continuous variable, one categorical variable) two intercepts one slope can plot regression lines data.fitted model ’s time diagnose . use fortify() function create data set contains (almost) variables need diagnose model. say almost need add studentized residuals data set. talk studentized residuals discuss outliers.","code":"\nlibrary(tidyverse)\nlibrary(ggrepel)\nlibrary(cowplot)\nlibrary(knitr)\nlibrary(kableExtra)\npulse <- read_csv2('https://notendur.hi.is/thj73/data/pulseEn.csv') \nlm1 <- lm(secondPulse ~ firstPulse + intervention, data = pulse)\nsummary(lm1)\n#> \n#> Call:\n#> lm(formula = secondPulse ~ firstPulse + intervention, data = pulse)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -29.274  -4.883  -0.122   3.838  53.886 \n#> \n#> Coefficients:\n#>                         Estimate Std. Error t value\n#> (Intercept)             31.71432    3.68556   8.605\n#> firstPulse               0.91999    0.04822  19.078\n#> interventionstationary -25.99111    1.16796 -22.253\n#>                        Pr(>|t|)    \n#> (Intercept)              <2e-16 ***\n#> firstPulse               <2e-16 ***\n#> interventionstationary   <2e-16 ***\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 12.04 on 451 degrees of freedom\n#>   (17 observations deleted due to missingness)\n#> Multiple R-squared:  0.6903, Adjusted R-squared:  0.689 \n#> F-statistic: 502.7 on 2 and 451 DF,  p-value: < 2.2e-16\nintercept1 <- coef(lm1)[1]\nintercept2 <- sum(coef(lm1)[c(1, 3)])\nslope <- coef(lm1)[2]\npulse %>%\n    na.omit() %>%\n    ggplot(aes(x = firstPulse, y = secondPulse, color = intervention)) +\n    geom_point() +\n    geom_abline(slope = slope, intercept = intercept1, lty = 2) +\n    geom_abline(slope = slope, intercept = intercept2, lty = 3) +\n    scale_color_brewer(type = 'seq', palette = 'Set1') +\n    theme_cowplot() +\n    theme(legend.position = 'bottom')\nflm1 <- fortify(lm1)            \nflm1$.rstudent <- rstudent(lm1) # studentized residuals\nflm1$.index <- rownames(flm1)   # observation number\nk <- ncol(model.matrix(lm1))    # number of predictors\nn <- nrow(model.matrix(lm1))    # number of observations\nflm1 <- \n    flm1 %>%\n    relocate(.index, .before = 'secondPulse')"},{"path":"lecture-9-diagnostics.html","id":"leverages-outliers-and-influential-points","chapter":"10 Lecture 9: Diagnostics","heading":"10.1 Leverages, outliers, and influential points","text":"","code":""},{"path":"lecture-9-diagnostics.html","id":"leverage-points","chapter":"10 Lecture 9: Diagnostics","heading":"10.1.1 Leverage points","text":"Leverages way quantify “weirdness” independent variables. mathematics leverages rely hat matrix \\(H\\) discuss requires linear algebra. Leverages 0 1, higher leverage implies “weirdness” independent variables. differentiate high -high leverages use heuristic \\(2k/n\\) \\(k\\) number predictors model \\(n\\) number observations used modeling process.variable .hat fortified data represents leverage values denote \\(h_i\\) \\(\\) \\(\\)-th row data. Let’s plot values label point exceeds threshold.Many data points seem leverage value exceeds threshold. values relatively small however; remember leverage 0 1 larger value implies “strangeness”. Let’s take look data point highest leverage.","code":"\nflm1 %>%\n    ggplot(aes(x = .index, y = .hat)) +\n    geom_point() +\n    geom_text_repel(aes(label = ifelse(.hat > 2*k/n, .index, ''))) +\n    geom_hline(yintercept = 2*k/n, lty = 2) +\n    theme_cowplot() +\n    theme(axis.ticks.x = element_blank(), \n          axis.text.x = element_blank())\nflm1 %>%\n    filter(.index == 170) %>%\n    kbl() %>%\n    kable_styling(full_width = F)"},{"path":"lecture-9-diagnostics.html","id":"outliers","chapter":"10 Lecture 9: Diagnostics","heading":"10.1.2 Outliers","text":"Outliers “strange” dependent values. main tool detect outliers studentized residuals \\(t_i\\). studentized residuals computed standardized residuals \\(r_i\\). defined:\\[\nr_i = \\frac{e_i}{\\sqrt{1 - h_i}\\hat{\\sigma}}, \\quad t_i = r_i \\sqrt{\\frac{n - k - 1}{n - k - r_i^2}}\n\\], \\(e_i\\) residual model, \\(h_i\\) leverage, \\(\\hat{\\sigma}\\) estimate \\(\\sigma\\), \\(n\\) \\(k\\) . standardized residual name standardized residual model.studentized residual define following heuristic: studentized residual exceeds -3 3 mark point potential outlier.see 10 values exceed threshold therefore subjected testing. null hypothesis studentized residual come \\(t\\)-distribution \\(n - k - 1\\) degrees freedom. reject null hypothesis studentized residual value exceeds theoretical \\(t\\)-value. test every residual need divide significance level \\(\\alpha = 0.05\\) tests. Therefore, theoretical \\(t\\)-value \\(t_{1 - \\frac{\\alpha}{2n}, n - p - 1}\\). distributing significance level equally tests; method equally distributing significance level called Bonferroni’s correction.Four points stand . Let’s remember indices continue diagnose model.","code":"\nflm1 %>%\n    ggplot(aes(x = .index, y = .rstudent)) +\n    geom_point() +\n    geom_hline(yintercept = 0, lty = 2) +\n    geom_hline(yintercept = 1, lty = 2, col = 'red') +\n    geom_hline(yintercept = -1, lty = 2, col = 'red') +\n    geom_hline(yintercept = 2, lty = 2, col = 'blue') +\n    geom_hline(yintercept = -2, lty = 2, col = 'blue') +\n    geom_hline(yintercept = 3, lty = 2, col = 'green') +\n    geom_hline(yintercept = -3, lty = 2, col = 'green') +\n    geom_text_repel(aes(label = ifelse(abs(.rstudent) > 3, .index, ''))) +\n    theme_cowplot() + \n    theme(axis.ticks.x = element_blank(), \n          axis.text.x = element_blank())\ntval <- qt(p = 1 - 0.05/(2 * n), df = n - k - 1)    # theoretical value\nflm1 %>%\n    mutate(rejectNull = abs(.rstudent) > tval) %>%\n    filter(rejectNull == T) %>%\n    kbl() %>%\n    kable_styling(full_width = F)"},{"path":"lecture-9-diagnostics.html","id":"influential-data-points","chapter":"10 Lecture 9: Diagnostics","heading":"10.1.3 Influential data points","text":"influential data point data point whose removal data causes large changes fit. misbehaving data points singular, , one, can usually identified quickly decision can made whether include . However, multiple strange data points may mask effect . One best ways determine whether data point influential examining studentized residual leverages. However, nice somehow combine two quantities single measure help us analysis. exactly purpose Cook’s distance, define :\\[\nC_i = \\frac{r_i^2}{k} \\frac{h_i}{(1 - h_i)}\n\\]constitutes “high” Cook’s distance? ’s relative. Generally calculate \\(C_i\\) data points, plot inspect data points considerably larger \\(C_i\\) value relative data points.see influential points. influential points see old acquaintances. points focus attention .","code":"\nflm1 %>%\n    ggplot(aes(x = .index, y = .cooksd)) +\n    geom_point() +\n    geom_hline(yintercept = 0.02, lty = 2) +\n    geom_text_repel(aes(label = ifelse(.cooksd > 0.02, .index, ''))) +\n    theme_cowplot() + \n    theme(axis.ticks.x = element_blank(), \n          axis.text.x = element_blank())"},{"path":"lecture-9-diagnostics.html","id":"assumptions-of-linear-regression","chapter":"10 Lecture 9: Diagnostics","heading":"10.2 Assumptions of linear regression","text":"","code":""},{"path":"lecture-9-diagnostics.html","id":"heteroskedasticity-and-linearity","chapter":"10 Lecture 9: Diagnostics","heading":"10.2.1 Heteroskedasticity and linearity","text":"important plot examine linearity heteroskedasticity fitted-values-versus-residuals plot:\nplot don’t really see evidence non-linearity however seems evidence heteroskedasticity. see .fitted increases size .resid.Another important plot residual plot. can choose plot residuals standardized residuals, prefer standardized residuals.","code":"\nflm1 %>%\n    ggplot(aes(x = .fitted, y = .resid)) +\n    geom_point() +\n    stat_smooth(method = 'loess') +\n    theme_cowplot()\n#> `geom_smooth()` using formula 'y ~ x'\nflm1 %>%\n    ggplot(aes(x = .index, y = .stdresid)) +\n    geom_point() +\n    geom_hline(yintercept = 0, lty = 2) +\n    theme_cowplot() +\n    theme(axis.ticks.x = element_blank(), \n          axis.text.x = element_blank())"},{"path":"lecture-9-diagnostics.html","id":"normality","chapter":"10 Lecture 9: Diagnostics","heading":"10.2.2 Normality","text":"use \\(QQ\\)-plot assess normal distribution. \\(x\\)-axis theoretical distribution; \\(y\\)-axis distribution residuals.model completely fails normal assumption. extremely heavy tails.","code":"\nflm1 %>%\n    ggplot(aes(sample = .stdresid)) +\n    stat_qq() +\n    stat_qq_line() +\n    theme_cowplot()  +\n    labs(x = 'Theoretical', y = 'Actual')"},{"path":"lecture-9-diagnostics.html","id":"what-should-we-do","chapter":"10 Lecture 9: Diagnostics","heading":"10.3 What should we do?","text":"High-leverage points, outliers, influential points: Check data data entry errors; examine physical context; exclude data point. choose exclude data point always report . Otherwise run risk accused dishonesty.High-leverage points, outliers, influential points: Check data data entry errors; examine physical context; exclude data point. choose exclude data point always report . Otherwise run risk accused dishonesty.Non-normality, linearity: Transform response (dependent) variable; add/remove/transform independent variables; change assumption normal assumption (Binomial, Gamma, etc).Non-normality, linearity: Transform response (dependent) variable; add/remove/transform independent variables; change assumption normal assumption (Binomial, Gamma, etc).Non-constant variance, correlated errors: Use generalized least square method. need estimate error structure.Non-constant variance, correlated errors: Use generalized least square method. need estimate error structure.process requires many iterations fitting diagnostics.","code":""}]
