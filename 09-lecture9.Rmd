# Lecture 9: Diagnostics

```{r, warning = F, message = F}
library(tidyverse)
library(ggrepel)
library(cowplot)
library(knitr)
library(kableExtra)
```

After fitting a model it is important to diagnose it. Things we need to examine are:

1. High leverage points, outliers, and influential points.

2. The assumptions of linear regression: linearity, normality, heteroskedasticity.

We will be using the pulse data to fit a model which we will then diagnose.

```{r, warning = F, message = F}
pulse <- read_csv2('https://notendur.hi.is/thj73/data/pulseEn.csv') 
```

The model we will fit is the following:

```{r}
lm1 <- lm(secondPulse ~ firstPulse + intervention, data = pulse)
```

Let's look at the summary of our model:

```{r}
summary(lm1)
```

As you can see, all coefficients are highly significant and our adjusted $R^2$ value is `r summary(lm1)$adj.r.squared`. As our model is an ANCOVA model (one continuous variable, one categorical variable) with two intercepts and one slope we can plot the regression lines over our data.

```{r}
intercept1 <- coef(lm1)[1]
intercept2 <- sum(coef(lm1)[c(1, 3)])
slope <- coef(lm1)[2]
pulse %>%
    na.omit() %>%
    ggplot(aes(x = firstPulse, y = secondPulse, color = intervention)) +
    geom_point() +
    geom_abline(slope = slope, intercept = intercept1, lty = 2) +
    geom_abline(slope = slope, intercept = intercept2, lty = 3) +
    scale_color_brewer(type = 'seq', palette = 'Set1') +
    theme_cowplot() +
    theme(legend.position = 'bottom')
```

Having fitted our model it's time to diagnose it. We will use the `fortify()` function to create a data set which contains (almost) all the variables we need to diagnose the model. I say almost because we need to add the *studentized* residuals to the data set. We will talk about studentized residuals when we discuss outliers.

```{r}
flm1 <- fortify(lm1)            
flm1$.rstudent <- rstudent(lm1) # studentized residuals
flm1$.index <- rownames(flm1)   # observation number
k <- ncol(model.matrix(lm1))    # number of predictors
n <- nrow(model.matrix(lm1))    # number of observations
flm1 <- 
    flm1 %>%
    relocate(.index, .before = 'secondPulse')
```

## Leverages, outliers, and influential points

### Leverage points
Leverages are a way to quantify any "weirdness" in our independent variables. The mathematics of leverages rely on the hat matrix $H$ which we will not discuss as it requires linear algebra. Leverages are between 0 and 1, where a higher leverage implies more "weirdness" in the independent variables. To differentiate between *high* and *not-high* leverages we will use the heuristic of $2k/n$ where $k$ is the number of predictors in the model and $n$ the number of observations used in the modeling process. 

The variable `.hat` in the fortified data represents our leverage values which we will denote with $h_i$ where $i$ is the $i$-th row in the data. Let's plot these values and label any point that exceeds our threshold.

```{r}
flm1 %>%
    ggplot(aes(x = .index, y = .hat)) +
    geom_point() +
    geom_text_repel(aes(label = ifelse(.hat > 2*k/n, .index, ''))) +
    geom_hline(yintercept = 2*k/n, lty = 2) +
    theme_cowplot() +
    theme(axis.ticks.x = element_blank(), 
          axis.text.x = element_blank())
```

Many data points seem to have a leverage value which exceeds our threshold. The values are relatively small however; remember that it the leverage is between 0 and 1 where a larger value implies more "strangeness". Let's take a look at the data of the point with the highest leverage. 

```{r}
flm1 %>%
    filter(.index == 170) %>%
    kbl() %>%
    kable_styling(full_width = F)
```


### Outliers
Outliers are "strange" dependent values. Our main tool to detect outliers are the studentized residuals $t_i$. The studentized residuals are computed from the *standardized* residuals $r_i$. Here is how both are defined:

$$
r_i = \frac{e_i}{\sqrt{1 - h_i}\hat{\sigma}}, \quad t_i = r_i \sqrt{\frac{n - k - 1}{n - k - r_i^2}}
$$

Here, $e_i$ is the residual from our model, $h_i$ is the leverage, $\hat{\sigma}$ is our estimate of $\sigma$, and $n$ and $k$ are as before. The standardized residual has that name because we have standardized the residual from our model.

For the *studentized* residual we define the following heuristic: if the studentized residual exceeds -3 or 3 we will mark that point as a potential outlier. 

```{r}
flm1 %>%
    ggplot(aes(x = .index, y = .rstudent)) +
    geom_point() +
    geom_hline(yintercept = 0, lty = 2) +
    geom_hline(yintercept = 1, lty = 2, col = 'red') +
    geom_hline(yintercept = -1, lty = 2, col = 'red') +
    geom_hline(yintercept = 2, lty = 2, col = 'blue') +
    geom_hline(yintercept = -2, lty = 2, col = 'blue') +
    geom_hline(yintercept = 3, lty = 2, col = 'green') +
    geom_hline(yintercept = -3, lty = 2, col = 'green') +
    geom_text_repel(aes(label = ifelse(abs(.rstudent) > 3, .index, ''))) +
    theme_cowplot() + 
    theme(axis.ticks.x = element_blank(), 
          axis.text.x = element_blank())
```

We see that `r nrow(flm1 %>% filter(abs(.rstudent) > 3))` values exceed our threshold and will therefore be subjected to further testing. Under the null hypothesis our studentized residual should come from a $t$-distribution with $n - k - 1$ degrees of freedom. We will reject the null hypothesis if our studentized residual value exceeds our theoretical $t$-value. As we will test every residual we need to divide our significance level $\alpha = 0.05$ between the tests. Therefore, our theoretical $t$-value is $t_{1 - \frac{\alpha}{2n}, n - p - 1}$. We are distributing our significance level equally between the tests; this method of equally distributing the significance level is called Bonferroni's correction.

```{r}
tval <- qt(p = 1 - 0.05/(2 * n), df = n - k - 1)    # theoretical value
flm1 %>%
    mutate(rejectNull = abs(.rstudent) > tval) %>%
    filter(rejectNull == T) %>%
    kbl() %>%
    kable_styling(full_width = F)
```

Four points stand out. Let's remember their indices as we continue to diagnose the model.


### Influential data points
An influential data point is a data point whose removal from the data causes large changes in the fit. When the misbehaving data points are singular, that is, there is only one, it can usually be identified quickly and a decision can be made whether to include it or not. However, when there are multiple strange data points they may mask the effect of each other. One of the best ways to determine whether a data point is influential is by examining the studentized residual and the leverages. However, it would be nice if we could somehow combine those two quantities into a single measure to help us with our analysis. That is exactly the purpose of Cook’s distance, which we define as:

$$
C_i = \frac{r_i^2}{k} \frac{h_i}{(1 - h_i)}
$$

So what constitutes a "high" Cook’s distance? It’s relative. Generally you calculate $C_i$ for all your data points, plot them and inspect data points which have a considerably larger $C_i$ value relative to other data points.

```{r}
flm1 %>%
    ggplot(aes(x = .index, y = .cooksd)) +
    geom_point() +
    geom_hline(yintercept = 0.02, lty = 2) +
    geom_text_repel(aes(label = ifelse(.cooksd > 0.02, .index, ''))) +
    theme_cowplot() + 
    theme(axis.ticks.x = element_blank(), 
          axis.text.x = element_blank())
```

We see some influential points. Of the influential points we see some old acquaintances. These are the points we should focus our attention on.

## Assumptions of linear regression

### Heteroskedasticity and linearity
The most important plot to examine linearity and heteroskedasticity is the fitted-values-versus-residuals plot:

```{r}
flm1 %>%
    ggplot(aes(x = .fitted, y = .resid)) +
    geom_point() +
    stat_smooth(method = 'loess') +
    theme_cowplot()
```
From the plot above we don't really see any evidence of non-linearity however there seems to be evidence of heteroskedasticity. We see this because as `.fitted` increases so does the size of `.resid`. 

Another important plot is the residual plot. You can choose to plot the residuals or the standardized residuals, I prefer the standardized residuals. 

```{r}
flm1 %>%
    ggplot(aes(x = .index, y = .stdresid)) +
    geom_point() +
    geom_hline(yintercept = 0, lty = 2) +
    theme_cowplot() +
    theme(axis.ticks.x = element_blank(), 
          axis.text.x = element_blank())
```
We use the plot above to evaluate heteroskedasticity as well. It isn't as obvious on this plot, hence why the fitted-values-versus-residuals plot is the most important one. You should always look at it anyways. 

### Normality
We use the $QQ$-plot to assess the normal distribution. The $x$-axis is the theoretical distribution; the $y$-axis is the distribution of our residuals. 

```{r}
flm1 %>%
    ggplot(aes(sample = .stdresid)) +
    stat_qq() +
    stat_qq_line() +
    theme_cowplot()  +
    labs(x = 'Theoretical', y = 'Actual')
```

Our model completely fails the normal assumption. There are some extremely heavy tails. 


## What should we do?

* *High-leverage points, outliers, and influential points*: Check the data for data entry errors; examine the physical context; exclude the data point. If you choose to exclude the data point you should always report that you did so and why you did it. Otherwise you run at risk of being accused of dishonesty.

* *Non-normality, linearity*: Transform the response (dependent) variable; add/remove/transform independent variables; change the assumption of the normal assumption (Binomial, Gamma, etc).

* *Non-constant variance, correlated errors*: Use the generalized least square method. You need to estimate the error structure.

This process requires many iterations of fitting and diagnostics. 

### Our model
Let's try to fix the heteroskedasticity of our data by applying the `boxcox()` transform. We look for a parameter $\lambda$ such that our data becomes as normally distributed as possible. The transform is as follows:

$$
b_i = \begin{cases}
\frac{(y_i - 1)^{\lambda}}{\lambda} & \mbox{if } y \neq 0 \\
\log(y_i) & \mbox{if } y = 0
\end{cases}
$$

We begin by finding $\lambda$:

```{r}
bc <- MASS::boxcox(lm1)
```

We see that the peak of the curve is approximately -0.5. We can get the exact value the following way:

```{r}
lambda <- bc$x[which(bc$y == max(bc$y))]
lambda
```

Let's add the transform to our data set and refit the model. 

```{r}
pulse <-
    pulse %>%
    mutate(bi = (secondPulse - 1)^lambda/lambda)
lm2 <- lm(bi ~ firstPulse + intervention, data = pulse)
```

```{r}
summary(lm2)
```

The adjusted $R^2$ value has increased significantly. Let's look at the fitted-versus-residuals plot to see if the heteroskedasticity has disappeared.

```{r}
flm2 <- fortify(lm2)
flm2 %>%
    ggplot(aes(x = .fitted, y = .resid)) +
    geom_point() +
    stat_smooth(method = 'loess') +
    theme_cowplot()
```

Now we actually see evidence of non-linearity (notice the parabola). Let's look at the $QQ$-plot. 
```{r}
flm2 %>%
    ggplot(aes(sample = .stdresid)) +
    stat_qq() +
    stat_qq_line() +
    labs(x = 'Theoretical', y = 'Actual') +
    theme_cowplot()
```
The $QQ$-plot looks better but there is still considerable evidence of non-normality. Back to the drawing board for us.

