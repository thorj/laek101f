# Lecture 2: Data wrangling

```{r, message=F, warning = F}
library(tidyverse)
library(here) # relative paths
library(readxl)
```

## R Projects and workflows
Last week we used the `mtcars` data set which is included with R. When we want load external data we have to tell R where to find it. To tell R where our files are, we need to supply R with the file paths. This is a good opportunity to introduce some basic workflow. 

Let us assume that we have been tasked with performing a preliminary analysis of some data set. Our supervisor expects us to:

a. Present our results in a couple of days.

b. Hand over the preliminary analysis to the resident data scientist for downstream modeling.

There are thus two things we have to keep in mind; we need to perform the preliminary analysis AND we need to do it in such a way that the data scientist can continue our work as painlessly as possible. We need to *get organized*. This means:

* For each project we work on, we create a *dedicated folder* for that specific project. Any files relevant to a project are **placed in the project folder**.

* The project folder is compartmentalized! That is, there are multiple subdirectories within each project folder, each serving a specific purpose.

Here is an example layout for this hypothetical scenario:

```
Within folder ourProject:
-code/      code to wrangle original data
-data/      output of code
-raw/       original data files
-report/    our .Rmd file
```

Having created our project folder with the relevant structure the next thing we do is to create an [R project file](https://support.rstudio.com/hc/en-us/articles/200526207-Using-RStudio-Projects). By creating an R project, we are telling R where our base of operations is (our working directory). This means that we can use relative paths, namely, instead of writing full file path names like `C:\Documents\ourProject\data\someData.csv` it suffices to write `data\someData.csv`.

Finally, I recommend you use the `here` package as `.Rmd` files use their location as the working directory. Using the file structure above as an example, since our `.Rmd` file will be saved within `report` if we try to point to the data file `someData.csv` with `data/someData.csv` we will be met with an error. `here` looks for the location of the R project file `.Rproj` and sets all file paths relative to `.Rproj`. 

After completing the preliminary analysis we can simply hand over the `ourProject` directory to the data scientist. This is reproducible research in its most basic form. 

### More intricate workflow
Look into [Snakemake](https://snakemake.readthedocs.io/en/stable/), `renv`, [github](https://github.com/), `DiagrammeR`, and [Docker](https://www.docker.com/).

## The anatomy of a file and external data
Consider the following data:

```
id;type;age
1;a;30
2;b;37
3;a;42
```
The first line is called the **header**. It is where the names of the columns (sometimes referred to as fields) are stated. Note that not all data comes with a header line. Most functions which read data into memory assume that there is a header line so you don't have to explicitly state that your file has one. The lines that follow our header store the data values. The semicolon `;` between each value acts as the **delimiter**. The delimiter helps R to map values to their respective columns. The delimiter is not necessarily `;`. Here is a non exhaustive list:

* Comma `,`. Data where the comma is the delimiter are typically stored in `.csv` files. `.csv` stands for *Comma-Separated Values*. You can read in `.csv` files with the `read_csv()` function from `readr`.

* Semicolon `;`. Data where the semicolon is the delimiter are typically stored in `.csv` files as well. You can read in data with semicolon-separated values with the `read_csv2()` function from `readr`.

* Tab `\t`. Data where `\t` (tab) is the delimiter are typically stored in `.csv` or `.tsv` files. `.tsv` stands for *Tab-Separated Values*. You can read in `.tsv` files with the `read_tsv()` function from `readr`.

There are other delimiters. You can use the general function `read_delim()` to specify them (and the ones mentioned above). Finally, Excel spreadsheets `.xls` and `.xlsx` are quite common as well. To read those into R we can use the `readxl` package.

Below are a few examples where I use `here`, `readr`, and `readxl` to read in some dummy data I created (see Appendix).

```{r}
dummyTSV <- read_tsv(here('data', 'l2_data_tsv.txt'))
dummyCSV <- read_csv(here('data', 'l2_data_csv.txt'))
dumyCSV2 <- read_csv2(here('data', 'l2_data_csv2.txt'))
dummyXLSX <- read_xlsx(here('data', 'l2_data_excel.xlsx'))
```

```{r}
dummyXLSX
```

## Factors
We will continue to use `dummyXLSX`. Let's get an overview of the data with the `glimpse()` function from `dplyr`:

```{r}
glimpse(dummyXLSX)
```
From `glimpse()` we see that we have four variables, three of which are of type double and one which is a character. Let's use the `summary()` function from the last lecture to further probe our data:

```{r}
summary(dummyXLSX)
```

All looks well except the `type` variable which only takes on values `a`, `b`, and `c`. I would like to see just how many observations fall under each type. This is a good opportunity to discuss factor variables. A factor variable is a variable which has *levels* and *labels*. The levels of `type` are the aforementioned `a`, `b`, and `c`, (the values `type` takes) and labels contains the "name" of each level. For example, `a` could be a participant with diabetes, `b` a participant with heart disease and `c` our controls. One of the levels of a factor variable will be the *baseline*. Unless told to do otherwise, R will arrange levels in alphabetical order so for `type`, `a` will be the baseline since it is the first letter of the alphabet.

Let's create a new variable based on the schema above.


```{r}
dummyXLSX <- 
    dummyXLSX %>% 
    mutate(factorType = factor(type, 
                               levels = c('a', 'b', 'c'), 
                               labels = c('Diabetes', 'Heart disease', 'Control')))
```

```{r}
summary(dummyXLSX)
```

## Filter
Now, say that I want to restrict my analysis to participants who scored over 50 on the "test" for `dummyXLSX` (variable `metric1`). A very useful tool is the `filter()` function from `dplyr` which allows us to subset our data based on logic operations. 

```{r}
dummyXLSX %>%
    filter(metric1 > 50)
```

Note how over half of our observations are gone. We can filter our data even further, for example by restricting our analysis to participants with Diabetes and Heart disease who scored higher than 50 on `metric1`.

```{r}
dummyXLSX %>%
    filter(metric1 > 50, type %in% c('a', 'b'))
```

The resulting data is even smaller. Below is a list of some of the operators we can use to compare values:

* `>`: Used for $x > y$ which is read as *x is greater than y*.

* `<`: Used for $x < y$ which is read as *x is lesser than y*.

* `>=`: Used for $x \geq y$ which is read as *x is greater OR equal to y*

* `<=`: Used for $x \leq y$ which is read as *x is lesser OR equal to y*.

* `==`: Used for $x = y$ which is read as *x is the same as y*. The double `==` is **not a mistake**.

Here is a small example of `==`:

```{r}
dummyXLSX %>%
    filter(factorType == 'Diabetes')
```

## ifelse and case_when
Assume we want to create a new variable such that participants who scored higher than 50 on `metric1` are labeled as *high* and those who scored lower or equal to 50 are labeled as low. We can use the `ifelse()` function to this end. First and example and then a dissection.

```{r}
dummyXLSX <-
    dummyXLSX %>%
    mutate(scoreCat = ifelse(metric1 > 50, 'High', 'Low'))
```

```{r}
dummyXLSX
```

So how does `ifelse()` work? The function takes in three inputs: a test we want to perform (is the `metric1` score higher than 50?), an action if the test is true (High), and an action if the test is false (metric1 $\leq$ 50; Low). We can create complicated tests with the `&` and `|` operators which symbolize AND and OR respectively. 

```{r}
dummyXLSX <-
    dummyXLSX %>%
    mutate(scoreCatAge = ifelse(metric1 > 50 & age > 30, 'Both true', 'One or both false'))
```

```{r}
dummyXLSX %>%
    select(age, metric1, scoreCatAge)
```

```{r}
dummyXLSX %>%
    mutate(orStatement = ifelse(metric1 > 50 | age < 30, 'One or both true', 'Both false')) %>%
    select(age, metric1, orStatement)
```


Assume we want to create a variable which combines multiple scenarios. It is possible to nest `ifelse()` within `itself()` but this quickly becomes obnoxious and confusing. This is where `case_when()` comes in as it allows us to strictly define what value our new variable takes based on the scenario. Here is one:

```{r}
dummyXLSX %>%
    mutate(scenarios = case_when(age > 35 & metric1 < 50 & type == 'c' ~ 'Control, > 35, < 50',
                                 age > 35 & metric1 > 50 & type == 'b' ~ 'Heart, > 35, > 50',
                                 age <= 35 & metric1 <= 50 & type == 'a' ~ 'Diabetes, <= 35, <= 50',
                                 TRUE ~ 'Whatever does not fit')) %>% 
    select(age, metric1, type, scenarios)
```

## summarize and group_by
A preliminary analysis can be divided into two (roughly) parts: the numerical analysis and the graphical analysis. 
Common statistics we are asked to compute are the sample size, mean, standard deviation, minimum, maximum, and quantiles (typically the 2.5% and 97.5% percentiles) of our sample. For that end we have the following functions: `n()`, `mean()`, `sd()`, `min()`, `max()`, and `quantile()`. All these functions take in some numerical vector and return the statistic of interest. We will discuss these statistics in more detail next week.

Let's compute these statistics for variable `age`:

```{r}
nrow(dummyXLSX)
mean(dummyXLSX$age)
sd(dummyXLSX$age)
min(dummyXLSX$age)
max(dummyXLSX$age)
quantile(dummyXLSX$age, probs = 0.025)
quantile(dummyXLSX$age, probs = 0.975)
```

An alternative way to do the above is to use the `summarize()` function:

```{r}
dummyXLSX %>%
    summarize(sampleSize = n(), 
              mean = mean(age), 
              standardDev = sd(age), 
              mini = min(age), 
              maxi = max(age), 
              q025 = quantile(age, 0.025), 
              q975 = quantile(age, 0.975))
```

Both methods yield the same answer. Now, imagine that we want to compute each of these statistics stratified by the type of the patient. This can be done with base R function but I recommend using the `summarize()` function with the `group_by()` function. To put it simply, the `group_by()` function allows us to define groups and then "wrangle" based on those groups. Let's see what happens.

```{r}
dummyXLSX %>% 
    group_by(type) %>%
    summarize(sampleSize = n(), 
              mean = mean(age), 
              standardDev = sd(age), 
              mini = min(age), 
              maxi = max(age), 
              q025 = quantile(age, 0.025), 
              q975 = quantile(age, 0.975))
```
The code is basically the same as before save for that one additional line. Pretty convenient, right?

## gather and spread
Data can be *long* or *wide*. When our data is wide we have one column for each variable. Imagine we conducted a study where we took some measurements twice over the research period. The data could look something like this:

```{r}
wideExample <- 
    tibble(id = 1:3, 
       measure1 = rnorm(3), 
       measure2 = rnorm(3))
```

```{r}
wideExample
```

Now, it might actually be more beneficial for each measurement to have its own row. When each measurement has its own row we say that the data is long. To convert from wide to long we use the `gather()` function. It has actually been superseeded by `pivot_longer()` but I'm more used to `gather()`. The function `gather()` takes in: the names of the two new columns we are creating ("key", "value") and then a selection of columns. We want each measurement to have its own line so our selection will be `measure1` and `measure2`.

```{r}
wideExample %>%
    gather(measurement, value, measure1, measure2)
```

Alternatively we can just use the `-` prefix to tell R which columns to ignore:

```{r}
wideExample %>%
    gather(measurement, value, -id)
```

To go from long to wide we have the function `spread()` or `pivot_wider()`. I encourage you to try it out for yourselves.

### Why this is useful
The long format is often useful when we want to fit models where we have repeated measurements. It's also useful for when we want to summarize multiple variables simultaneously. Recall from the `summarize()` section when we summarized `age`. There are many more variables that we might be interested in summarizing such as `metric1`. Here is an example where we combine `gather()`, `group_by()`, and `summarize()`.

```{r}
# Let's add another variable to dummyXLSX
dummyXLSX %>%
    mutate(someVar = rnorm(n = n())) %>%
    gather(variables, values, age, metric1, someVar) %>%
    group_by(variables) %>%
    summarize(mean = mean(values), 
              sd = sd(values),
              median = median(values))
```

Here is another example where we include the type of the participants.

```{r}
dummyXLSX %>%
    mutate(someVar = rnorm(n = n())) %>%
    gather(variables, values, age, metric1, someVar) %>%
    group_by(variables, type) %>%
    summarize(mean = mean(values), 
              sd = sd(values),
              median = median(values)) %>%
    arrange(type)
```



## Appendix

```{r, eval = F, code = readLines(here('scripts', 'createFakeData.R'))}
```

